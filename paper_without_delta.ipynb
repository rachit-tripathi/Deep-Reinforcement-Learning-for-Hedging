{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "377ee645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from financial_models.option_price_models import BSM\n",
    "from financial_models.asset_price_models import GBM\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, input, hidden, out_size, num_layers, f):\n",
    "        super(FullyConnected, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.first_layer = nn.Linear(input, hidden)\n",
    "\n",
    "        self.linear = nn.ModuleList([nn.Linear(hidden, hidden) for _ in range(num_layers)])\n",
    "\n",
    "        self.f = f\n",
    "\n",
    "        self.out_layer = nn.Linear(hidden, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(self.first_layer(x))\n",
    "        for layer in range(self.num_layers):\n",
    "            x = self.linear[layer](x)\n",
    "            x = self.f(x)\n",
    "\n",
    "        x = self.f(x)\n",
    "        x = self.out_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def generate_data(apm, opm, num_steps, dt, n=3):\n",
    "    D = []\n",
    "    for j in range(n):\n",
    "        apm.reset()\n",
    "        price = apm.get_current_price()\n",
    "        opt_price = opm.compute_option_price(T, price, mode=\"ttm\")\n",
    "        for p in range(num_steps):\n",
    "            apm.compute_next_price()\n",
    "            next_price = apm.get_current_price()\n",
    "            next_opt_price = opm.compute_option_price(T - (p + 1) * dt, next_price, mode=\"ttm\")\n",
    "            delta = opm.compute_delta_ttm(T - p * dt, price)\n",
    "            D.append({\"p\": price, \"np\": next_price, \"op\": opt_price, \"nop\": next_opt_price, \"ttm\": T - p * dt,\n",
    "                      \"nttm\": T - (p + 1) * dt, \"delta\": delta})\n",
    "            price = next_price\n",
    "            opt_price = next_opt_price\n",
    "    return D\n",
    "\n",
    "\n",
    "def test(model, apm, opm, num_steps, dt, n=10):\n",
    "    losses = []\n",
    "    delta_losses = []\n",
    "    for i in range(n):\n",
    "        D = generate_data(apm, opm, num_steps, dt, n=1)\n",
    "        old_delta = 0\n",
    "        old_out = 0\n",
    "        for tupel in D:\n",
    "            inp = torch.tensor(np.array([old_out, tupel[\"p\"], tupel[\"ttm\"]])).double()\n",
    "            out = model(inp)\n",
    "            if i == 1:\n",
    "                print(inp)\n",
    "                print(out)\n",
    "\n",
    "            trading_costs = (T / num_steps) * (abs(old_out - out) + 0.01 * (old_out - out) ** 2)\n",
    "            pl = (-tupel[\"nop\"] + tupel[\"op\"]) + out * (tupel[\"np\"] - tupel[\"p\"]) - trading_costs\n",
    "            loss = torch.pow(pl, 2) - 1 / 1000 * pl\n",
    "            losses.append(loss.detach().numpy()[0])\n",
    "\n",
    "            trading_costs_delta = (T / num_steps) * (abs(old_delta - tupel[\"delta\"]) + 0.01 *\n",
    "                                                     (old_delta - tupel[\"delta\"]) ** 2)\n",
    "            pl_delta = (-tupel[\"nop\"] + tupel[\"op\"]) + tupel[\"delta\"] * (tupel[\"np\"] - tupel[\"p\"]) - trading_costs_delta\n",
    "            delta_loss = (pl_delta) ** 2 - 1/1000 *pl_delta\n",
    "            delta_losses.append(delta_loss)\n",
    "            old_delta = tupel[\"delta\"]\n",
    "            old_out = out.detach().numpy()[0]\n",
    "    return losses, delta_losses, np.mean(losses), np.mean(delta_losses)\n",
    "\n",
    "\n",
    "volatility = 0.15\n",
    "strike_price = 1\n",
    "starting_price = 1\n",
    "mu = 0.0\n",
    "T = 1.0\n",
    "num_steps = 128\n",
    "dt = T / num_steps\n",
    "risk_free_interest_rate = 0.01\n",
    "\n",
    "seed = 345\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model = FullyConnected(3, 16, 1, 5, f=torch.nn.functional.relu)\n",
    "model.double()\n",
    "apm = GBM(mu=mu, dt=dt, s_0=starting_price, sigma=volatility)\n",
    "opm = BSM(strike_price=strike_price, risk_free_interest_rate=risk_free_interest_rate, volatility=volatility, T=T, dt=dt)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_eps = 800\n",
    "norm_factor = 10000\n",
    "\n",
    "test_res = []\n",
    "test_res_delta = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e27ad0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0113,  0.9922], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0020,  0.9844], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0189,  0.9766], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0318,  0.9688], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0310,  0.9609], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0256,  0.9531], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0431,  0.9453], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0628,  0.9375], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0502,  0.9297], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0465,  0.9219], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0586,  0.9141], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0663,  0.9062], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0731,  0.8984], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0647,  0.8906], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0828,  0.8828], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0944,  0.8750], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0841,  0.8672], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0758,  0.8594], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0632,  0.8516], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0506,  0.8438], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0712,  0.8359], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0494,  0.8281], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0371,  0.8203], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0348,  0.8125], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0333,  0.8047], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0313,  0.7969], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0193,  0.7891], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0344,  0.7812], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0215,  0.7734], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0311,  0.7656], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0568,  0.7578], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0638,  0.7500], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0570,  0.7422], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0284,  0.7344], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0341,  0.7266], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0200,  0.7188], dtype=torch.float64)\n",
      "tensor([-0.0850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0850,  1.0101,  0.7109], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  1.0072,  0.7031], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  1.0054,  0.6953], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  1.0073,  0.6875], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  1.0159,  0.6797], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  1.0117,  0.6719], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  1.0081,  0.6641], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  1.0257,  0.6562], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  1.0052,  0.6484], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  1.0011,  0.6406], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9869,  0.6328], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9962,  0.6250], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9792,  0.6172], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9738,  0.6094], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9653,  0.6016], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9591,  0.5938], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9675,  0.5859], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9703,  0.5781], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9358,  0.5703], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9631,  0.5625], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9584,  0.5547], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9657,  0.5469], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9573,  0.5391], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9475,  0.5312], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9635,  0.5234], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9811,  0.5156], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9747,  0.5078], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9822,  0.5000], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9722,  0.4922], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9641,  0.4844], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9593,  0.4766], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9421,  0.4688], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.9549,  0.4609], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9749,  0.4531], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9857,  0.4453], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9719,  0.4375], dtype=torch.float64)\n",
      "tensor([-0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0849,  0.9679,  0.4297], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.9612,  0.4219], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.9470,  0.4141], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.9260,  0.4062], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.9070,  0.3984], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.9218,  0.3906], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.9109,  0.3828], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.9092,  0.3750], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8962,  0.3672], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8808,  0.3594], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8804,  0.3516], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8793,  0.3438], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8806,  0.3359], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8577,  0.3281], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8454,  0.3203], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8476,  0.3125], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8668,  0.3047], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8730,  0.2969], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8876,  0.2891], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8929,  0.2812], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8916,  0.2734], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8979,  0.2656], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8981,  0.2578], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8871,  0.2500], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8868,  0.2422], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.9022,  0.2344], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8833,  0.2266], dtype=torch.float64)\n",
      "tensor([-0.0848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0848,  0.8659,  0.2188], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8814,  0.2109], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8771,  0.2031], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8605,  0.1953], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8511,  0.1875], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8502,  0.1797], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0847,  0.8515,  0.1719], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8500,  0.1641], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8594,  0.1562], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8743,  0.1484], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8565,  0.1406], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8647,  0.1328], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8531,  0.1250], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8507,  0.1172], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8626,  0.1094], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8605,  0.1016], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8448,  0.0938], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8450,  0.0859], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8455,  0.0781], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8694,  0.0703], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8691,  0.0625], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8797,  0.0547], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8833,  0.0469], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8829,  0.0391], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.8954,  0.0312], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.9085,  0.0234], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.9181,  0.0156], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0847,  0.9170,  0.0078], dtype=torch.float64)\n",
      "tensor([-0.0847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  7.040437300994911e-05 5.745904537411398e-07\n",
      "loss: [848.57807311]\n",
      "episode:  1\n",
      "loss: [1091.51969163]\n",
      "episode:  2\n",
      "loss: [407.53117124]\n",
      "episode:  3\n",
      "loss: [1096.548847]\n",
      "episode:  4\n",
      "loss: [615.04921927]\n",
      "episode:  5\n",
      "loss: [771.06435064]\n",
      "episode:  6\n",
      "loss: [515.81926683]\n",
      "episode:  7\n",
      "loss: [356.38625389]\n",
      "episode:  8\n",
      "loss: [318.93284256]\n",
      "episode:  9\n",
      "loss: [143.46440104]\n",
      "episode:  10\n",
      "loss: [261.29307015]\n",
      "episode:  11\n",
      "loss: [293.04802183]\n",
      "episode:  12\n",
      "loss: [286.31823767]\n",
      "episode:  13\n",
      "loss: [205.92431052]\n",
      "episode:  14\n",
      "loss: [276.13571757]\n",
      "episode:  15\n",
      "loss: [233.88588292]\n",
      "episode:  16\n",
      "loss: [226.92067194]\n",
      "episode:  17\n",
      "loss: [273.67464846]\n",
      "episode:  18\n",
      "loss: [161.4920746]\n",
      "episode:  19\n",
      "loss: [231.71983695]\n",
      "episode:  20\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.5129], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5129, 1.0179, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5131], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5131, 1.0091, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5131], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5131, 1.0082, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5132], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5132, 0.9846, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5134], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5134, 0.9893, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5134], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5134, 0.9988, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5133], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5133, 0.9982, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5134], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5134, 0.9849, 0.9375], dtype=torch.float64)\n",
      "tensor([0.5135], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5135, 0.9963, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5134], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5134, 1.0052, 0.9219], dtype=torch.float64)\n",
      "tensor([0.5134], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5134, 1.0027, 0.9141], dtype=torch.float64)\n",
      "tensor([0.5134], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5134, 0.9818, 0.9062], dtype=torch.float64)\n",
      "tensor([0.5136], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5136, 0.9967, 0.8984], dtype=torch.float64)\n",
      "tensor([0.5135], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5135, 0.9853, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5136], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5136, 1.0000, 0.8828], dtype=torch.float64)\n",
      "tensor([0.5136], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5136, 1.0173, 0.8750], dtype=torch.float64)\n",
      "tensor([0.5135], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5135, 1.0333, 0.8672], dtype=torch.float64)\n",
      "tensor([0.5134], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5134, 1.0391, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5134], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5134, 1.0437, 0.8516], dtype=torch.float64)\n",
      "tensor([0.5134], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5134, 1.0817, 0.8438], dtype=torch.float64)\n",
      "tensor([0.5132], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5132, 1.1047, 0.8359], dtype=torch.float64)\n",
      "tensor([0.5130], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5130, 1.1177, 0.8281], dtype=torch.float64)\n",
      "tensor([0.5130], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5130, 1.1018, 0.8203], dtype=torch.float64)\n",
      "tensor([0.5131], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5131, 1.1224, 0.8125], dtype=torch.float64)\n",
      "tensor([0.5130], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5130, 1.1434, 0.8047], dtype=torch.float64)\n",
      "tensor([0.5129], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5129, 1.1544, 0.7969], dtype=torch.float64)\n",
      "tensor([0.5129], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5129, 1.1737, 0.7891], dtype=torch.float64)\n",
      "tensor([0.5128], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5128, 1.1790, 0.7812], dtype=torch.float64)\n",
      "tensor([0.5127], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5127, 1.1808, 0.7734], dtype=torch.float64)\n",
      "tensor([0.5128], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5128, 1.1831, 0.7656], dtype=torch.float64)\n",
      "tensor([0.5128], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5128, 1.1577, 0.7578], dtype=torch.float64)\n",
      "tensor([0.5130], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5130, 1.1496, 0.7500], dtype=torch.float64)\n",
      "tensor([0.5130], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5130, 1.1627, 0.7422], dtype=torch.float64)\n",
      "tensor([0.5130], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5130, 1.1693, 0.7344], dtype=torch.float64)\n",
      "tensor([0.5130], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5130, 1.1838, 0.7266], dtype=torch.float64)\n",
      "tensor([0.5129], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5129, 1.1961, 0.7188], dtype=torch.float64)\n",
      "tensor([0.5128], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5128, 1.2088, 0.7109], dtype=torch.float64)\n",
      "tensor([0.5128], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5128, 1.2015, 0.7031], dtype=torch.float64)\n",
      "tensor([0.5128], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5128, 1.1981, 0.6953], dtype=torch.float64)\n",
      "tensor([0.5129], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5129, 1.2127, 0.6875], dtype=torch.float64)\n",
      "tensor([0.5128], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5128, 1.2015, 0.6797], dtype=torch.float64)\n",
      "tensor([0.5129], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5129, 1.1901, 0.6719], dtype=torch.float64)\n",
      "tensor([0.5130], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5130, 1.1787, 0.6641], dtype=torch.float64)\n",
      "tensor([0.5131], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5131, 1.1841, 0.6562], dtype=torch.float64)\n",
      "tensor([0.5131], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5131, 1.2051, 0.6484], dtype=torch.float64)\n",
      "tensor([0.5130], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5130, 1.1908, 0.6406], dtype=torch.float64)\n",
      "tensor([0.5131], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5131, 1.2008, 0.6328], dtype=torch.float64)\n",
      "tensor([0.5131], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5131, 1.2076, 0.6250], dtype=torch.float64)\n",
      "tensor([0.5131], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5131, 1.1968, 0.6172], dtype=torch.float64)\n",
      "tensor([0.5132], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5132, 1.1925, 0.6094], dtype=torch.float64)\n",
      "tensor([0.5132], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5132, 1.1794, 0.6016], dtype=torch.float64)\n",
      "tensor([0.5133], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5133, 1.1610, 0.5938], dtype=torch.float64)\n",
      "tensor([0.5135], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5135, 1.1742, 0.5859], dtype=torch.float64)\n",
      "tensor([0.5134], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5134, 1.1678, 0.5781], dtype=torch.float64)\n",
      "tensor([0.5135], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5135, 1.1471, 0.5703], dtype=torch.float64)\n",
      "tensor([0.5136], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5136, 1.1375, 0.5625], dtype=torch.float64)\n",
      "tensor([0.5137], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5137, 1.1313, 0.5547], dtype=torch.float64)\n",
      "tensor([0.5138], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5138, 1.1437, 0.5469], dtype=torch.float64)\n",
      "tensor([0.5137], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5137, 1.1235, 0.5391], dtype=torch.float64)\n",
      "tensor([0.5139], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5139, 1.1448, 0.5312], dtype=torch.float64)\n",
      "tensor([0.5138], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5138, 1.1440, 0.5234], dtype=torch.float64)\n",
      "tensor([0.5138], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5138, 1.1264, 0.5156], dtype=torch.float64)\n",
      "tensor([0.5139], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5139, 1.1296, 0.5078], dtype=torch.float64)\n",
      "tensor([0.5139], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5139, 1.1325, 0.5000], dtype=torch.float64)\n",
      "tensor([0.5139], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5139, 1.1391, 0.4922], dtype=torch.float64)\n",
      "tensor([0.5139], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5139, 1.1351, 0.4844], dtype=torch.float64)\n",
      "tensor([0.5140], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5140, 1.1295, 0.4766], dtype=torch.float64)\n",
      "tensor([0.5140], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5140, 1.1442, 0.4688], dtype=torch.float64)\n",
      "tensor([0.5139], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5139, 1.1540, 0.4609], dtype=torch.float64)\n",
      "tensor([0.5139], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5139, 1.1329, 0.4531], dtype=torch.float64)\n",
      "tensor([0.5141], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5141, 1.1381, 0.4453], dtype=torch.float64)\n",
      "tensor([0.5140], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5140, 1.1209, 0.4375], dtype=torch.float64)\n",
      "tensor([0.5142], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5142, 1.1248, 0.4297], dtype=torch.float64)\n",
      "tensor([0.5142], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5142, 1.1070, 0.4219], dtype=torch.float64)\n",
      "tensor([0.5143], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5143, 1.1041, 0.4141], dtype=torch.float64)\n",
      "tensor([0.5143], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5143, 1.0931, 0.4062], dtype=torch.float64)\n",
      "tensor([0.5144], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5144, 1.0656, 0.3984], dtype=torch.float64)\n",
      "tensor([0.5146], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5146, 1.0613, 0.3906], dtype=torch.float64)\n",
      "tensor([0.5147], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5147, 1.0548, 0.3828], dtype=torch.float64)\n",
      "tensor([0.5147], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5147, 1.0384, 0.3750], dtype=torch.float64)\n",
      "tensor([0.5149], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5149, 1.0552, 0.3672], dtype=torch.float64)\n",
      "tensor([0.5148], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5148, 1.0574, 0.3594], dtype=torch.float64)\n",
      "tensor([0.5148], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5148, 1.0438, 0.3516], dtype=torch.float64)\n",
      "tensor([0.5149], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5149, 1.0325, 0.3438], dtype=torch.float64)\n",
      "tensor([0.5150], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5150, 1.0226, 0.3359], dtype=torch.float64)\n",
      "tensor([0.5151], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5151, 1.0185, 0.3281], dtype=torch.float64)\n",
      "tensor([0.5152], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5152, 1.0115, 0.3203], dtype=torch.float64)\n",
      "tensor([0.5153], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5153, 0.9984, 0.3125], dtype=torch.float64)\n",
      "tensor([0.5154], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5154, 0.9938, 0.3047], dtype=torch.float64)\n",
      "tensor([0.5154], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5154, 0.9756, 0.2969], dtype=torch.float64)\n",
      "tensor([0.5156], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5156, 0.9648, 0.2891], dtype=torch.float64)\n",
      "tensor([0.5157], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5157, 0.9444, 0.2812], dtype=torch.float64)\n",
      "tensor([0.5158], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5158, 0.9352, 0.2734], dtype=torch.float64)\n",
      "tensor([0.5159], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5159, 0.9513, 0.2656], dtype=torch.float64)\n",
      "tensor([0.5159], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5159, 0.9870, 0.2578], dtype=torch.float64)\n",
      "tensor([0.5156], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5156, 0.9918, 0.2500], dtype=torch.float64)\n",
      "tensor([0.5156], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5156, 0.9653, 0.2422], dtype=torch.float64)\n",
      "tensor([0.5158], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5158, 0.9642, 0.2344], dtype=torch.float64)\n",
      "tensor([0.5159], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5159, 0.9511, 0.2266], dtype=torch.float64)\n",
      "tensor([0.5160], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5160, 0.9398, 0.2188], dtype=torch.float64)\n",
      "tensor([0.5161], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5161, 0.9620, 0.2109], dtype=torch.float64)\n",
      "tensor([0.5160], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5160, 0.9517, 0.2031], dtype=torch.float64)\n",
      "tensor([0.5161], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5161, 0.9373, 0.1953], dtype=torch.float64)\n",
      "tensor([0.5163], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5163, 0.9482, 0.1875], dtype=torch.float64)\n",
      "tensor([0.5162], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5162, 0.9205, 0.1797], dtype=torch.float64)\n",
      "tensor([0.5165], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5165, 0.9220, 0.1719], dtype=torch.float64)\n",
      "tensor([0.5165], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5165, 0.9351, 0.1641], dtype=torch.float64)\n",
      "tensor([0.5164], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5164, 0.9346, 0.1562], dtype=torch.float64)\n",
      "tensor([0.5165], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5165, 0.9342, 0.1484], dtype=torch.float64)\n",
      "tensor([0.5165], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5165, 0.9349, 0.1406], dtype=torch.float64)\n",
      "tensor([0.5166], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5166, 0.9622, 0.1328], dtype=torch.float64)\n",
      "tensor([0.5164], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5164, 0.9634, 0.1250], dtype=torch.float64)\n",
      "tensor([0.5164], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5164, 0.9529, 0.1172], dtype=torch.float64)\n",
      "tensor([0.5165], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5165, 0.9330, 0.1094], dtype=torch.float64)\n",
      "tensor([0.5167], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5167, 0.9270, 0.1016], dtype=torch.float64)\n",
      "tensor([0.5168], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5168, 0.9196, 0.0938], dtype=torch.float64)\n",
      "tensor([0.5169], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5169, 0.9156, 0.0859], dtype=torch.float64)\n",
      "tensor([0.5170], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5170, 0.9280, 0.0781], dtype=torch.float64)\n",
      "tensor([0.5170], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5170, 0.9386, 0.0703], dtype=torch.float64)\n",
      "tensor([0.5169], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5169, 0.9480, 0.0625], dtype=torch.float64)\n",
      "tensor([0.5169], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5169, 0.9388, 0.0547], dtype=torch.float64)\n",
      "tensor([0.5170], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5170, 0.9728, 0.0469], dtype=torch.float64)\n",
      "tensor([0.5167], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5167, 0.9697, 0.0391], dtype=torch.float64)\n",
      "tensor([0.5168], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5168, 0.9592, 0.0312], dtype=torch.float64)\n",
      "tensor([0.5169], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5169, 0.9719, 0.0234], dtype=torch.float64)\n",
      "tensor([0.5169], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5169, 0.9929, 0.0156], dtype=torch.float64)\n",
      "tensor([0.5167], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5167, 1.0059, 0.0078], dtype=torch.float64)\n",
      "tensor([0.5167], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_result:  1.5439828849438335e-05 6.07355110863272e-07\n",
      "loss: [189.74506026]\n",
      "episode:  21\n",
      "loss: [249.86054132]\n",
      "episode:  22\n",
      "loss: [333.57240741]\n",
      "episode:  23\n",
      "loss: [210.04192427]\n",
      "episode:  24\n",
      "loss: [243.51149549]\n",
      "episode:  25\n",
      "loss: [202.69693761]\n",
      "episode:  26\n",
      "loss: [175.20678121]\n",
      "episode:  27\n",
      "loss: [174.49016271]\n",
      "episode:  28\n",
      "loss: [223.10516531]\n",
      "episode:  29\n",
      "loss: [169.6783051]\n",
      "episode:  30\n",
      "loss: [219.8624938]\n",
      "episode:  31\n",
      "loss: [200.08993523]\n",
      "episode:  32\n",
      "loss: [182.72448659]\n",
      "episode:  33\n",
      "loss: [231.57177705]\n",
      "episode:  34\n",
      "loss: [135.61343525]\n",
      "episode:  35\n",
      "loss: [190.43949475]\n",
      "episode:  36\n",
      "loss: [183.69807767]\n",
      "episode:  37\n",
      "loss: [271.15377807]\n",
      "episode:  38\n",
      "loss: [266.36865688]\n",
      "episode:  39\n",
      "loss: [214.11230159]\n",
      "episode:  40\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.5894], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5894, 0.9834, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5886], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5886, 0.9926, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5886], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5886, 0.9777, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5886], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5886, 0.9835, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5886], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5886, 0.9758, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5886], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5886, 0.9866, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5886], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5886, 1.0081, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5886], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5886, 1.0059, 0.9375], dtype=torch.float64)\n",
      "tensor([0.5886], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5886, 0.9964, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5885], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5885, 1.0035, 0.9219], dtype=torch.float64)\n",
      "tensor([0.5885], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5885, 0.9905, 0.9141], dtype=torch.float64)\n",
      "tensor([0.5885], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5885, 0.9770, 0.9062], dtype=torch.float64)\n",
      "tensor([0.5885], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5885, 0.9827, 0.8984], dtype=torch.float64)\n",
      "tensor([0.5885], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5885, 0.9937, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5885], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5885, 1.0154, 0.8828], dtype=torch.float64)\n",
      "tensor([0.5885], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5885, 1.0036, 0.8750], dtype=torch.float64)\n",
      "tensor([0.5885], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5885, 1.0112, 0.8672], dtype=torch.float64)\n",
      "tensor([0.5884], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5884, 1.0152, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5884], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5884, 1.0302, 0.8516], dtype=torch.float64)\n",
      "tensor([0.5884], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5884, 1.0176, 0.8438], dtype=torch.float64)\n",
      "tensor([0.5884], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5884, 1.0109, 0.8359], dtype=torch.float64)\n",
      "tensor([0.5884], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5884, 1.0065, 0.8281], dtype=torch.float64)\n",
      "tensor([0.5884], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5884, 0.9892, 0.8203], dtype=torch.float64)\n",
      "tensor([0.5884], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5884, 0.9777, 0.8125], dtype=torch.float64)\n",
      "tensor([0.5883], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5883, 0.9849, 0.8047], dtype=torch.float64)\n",
      "tensor([0.5883], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5883, 0.9928, 0.7969], dtype=torch.float64)\n",
      "tensor([0.5883], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5883, 0.9813, 0.7891], dtype=torch.float64)\n",
      "tensor([0.5883], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5883, 0.9761, 0.7812], dtype=torch.float64)\n",
      "tensor([0.5883], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5883, 0.9899, 0.7734], dtype=torch.float64)\n",
      "tensor([0.5883], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5883, 0.9891, 0.7656], dtype=torch.float64)\n",
      "tensor([0.5883], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5883, 0.9959, 0.7578], dtype=torch.float64)\n",
      "tensor([0.5883], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5883, 0.9988, 0.7500], dtype=torch.float64)\n",
      "tensor([0.5882], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5882, 1.0078, 0.7422], dtype=torch.float64)\n",
      "tensor([0.5882], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5882, 1.0111, 0.7344], dtype=torch.float64)\n",
      "tensor([0.5882], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5882, 0.9958, 0.7266], dtype=torch.float64)\n",
      "tensor([0.5882], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5882, 0.9905, 0.7188], dtype=torch.float64)\n",
      "tensor([0.5882], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5882, 0.9556, 0.7109], dtype=torch.float64)\n",
      "tensor([0.5882], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5882, 0.9645, 0.7031], dtype=torch.float64)\n",
      "tensor([0.5882], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5882, 0.9481, 0.6953], dtype=torch.float64)\n",
      "tensor([0.5881], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5881, 0.9404, 0.6875], dtype=torch.float64)\n",
      "tensor([0.5881], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5881, 0.9276, 0.6797], dtype=torch.float64)\n",
      "tensor([0.5881], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5881, 0.9205, 0.6719], dtype=torch.float64)\n",
      "tensor([0.5881], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5881, 0.9361, 0.6641], dtype=torch.float64)\n",
      "tensor([0.5881], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5881, 0.9487, 0.6562], dtype=torch.float64)\n",
      "tensor([0.5881], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5881, 0.9544, 0.6484], dtype=torch.float64)\n",
      "tensor([0.5881], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5881, 0.9440, 0.6406], dtype=torch.float64)\n",
      "tensor([0.5880], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5880, 0.9492, 0.6328], dtype=torch.float64)\n",
      "tensor([0.5880], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5880, 0.9359, 0.6250], dtype=torch.float64)\n",
      "tensor([0.5880], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5880, 0.9233, 0.6172], dtype=torch.float64)\n",
      "tensor([0.5880], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5880, 0.9008, 0.6094], dtype=torch.float64)\n",
      "tensor([0.5880], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5880, 0.9054, 0.6016], dtype=torch.float64)\n",
      "tensor([0.5880], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5880, 0.8878, 0.5938], dtype=torch.float64)\n",
      "tensor([0.5880], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5880, 0.8858, 0.5859], dtype=torch.float64)\n",
      "tensor([0.5880], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5880, 0.8767, 0.5781], dtype=torch.float64)\n",
      "tensor([0.5879], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5879, 0.8942, 0.5703], dtype=torch.float64)\n",
      "tensor([0.5879], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5879, 0.8860, 0.5625], dtype=torch.float64)\n",
      "tensor([0.5879], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5879, 0.8983, 0.5547], dtype=torch.float64)\n",
      "tensor([0.5879], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5879, 0.9179, 0.5469], dtype=torch.float64)\n",
      "tensor([0.5879], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5879, 0.9213, 0.5391], dtype=torch.float64)\n",
      "tensor([0.5879], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5879, 0.9115, 0.5312], dtype=torch.float64)\n",
      "tensor([0.5879], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5879, 0.9030, 0.5234], dtype=torch.float64)\n",
      "tensor([0.5879], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5879, 0.8833, 0.5156], dtype=torch.float64)\n",
      "tensor([0.5878], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5878, 0.8982, 0.5078], dtype=torch.float64)\n",
      "tensor([0.5878], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5878, 0.9195, 0.5000], dtype=torch.float64)\n",
      "tensor([0.5878], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5878, 0.9349, 0.4922], dtype=torch.float64)\n",
      "tensor([0.5878], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5878, 0.9221, 0.4844], dtype=torch.float64)\n",
      "tensor([0.5878], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5878, 0.9416, 0.4766], dtype=torch.float64)\n",
      "tensor([0.5878], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5878, 0.9533, 0.4688], dtype=torch.float64)\n",
      "tensor([0.5878], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5878, 0.9786, 0.4609], dtype=torch.float64)\n",
      "tensor([0.5878], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5878, 0.9981, 0.4531], dtype=torch.float64)\n",
      "tensor([0.5878], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5878, 0.9903, 0.4453], dtype=torch.float64)\n",
      "tensor([0.5877], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5877, 0.9936, 0.4375], dtype=torch.float64)\n",
      "tensor([0.5877], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5877, 1.0184, 0.4297], dtype=torch.float64)\n",
      "tensor([0.5877], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5877, 1.0149, 0.4219], dtype=torch.float64)\n",
      "tensor([0.5877], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5877, 1.0202, 0.4141], dtype=torch.float64)\n",
      "tensor([0.5877], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5877, 1.0211, 0.4062], dtype=torch.float64)\n",
      "tensor([0.5877], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5877, 1.0445, 0.3984], dtype=torch.float64)\n",
      "tensor([0.5877], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5877, 1.0541, 0.3906], dtype=torch.float64)\n",
      "tensor([0.5877], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5877, 1.0611, 0.3828], dtype=torch.float64)\n",
      "tensor([0.5877], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5877, 1.0588, 0.3750], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5877], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5877, 1.0550, 0.3672], dtype=torch.float64)\n",
      "tensor([0.5877], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5877, 1.0468, 0.3594], dtype=torch.float64)\n",
      "tensor([0.5877], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5877, 1.0236, 0.3516], dtype=torch.float64)\n",
      "tensor([0.5876], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5876, 1.0276, 0.3438], dtype=torch.float64)\n",
      "tensor([0.5876], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5876, 1.0266, 0.3359], dtype=torch.float64)\n",
      "tensor([0.5876], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5876, 1.0481, 0.3281], dtype=torch.float64)\n",
      "tensor([0.5877], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5877, 1.0440, 0.3203], dtype=torch.float64)\n",
      "tensor([0.5876], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5876, 1.0303, 0.3125], dtype=torch.float64)\n",
      "tensor([0.5876], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5876, 1.0311, 0.3047], dtype=torch.float64)\n",
      "tensor([0.5876], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5876, 1.0467, 0.2969], dtype=torch.float64)\n",
      "tensor([0.5877], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5877, 1.0375, 0.2891], dtype=torch.float64)\n",
      "tensor([0.5876], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5876, 1.0274, 0.2812], dtype=torch.float64)\n",
      "tensor([0.5875], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5875, 1.0210, 0.2734], dtype=torch.float64)\n",
      "tensor([0.5875], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5875, 1.0134, 0.2656], dtype=torch.float64)\n",
      "tensor([0.5875], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5875, 1.0146, 0.2578], dtype=torch.float64)\n",
      "tensor([0.5875], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5875, 1.0098, 0.2500], dtype=torch.float64)\n",
      "tensor([0.5874], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5874, 1.0109, 0.2422], dtype=torch.float64)\n",
      "tensor([0.5874], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5874, 0.9892, 0.2344], dtype=torch.float64)\n",
      "tensor([0.5873], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5873, 0.9825, 0.2266], dtype=torch.float64)\n",
      "tensor([0.5873], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5873, 0.9647, 0.2188], dtype=torch.float64)\n",
      "tensor([0.5872], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5872, 0.9529, 0.2109], dtype=torch.float64)\n",
      "tensor([0.5872], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5872, 0.9677, 0.2031], dtype=torch.float64)\n",
      "tensor([0.5872], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5872, 0.9437, 0.1953], dtype=torch.float64)\n",
      "tensor([0.5871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5871, 0.9313, 0.1875], dtype=torch.float64)\n",
      "tensor([0.5871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5871, 0.9321, 0.1797], dtype=torch.float64)\n",
      "tensor([0.5871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5871, 0.9347, 0.1719], dtype=torch.float64)\n",
      "tensor([0.5870], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5870, 0.9411, 0.1641], dtype=torch.float64)\n",
      "tensor([0.5870], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5870, 0.9571, 0.1562], dtype=torch.float64)\n",
      "tensor([0.5871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5871, 0.9585, 0.1484], dtype=torch.float64)\n",
      "tensor([0.5871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5871, 0.9838, 0.1406], dtype=torch.float64)\n",
      "tensor([0.5872], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5872, 0.9719, 0.1328], dtype=torch.float64)\n",
      "tensor([0.5871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5871, 0.9535, 0.1250], dtype=torch.float64)\n",
      "tensor([0.5870], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5870, 0.9503, 0.1172], dtype=torch.float64)\n",
      "tensor([0.5870], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5870, 0.9527, 0.1094], dtype=torch.float64)\n",
      "tensor([0.5870], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5870, 0.9581, 0.1016], dtype=torch.float64)\n",
      "tensor([0.5871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5871, 0.9757, 0.0938], dtype=torch.float64)\n",
      "tensor([0.5871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5871, 0.9702, 0.0859], dtype=torch.float64)\n",
      "tensor([0.5871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5871, 0.9679, 0.0781], dtype=torch.float64)\n",
      "tensor([0.5871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5871, 0.9658, 0.0703], dtype=torch.float64)\n",
      "tensor([0.5870], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5870, 0.9659, 0.0625], dtype=torch.float64)\n",
      "tensor([0.5870], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5870, 0.9542, 0.0547], dtype=torch.float64)\n",
      "tensor([0.5869], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5869, 0.9547, 0.0469], dtype=torch.float64)\n",
      "tensor([0.5869], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5869, 0.9758, 0.0391], dtype=torch.float64)\n",
      "tensor([0.5870], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5870, 0.9871, 0.0312], dtype=torch.float64)\n",
      "tensor([0.5871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5871, 1.0045, 0.0234], dtype=torch.float64)\n",
      "tensor([0.5872], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5872, 1.0104, 0.0156], dtype=torch.float64)\n",
      "tensor([0.5872], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5872, 0.9900, 0.0078], dtype=torch.float64)\n",
      "tensor([0.5871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.6317819685764417e-05 6.081765725673269e-07\n",
      "loss: [158.1659892]\n",
      "episode:  41\n",
      "loss: [233.40243596]\n",
      "episode:  42\n",
      "loss: [252.81123684]\n",
      "episode:  43\n",
      "loss: [136.04820155]\n",
      "episode:  44\n",
      "loss: [266.14776977]\n",
      "episode:  45\n",
      "loss: [290.29388404]\n",
      "episode:  46\n",
      "loss: [242.09695589]\n",
      "episode:  47\n",
      "loss: [164.08078412]\n",
      "episode:  48\n",
      "loss: [223.0827947]\n",
      "episode:  49\n",
      "loss: [170.53887235]\n",
      "episode:  50\n",
      "loss: [162.94470136]\n",
      "episode:  51\n",
      "loss: [221.23595398]\n",
      "episode:  52\n",
      "loss: [189.37948495]\n",
      "episode:  53\n",
      "loss: [203.96329673]\n",
      "episode:  54\n",
      "loss: [224.86548467]\n",
      "episode:  55\n",
      "loss: [182.52855857]\n",
      "episode:  56\n",
      "loss: [196.95637423]\n",
      "episode:  57\n",
      "loss: [204.76230976]\n",
      "episode:  58\n",
      "loss: [149.60731116]\n",
      "episode:  59\n",
      "loss: [144.17311165]\n",
      "episode:  60\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.5686], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5686, 1.0229, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5672], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5672, 1.0377, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5672], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5672, 0.9989, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5672], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5672, 0.9697, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5672], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5672, 0.9537, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 0.9770, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 0.9736, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 0.9596, 0.9375], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 0.9549, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 0.9520, 0.9219], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 0.9595, 0.9141], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 0.9740, 0.9062], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 0.9703, 0.8984], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 0.9732, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 0.9918, 0.8828], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 0.9613, 0.8750], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9321, 0.8672], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9231, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9260, 0.8516], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9333, 0.8438], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9376, 0.8359], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5670, 0.9519, 0.8281], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9551, 0.8203], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9649, 0.8125], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9547, 0.8047], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9669, 0.7969], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9718, 0.7891], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9734, 0.7812], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9816, 0.7734], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9699, 0.7656], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9851, 0.7578], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9707, 0.7500], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9803, 0.7422], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9888, 0.7344], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9825, 0.7266], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9540, 0.7188], dtype=torch.float64)\n",
      "tensor([0.5669], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5669, 0.9694, 0.7109], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9931, 0.7031], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 1.0101, 0.6953], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 1.0030, 0.6875], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 0.9994, 0.6797], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 1.0030, 0.6719], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 0.9904, 0.6641], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9981, 0.6562], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 0.9814, 0.6484], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9734, 0.6406], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9674, 0.6328], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9809, 0.6250], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9737, 0.6172], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 0.9670, 0.6094], dtype=torch.float64)\n",
      "tensor([0.5669], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5669, 1.0010, 0.6016], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 1.0023, 0.5938], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 1.0167, 0.5859], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 1.0226, 0.5781], dtype=torch.float64)\n",
      "tensor([0.5672], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5672, 1.0513, 0.5703], dtype=torch.float64)\n",
      "tensor([0.5673], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5673, 1.0356, 0.5625], dtype=torch.float64)\n",
      "tensor([0.5672], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5672, 1.0376, 0.5547], dtype=torch.float64)\n",
      "tensor([0.5672], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5672, 1.0296, 0.5469], dtype=torch.float64)\n",
      "tensor([0.5672], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5672, 1.0500, 0.5391], dtype=torch.float64)\n",
      "tensor([0.5673], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5673, 1.0624, 0.5312], dtype=torch.float64)\n",
      "tensor([0.5673], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5673, 1.0555, 0.5234], dtype=torch.float64)\n",
      "tensor([0.5673], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5673, 1.0573, 0.5156], dtype=torch.float64)\n",
      "tensor([0.5673], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5673, 1.0492, 0.5078], dtype=torch.float64)\n",
      "tensor([0.5673], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5673, 1.0736, 0.5000], dtype=torch.float64)\n",
      "tensor([0.5674], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5674, 1.0771, 0.4922], dtype=torch.float64)\n",
      "tensor([0.5674], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5674, 1.0840, 0.4844], dtype=torch.float64)\n",
      "tensor([0.5674], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5674, 1.0728, 0.4766], dtype=torch.float64)\n",
      "tensor([0.5674], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5674, 1.0746, 0.4688], dtype=torch.float64)\n",
      "tensor([0.5674], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5674, 1.0845, 0.4609], dtype=torch.float64)\n",
      "tensor([0.5675], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5675, 1.0865, 0.4531], dtype=torch.float64)\n",
      "tensor([0.5675], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5675, 1.0759, 0.4453], dtype=torch.float64)\n",
      "tensor([0.5674], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5674, 1.0730, 0.4375], dtype=torch.float64)\n",
      "tensor([0.5674], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5674, 1.0535, 0.4297], dtype=torch.float64)\n",
      "tensor([0.5673], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5673, 1.0811, 0.4219], dtype=torch.float64)\n",
      "tensor([0.5674], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5674, 1.0745, 0.4141], dtype=torch.float64)\n",
      "tensor([0.5674], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5674, 1.0819, 0.4062], dtype=torch.float64)\n",
      "tensor([0.5674], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5674, 1.0742, 0.3984], dtype=torch.float64)\n",
      "tensor([0.5674], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5674, 1.0688, 0.3906], dtype=torch.float64)\n",
      "tensor([0.5673], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5673, 1.0872, 0.3828], dtype=torch.float64)\n",
      "tensor([0.5674], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5674, 1.0889, 0.3750], dtype=torch.float64)\n",
      "tensor([0.5674], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5674, 1.0854, 0.3672], dtype=torch.float64)\n",
      "tensor([0.5674], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5674, 1.1328, 0.3594], dtype=torch.float64)\n",
      "tensor([0.5677], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5677, 1.1455, 0.3516], dtype=torch.float64)\n",
      "tensor([0.5678], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5678, 1.1228, 0.3438], dtype=torch.float64)\n",
      "tensor([0.5676], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5676, 1.1189, 0.3359], dtype=torch.float64)\n",
      "tensor([0.5676], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5676, 1.1379, 0.3281], dtype=torch.float64)\n",
      "tensor([0.5677], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5677, 1.1367, 0.3203], dtype=torch.float64)\n",
      "tensor([0.5677], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5677, 1.1447, 0.3125], dtype=torch.float64)\n",
      "tensor([0.5677], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5677, 1.1502, 0.3047], dtype=torch.float64)\n",
      "tensor([0.5678], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5678, 1.1570, 0.2969], dtype=torch.float64)\n",
      "tensor([0.5678], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5678, 1.1472, 0.2891], dtype=torch.float64)\n",
      "tensor([0.5677], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5677, 1.1325, 0.2812], dtype=torch.float64)\n",
      "tensor([0.5676], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5676, 1.1606, 0.2734], dtype=torch.float64)\n",
      "tensor([0.5678], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5678, 1.1533, 0.2656], dtype=torch.float64)\n",
      "tensor([0.5677], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5677, 1.1642, 0.2578], dtype=torch.float64)\n",
      "tensor([0.5678], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5678, 1.1621, 0.2500], dtype=torch.float64)\n",
      "tensor([0.5677], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5677, 1.1753, 0.2422], dtype=torch.float64)\n",
      "tensor([0.5678], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5678, 1.1893, 0.2344], dtype=torch.float64)\n",
      "tensor([0.5679], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5679, 1.2072, 0.2266], dtype=torch.float64)\n",
      "tensor([0.5680], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5680, 1.1788, 0.2188], dtype=torch.float64)\n",
      "tensor([0.5678], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5678, 1.1654, 0.2109], dtype=torch.float64)\n",
      "tensor([0.5677], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5677, 1.1597, 0.2031], dtype=torch.float64)\n",
      "tensor([0.5676], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5676, 1.1490, 0.1953], dtype=torch.float64)\n",
      "tensor([0.5675], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5675, 1.1612, 0.1875], dtype=torch.float64)\n",
      "tensor([0.5676], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5676, 1.1549, 0.1797], dtype=torch.float64)\n",
      "tensor([0.5675], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5675, 1.1449, 0.1719], dtype=torch.float64)\n",
      "tensor([0.5674], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5674, 1.1302, 0.1641], dtype=torch.float64)\n",
      "tensor([0.5673], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5673, 1.1268, 0.1562], dtype=torch.float64)\n",
      "tensor([0.5673], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5673, 1.0974, 0.1484], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 1.0928, 0.1406], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 1.0972, 0.1328], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 1.1033, 0.1250], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 1.0946, 0.1172], dtype=torch.float64)\n",
      "tensor([0.5669], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5669, 1.0853, 0.1094], dtype=torch.float64)\n",
      "tensor([0.5668], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5668, 1.0721, 0.1016], dtype=torch.float64)\n",
      "tensor([0.5667], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5667, 1.0765, 0.0938], dtype=torch.float64)\n",
      "tensor([0.5668], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5668, 1.1098, 0.0859], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 1.1139, 0.0781], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 1.1095, 0.0703], dtype=torch.float64)\n",
      "tensor([0.5669], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5669, 1.0972, 0.0625], dtype=torch.float64)\n",
      "tensor([0.5668], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5668, 1.1201, 0.0547], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 1.1136, 0.0469], dtype=torch.float64)\n",
      "tensor([0.5669], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5669, 1.1113, 0.0391], dtype=torch.float64)\n",
      "tensor([0.5669], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5669, 1.1205, 0.0312], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 1.1262, 0.0234], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 1.1425, 0.0156], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 1.1622, 0.0078], dtype=torch.float64)\n",
      "tensor([0.5672], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_result:  1.517092814483762e-05 6.9031849806856e-07\n",
      "loss: [157.88547531]\n",
      "episode:  61\n",
      "loss: [208.23487307]\n",
      "episode:  62\n",
      "loss: [141.01603958]\n",
      "episode:  63\n",
      "loss: [216.83013523]\n",
      "episode:  64\n",
      "loss: [198.15513983]\n",
      "episode:  65\n",
      "loss: [163.19930071]\n",
      "episode:  66\n",
      "loss: [236.65757012]\n",
      "episode:  67\n",
      "loss: [196.68567365]\n",
      "episode:  68\n",
      "loss: [164.24541368]\n",
      "episode:  69\n",
      "loss: [241.69161739]\n",
      "episode:  70\n",
      "loss: [262.21752945]\n",
      "episode:  71\n",
      "loss: [192.80435463]\n",
      "episode:  72\n",
      "loss: [232.67417383]\n",
      "episode:  73\n",
      "loss: [163.0631097]\n",
      "episode:  74\n",
      "loss: [232.27772292]\n",
      "episode:  75\n",
      "loss: [210.19278069]\n",
      "episode:  76\n",
      "loss: [190.6131849]\n",
      "episode:  77\n",
      "loss: [169.08092199]\n",
      "episode:  78\n",
      "loss: [228.61816522]\n",
      "episode:  79\n",
      "loss: [186.6860728]\n",
      "episode:  80\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6003], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6003, 1.0216, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5976], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5976, 1.0228, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5976], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5976, 1.0170, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5976], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5976, 1.0535, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5979], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5979, 1.0465, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5978], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5978, 1.0370, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5978], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5978, 1.0426, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5978], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5978, 1.0493, 0.9375], dtype=torch.float64)\n",
      "tensor([0.5979], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5979, 1.0347, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5977], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5977, 0.9992, 0.9219], dtype=torch.float64)\n",
      "tensor([0.5974], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5974, 0.9936, 0.9141], dtype=torch.float64)\n",
      "tensor([0.5974], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5974, 0.9847, 0.9062], dtype=torch.float64)\n",
      "tensor([0.5973], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5973, 1.0040, 0.8984], dtype=torch.float64)\n",
      "tensor([0.5975], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5975, 0.9963, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5974], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5974, 1.0038, 0.8828], dtype=torch.float64)\n",
      "tensor([0.5975], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5975, 0.9813, 0.8750], dtype=torch.float64)\n",
      "tensor([0.5973], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5973, 0.9907, 0.8672], dtype=torch.float64)\n",
      "tensor([0.5974], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5974, 0.9642, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5971], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5971, 0.9740, 0.8516], dtype=torch.float64)\n",
      "tensor([0.5972], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5972, 0.9847, 0.8438], dtype=torch.float64)\n",
      "tensor([0.5973], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5973, 0.9723, 0.8359], dtype=torch.float64)\n",
      "tensor([0.5972], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5972, 0.9673, 0.8281], dtype=torch.float64)\n",
      "tensor([0.5971], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5971, 0.9638, 0.8203], dtype=torch.float64)\n",
      "tensor([0.5970], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5970, 0.9494, 0.8125], dtype=torch.float64)\n",
      "tensor([0.5969], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5969, 0.9211, 0.8047], dtype=torch.float64)\n",
      "tensor([0.5965], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5965, 0.8987, 0.7969], dtype=torch.float64)\n",
      "tensor([0.5962], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5962, 0.9006, 0.7891], dtype=torch.float64)\n",
      "tensor([0.5962], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5962, 0.8919, 0.7812], dtype=torch.float64)\n",
      "tensor([0.5961], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5961, 0.8858, 0.7734], dtype=torch.float64)\n",
      "tensor([0.5960], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5960, 0.8737, 0.7656], dtype=torch.float64)\n",
      "tensor([0.5959], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5959, 0.8582, 0.7578], dtype=torch.float64)\n",
      "tensor([0.5957], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5957, 0.8806, 0.7500], dtype=torch.float64)\n",
      "tensor([0.5959], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5959, 0.8769, 0.7422], dtype=torch.float64)\n",
      "tensor([0.5959], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5959, 0.8691, 0.7344], dtype=torch.float64)\n",
      "tensor([0.5957], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5957, 0.8610, 0.7266], dtype=torch.float64)\n",
      "tensor([0.5956], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5956, 0.8872, 0.7188], dtype=torch.float64)\n",
      "tensor([0.5959], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5959, 0.8910, 0.7109], dtype=torch.float64)\n",
      "tensor([0.5959], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5959, 0.8954, 0.7031], dtype=torch.float64)\n",
      "tensor([0.5960], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5960, 0.9051, 0.6953], dtype=torch.float64)\n",
      "tensor([0.5961], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5961, 0.8942, 0.6875], dtype=torch.float64)\n",
      "tensor([0.5959], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5959, 0.8885, 0.6797], dtype=torch.float64)\n",
      "tensor([0.5958], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5958, 0.8976, 0.6719], dtype=torch.float64)\n",
      "tensor([0.5959], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5959, 0.8790, 0.6641], dtype=torch.float64)\n",
      "tensor([0.5957], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5957, 0.8734, 0.6562], dtype=torch.float64)\n",
      "tensor([0.5956], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5956, 0.8669, 0.6484], dtype=torch.float64)\n",
      "tensor([0.5955], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5955, 0.8501, 0.6406], dtype=torch.float64)\n",
      "tensor([0.5952], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5952, 0.8441, 0.6328], dtype=torch.float64)\n",
      "tensor([0.5951], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5951, 0.8533, 0.6250], dtype=torch.float64)\n",
      "tensor([0.5952], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5952, 0.8589, 0.6172], dtype=torch.float64)\n",
      "tensor([0.5952], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5952, 0.8684, 0.6094], dtype=torch.float64)\n",
      "tensor([0.5953], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5953, 0.8754, 0.6016], dtype=torch.float64)\n",
      "tensor([0.5953], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5953, 0.8607, 0.5938], dtype=torch.float64)\n",
      "tensor([0.5951], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5951, 0.8469, 0.5859], dtype=torch.float64)\n",
      "tensor([0.5949], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5949, 0.8585, 0.5781], dtype=torch.float64)\n",
      "tensor([0.5950], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5950, 0.8618, 0.5703], dtype=torch.float64)\n",
      "tensor([0.5950], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5950, 0.8578, 0.5625], dtype=torch.float64)\n",
      "tensor([0.5949], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5949, 0.8460, 0.5547], dtype=torch.float64)\n",
      "tensor([0.5947], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5947, 0.8379, 0.5469], dtype=torch.float64)\n",
      "tensor([0.5946], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5946, 0.8364, 0.5391], dtype=torch.float64)\n",
      "tensor([0.5945], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5945, 0.8419, 0.5312], dtype=torch.float64)\n",
      "tensor([0.5945], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5945, 0.8380, 0.5234], dtype=torch.float64)\n",
      "tensor([0.5944], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5944, 0.8294, 0.5156], dtype=torch.float64)\n",
      "tensor([0.5943], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5943, 0.8246, 0.5078], dtype=torch.float64)\n",
      "tensor([0.5942], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5942, 0.8239, 0.5000], dtype=torch.float64)\n",
      "tensor([0.5941], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5941, 0.8429, 0.4922], dtype=torch.float64)\n",
      "tensor([0.5943], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5943, 0.8371, 0.4844], dtype=torch.float64)\n",
      "tensor([0.5942], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5942, 0.8258, 0.4766], dtype=torch.float64)\n",
      "tensor([0.5940], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5940, 0.8315, 0.4688], dtype=torch.float64)\n",
      "tensor([0.5941], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5941, 0.8193, 0.4609], dtype=torch.float64)\n",
      "tensor([0.5939], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5939, 0.8295, 0.4531], dtype=torch.float64)\n",
      "tensor([0.5940], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5940, 0.8328, 0.4453], dtype=torch.float64)\n",
      "tensor([0.5940], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5940, 0.8366, 0.4375], dtype=torch.float64)\n",
      "tensor([0.5940], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5940, 0.8381, 0.4297], dtype=torch.float64)\n",
      "tensor([0.5939], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5939, 0.8412, 0.4219], dtype=torch.float64)\n",
      "tensor([0.5939], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5939, 0.8275, 0.4141], dtype=torch.float64)\n",
      "tensor([0.5937], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5937, 0.8575, 0.4062], dtype=torch.float64)\n",
      "tensor([0.5941], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5941, 0.8343, 0.3984], dtype=torch.float64)\n",
      "tensor([0.5937], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5937, 0.8307, 0.3906], dtype=torch.float64)\n",
      "tensor([0.5937], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5937, 0.8455, 0.3828], dtype=torch.float64)\n",
      "tensor([0.5939], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5939, 0.8494, 0.3750], dtype=torch.float64)\n",
      "tensor([0.5939], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5939, 0.8451, 0.3672], dtype=torch.float64)\n",
      "tensor([0.5938], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5938, 0.8459, 0.3594], dtype=torch.float64)\n",
      "tensor([0.5938], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5938, 0.8556, 0.3516], dtype=torch.float64)\n",
      "tensor([0.5939], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5939, 0.8500, 0.3438], dtype=torch.float64)\n",
      "tensor([0.5938], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5938, 0.8552, 0.3359], dtype=torch.float64)\n",
      "tensor([0.5939], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5939, 0.8371, 0.3281], dtype=torch.float64)\n",
      "tensor([0.5936], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5936, 0.8356, 0.3203], dtype=torch.float64)\n",
      "tensor([0.5935], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5935, 0.8482, 0.3125], dtype=torch.float64)\n",
      "tensor([0.5937], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5937, 0.8499, 0.3047], dtype=torch.float64)\n",
      "tensor([0.5937], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5937, 0.8617, 0.2969], dtype=torch.float64)\n",
      "tensor([0.5938], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5938, 0.8234, 0.2891], dtype=torch.float64)\n",
      "tensor([0.5932], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5932, 0.8116, 0.2812], dtype=torch.float64)\n",
      "tensor([0.5930], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5930, 0.7977, 0.2734], dtype=torch.float64)\n",
      "tensor([0.5928], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5928, 0.7975, 0.2656], dtype=torch.float64)\n",
      "tensor([0.5928], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5928, 0.8044, 0.2578], dtype=torch.float64)\n",
      "tensor([0.5929], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5929, 0.8134, 0.2500], dtype=torch.float64)\n",
      "tensor([0.5930], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5930, 0.8067, 0.2422], dtype=torch.float64)\n",
      "tensor([0.5928], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5928, 0.8111, 0.2344], dtype=torch.float64)\n",
      "tensor([0.5929], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5929, 0.8156, 0.2266], dtype=torch.float64)\n",
      "tensor([0.5929], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5929, 0.7963, 0.2188], dtype=torch.float64)\n",
      "tensor([0.5926], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5926, 0.8149, 0.2109], dtype=torch.float64)\n",
      "tensor([0.5929], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5929, 0.8203, 0.2031], dtype=torch.float64)\n",
      "tensor([0.5929], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5929, 0.8167, 0.1953], dtype=torch.float64)\n",
      "tensor([0.5928], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5928, 0.8116, 0.1875], dtype=torch.float64)\n",
      "tensor([0.5927], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5927, 0.8162, 0.1797], dtype=torch.float64)\n",
      "tensor([0.5928], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5928, 0.8183, 0.1719], dtype=torch.float64)\n",
      "tensor([0.5928], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5928, 0.8182, 0.1641], dtype=torch.float64)\n",
      "tensor([0.5928], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5928, 0.8270, 0.1562], dtype=torch.float64)\n",
      "tensor([0.5929], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5929, 0.8197, 0.1484], dtype=torch.float64)\n",
      "tensor([0.5927], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5927, 0.8460, 0.1406], dtype=torch.float64)\n",
      "tensor([0.5931], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5931, 0.8357, 0.1328], dtype=torch.float64)\n",
      "tensor([0.5929], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5929, 0.8251, 0.1250], dtype=torch.float64)\n",
      "tensor([0.5927], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5927, 0.8211, 0.1172], dtype=torch.float64)\n",
      "tensor([0.5927], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5927, 0.8300, 0.1094], dtype=torch.float64)\n",
      "tensor([0.5928], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5928, 0.8278, 0.1016], dtype=torch.float64)\n",
      "tensor([0.5927], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5927, 0.8379, 0.0938], dtype=torch.float64)\n",
      "tensor([0.5929], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5929, 0.8528, 0.0859], dtype=torch.float64)\n",
      "tensor([0.5931], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5931, 0.8267, 0.0781], dtype=torch.float64)\n",
      "tensor([0.5926], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5926, 0.8198, 0.0703], dtype=torch.float64)\n",
      "tensor([0.5925], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5925, 0.8129, 0.0625], dtype=torch.float64)\n",
      "tensor([0.5924], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5924, 0.8279, 0.0547], dtype=torch.float64)\n",
      "tensor([0.5926], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5926, 0.8122, 0.0469], dtype=torch.float64)\n",
      "tensor([0.5923], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5923, 0.8035, 0.0391], dtype=torch.float64)\n",
      "tensor([0.5922], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5922, 0.8037, 0.0312], dtype=torch.float64)\n",
      "tensor([0.5922], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5922, 0.8209, 0.0234], dtype=torch.float64)\n",
      "tensor([0.5924], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5924, 0.8131, 0.0156], dtype=torch.float64)\n",
      "tensor([0.5923], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5923, 0.8096, 0.0078], dtype=torch.float64)\n",
      "tensor([0.5922], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.7057597142239386e-05 5.376053092067889e-07\n",
      "loss: [182.94378215]\n",
      "episode:  81\n",
      "loss: [233.34737796]\n",
      "episode:  82\n",
      "loss: [278.74024187]\n",
      "episode:  83\n",
      "loss: [222.6963185]\n",
      "episode:  84\n",
      "loss: [183.70179194]\n",
      "episode:  85\n",
      "loss: [171.90195535]\n",
      "episode:  86\n",
      "loss: [188.51105725]\n",
      "episode:  87\n",
      "loss: [214.52750042]\n",
      "episode:  88\n",
      "loss: [202.06358819]\n",
      "episode:  89\n",
      "loss: [152.25120013]\n",
      "episode:  90\n",
      "loss: [176.43166847]\n",
      "episode:  91\n",
      "loss: [238.82966932]\n",
      "episode:  92\n",
      "loss: [256.99873402]\n",
      "episode:  93\n",
      "loss: [236.99562463]\n",
      "episode:  94\n",
      "loss: [274.90021953]\n",
      "episode:  95\n",
      "loss: [135.98252427]\n",
      "episode:  96\n",
      "loss: [168.96289664]\n",
      "episode:  97\n",
      "loss: [237.7424066]\n",
      "episode:  98\n",
      "loss: [120.76923205]\n",
      "episode:  99\n",
      "loss: [259.06804999]\n",
      "episode:  100\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6031], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6031, 0.9839, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5933], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5933, 0.9606, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5927], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5927, 0.9673, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5929], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5929, 0.9846, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5935], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5935, 0.9889, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5936], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5936, 0.9777, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5932], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5932, 0.9802, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5933], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5933, 0.9955, 0.9375], dtype=torch.float64)\n",
      "tensor([0.5938], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5938, 0.9853, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5935], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5935, 0.9942, 0.9219], dtype=torch.float64)\n",
      "tensor([0.5938], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5938, 1.0151, 0.9141], dtype=torch.float64)\n",
      "tensor([0.5944], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5944, 1.0173, 0.9062], dtype=torch.float64)\n",
      "tensor([0.5944], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5944, 0.9943, 0.8984], dtype=torch.float64)\n",
      "tensor([0.5937], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5937, 1.0014, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5940], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5940, 0.9802, 0.8828], dtype=torch.float64)\n",
      "tensor([0.5933], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5933, 0.9548, 0.8750], dtype=torch.float64)\n",
      "tensor([0.5926], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5926, 0.9494, 0.8672], dtype=torch.float64)\n",
      "tensor([0.5924], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5924, 0.9455, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5923], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5923, 0.9466, 0.8516], dtype=torch.float64)\n",
      "tensor([0.5923], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5923, 0.9371, 0.8438], dtype=torch.float64)\n",
      "tensor([0.5920], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5920, 0.9281, 0.8359], dtype=torch.float64)\n",
      "tensor([0.5918], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5918, 0.9617, 0.8281], dtype=torch.float64)\n",
      "tensor([0.5928], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5928, 0.9511, 0.8203], dtype=torch.float64)\n",
      "tensor([0.5925], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5925, 0.9349, 0.8125], dtype=torch.float64)\n",
      "tensor([0.5920], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5920, 0.9147, 0.8047], dtype=torch.float64)\n",
      "tensor([0.5913], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5913, 0.9139, 0.7969], dtype=torch.float64)\n",
      "tensor([0.5913], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5913, 0.8969, 0.7891], dtype=torch.float64)\n",
      "tensor([0.5906], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5906, 0.8791, 0.7812], dtype=torch.float64)\n",
      "tensor([0.5900], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5900, 0.8774, 0.7734], dtype=torch.float64)\n",
      "tensor([0.5899], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5899, 0.8720, 0.7656], dtype=torch.float64)\n",
      "tensor([0.5897], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5897, 0.8559, 0.7578], dtype=torch.float64)\n",
      "tensor([0.5891], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5891, 0.8728, 0.7500], dtype=torch.float64)\n",
      "tensor([0.5897], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5897, 0.8645, 0.7422], dtype=torch.float64)\n",
      "tensor([0.5894], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5894, 0.8957, 0.7344], dtype=torch.float64)\n",
      "tensor([0.5904], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5904, 0.8954, 0.7266], dtype=torch.float64)\n",
      "tensor([0.5903], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5903, 0.8904, 0.7188], dtype=torch.float64)\n",
      "tensor([0.5901], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5901, 0.9036, 0.7109], dtype=torch.float64)\n",
      "tensor([0.5905], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5905, 0.9215, 0.7031], dtype=torch.float64)\n",
      "tensor([0.5911], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5911, 0.9436, 0.6953], dtype=torch.float64)\n",
      "tensor([0.5919], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5919, 0.9425, 0.6875], dtype=torch.float64)\n",
      "tensor([0.5918], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5918, 0.9493, 0.6797], dtype=torch.float64)\n",
      "tensor([0.5921], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5921, 0.9559, 0.6719], dtype=torch.float64)\n",
      "tensor([0.5923], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5923, 0.9555, 0.6641], dtype=torch.float64)\n",
      "tensor([0.5923], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5923, 0.9585, 0.6562], dtype=torch.float64)\n",
      "tensor([0.5924], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5924, 0.9756, 0.6484], dtype=torch.float64)\n",
      "tensor([0.5930], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5930, 0.9973, 0.6406], dtype=torch.float64)\n",
      "tensor([0.5938], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5938, 0.9884, 0.6328], dtype=torch.float64)\n",
      "tensor([0.5934], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5934, 0.9794, 0.6250], dtype=torch.float64)\n",
      "tensor([0.5931], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5931, 0.9815, 0.6172], dtype=torch.float64)\n",
      "tensor([0.5932], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5932, 0.9815, 0.6094], dtype=torch.float64)\n",
      "tensor([0.5932], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5932, 0.9809, 0.6016], dtype=torch.float64)\n",
      "tensor([0.5931], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5931, 0.9927, 0.5938], dtype=torch.float64)\n",
      "tensor([0.5936], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5936, 0.9818, 0.5859], dtype=torch.float64)\n",
      "tensor([0.5931], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5931, 0.9944, 0.5781], dtype=torch.float64)\n",
      "tensor([0.5936], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5936, 1.0064, 0.5703], dtype=torch.float64)\n",
      "tensor([0.5940], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5940, 1.0207, 0.5625], dtype=torch.float64)\n",
      "tensor([0.5946], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5946, 1.0148, 0.5547], dtype=torch.float64)\n",
      "tensor([0.5943], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5943, 1.0090, 0.5469], dtype=torch.float64)\n",
      "tensor([0.5941], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5941, 1.0129, 0.5391], dtype=torch.float64)\n",
      "tensor([0.5942], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5942, 1.0276, 0.5312], dtype=torch.float64)\n",
      "tensor([0.5948], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5948, 1.0393, 0.5234], dtype=torch.float64)\n",
      "tensor([0.5952], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5952, 1.0402, 0.5156], dtype=torch.float64)\n",
      "tensor([0.5952], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5952, 1.0666, 0.5078], dtype=torch.float64)\n",
      "tensor([0.5962], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5962, 1.0944, 0.5000], dtype=torch.float64)\n",
      "tensor([0.5972], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5972, 1.0765, 0.4922], dtype=torch.float64)\n",
      "tensor([0.5965], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5965, 1.0828, 0.4844], dtype=torch.float64)\n",
      "tensor([0.5967], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5967, 1.0821, 0.4766], dtype=torch.float64)\n",
      "tensor([0.5967], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5967, 1.0876, 0.4688], dtype=torch.float64)\n",
      "tensor([0.5969], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5969, 1.1012, 0.4609], dtype=torch.float64)\n",
      "tensor([0.5974], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5974, 1.1350, 0.4531], dtype=torch.float64)\n",
      "tensor([0.5986], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5986, 1.1542, 0.4453], dtype=torch.float64)\n",
      "tensor([0.5994], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5994, 1.1719, 0.4375], dtype=torch.float64)\n",
      "tensor([0.6000], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6000, 1.1928, 0.4297], dtype=torch.float64)\n",
      "tensor([0.6008], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6008, 1.2096, 0.4219], dtype=torch.float64)\n",
      "tensor([0.6015], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6015, 1.2060, 0.4141], dtype=torch.float64)\n",
      "tensor([0.6013], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6013, 1.2211, 0.4062], dtype=torch.float64)\n",
      "tensor([0.6019], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6019, 1.1949, 0.3984], dtype=torch.float64)\n",
      "tensor([0.6009], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6009, 1.1784, 0.3906], dtype=torch.float64)\n",
      "tensor([0.6003], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6003, 1.1618, 0.3828], dtype=torch.float64)\n",
      "tensor([0.5997], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5997, 1.1809, 0.3750], dtype=torch.float64)\n",
      "tensor([0.6004], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6004, 1.1748, 0.3672], dtype=torch.float64)\n",
      "tensor([0.6002], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6002, 1.1850, 0.3594], dtype=torch.float64)\n",
      "tensor([0.6006], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6006, 1.2003, 0.3516], dtype=torch.float64)\n",
      "tensor([0.6012], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6012, 1.2091, 0.3438], dtype=torch.float64)\n",
      "tensor([0.6015], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6015, 1.2290, 0.3359], dtype=torch.float64)\n",
      "tensor([0.6023], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6023, 1.2446, 0.3281], dtype=torch.float64)\n",
      "tensor([0.6029], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6029, 1.2350, 0.3203], dtype=torch.float64)\n",
      "tensor([0.6025], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6025, 1.2487, 0.3125], dtype=torch.float64)\n",
      "tensor([0.6030], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6030, 1.2348, 0.3047], dtype=torch.float64)\n",
      "tensor([0.6025], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6025, 1.2095, 0.2969], dtype=torch.float64)\n",
      "tensor([0.6015], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6015, 1.2354, 0.2891], dtype=torch.float64)\n",
      "tensor([0.6025], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6025, 1.2247, 0.2812], dtype=torch.float64)\n",
      "tensor([0.6021], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6021, 1.2081, 0.2734], dtype=torch.float64)\n",
      "tensor([0.6015], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6015, 1.2180, 0.2656], dtype=torch.float64)\n",
      "tensor([0.6019], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6019, 1.1954, 0.2578], dtype=torch.float64)\n",
      "tensor([0.6010], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6010, 1.2003, 0.2500], dtype=torch.float64)\n",
      "tensor([0.6012], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6012, 1.1745, 0.2422], dtype=torch.float64)\n",
      "tensor([0.6002], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6002, 1.1571, 0.2344], dtype=torch.float64)\n",
      "tensor([0.5996], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5996, 1.1741, 0.2266], dtype=torch.float64)\n",
      "tensor([0.6003], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6003, 1.1722, 0.2188], dtype=torch.float64)\n",
      "tensor([0.6002], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6002, 1.1890, 0.2109], dtype=torch.float64)\n",
      "tensor([0.6008], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6008, 1.2110, 0.2031], dtype=torch.float64)\n",
      "tensor([0.6017], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6017, 1.2205, 0.1953], dtype=torch.float64)\n",
      "tensor([0.6020], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6020, 1.2338, 0.1875], dtype=torch.float64)\n",
      "tensor([0.6025], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6025, 1.2303, 0.1797], dtype=torch.float64)\n",
      "tensor([0.6024], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6024, 1.2151, 0.1719], dtype=torch.float64)\n",
      "tensor([0.6018], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6018, 1.2249, 0.1641], dtype=torch.float64)\n",
      "tensor([0.6022], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6022, 1.2378, 0.1562], dtype=torch.float64)\n",
      "tensor([0.6027], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6027, 1.2321, 0.1484], dtype=torch.float64)\n",
      "tensor([0.6025], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6025, 1.2475, 0.1406], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6031], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6031, 1.2538, 0.1328], dtype=torch.float64)\n",
      "tensor([0.6033], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6033, 1.2381, 0.1250], dtype=torch.float64)\n",
      "tensor([0.6027], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6027, 1.2153, 0.1172], dtype=torch.float64)\n",
      "tensor([0.6019], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6019, 1.2309, 0.1094], dtype=torch.float64)\n",
      "tensor([0.6025], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6025, 1.2344, 0.1016], dtype=torch.float64)\n",
      "tensor([0.6026], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6026, 1.2352, 0.0938], dtype=torch.float64)\n",
      "tensor([0.6026], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6026, 1.2288, 0.0859], dtype=torch.float64)\n",
      "tensor([0.6024], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6024, 1.2287, 0.0781], dtype=torch.float64)\n",
      "tensor([0.6024], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6024, 1.2471, 0.0703], dtype=torch.float64)\n",
      "tensor([0.6031], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6031, 1.2298, 0.0625], dtype=torch.float64)\n",
      "tensor([0.6025], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6025, 1.2210, 0.0547], dtype=torch.float64)\n",
      "tensor([0.6021], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6021, 1.2255, 0.0469], dtype=torch.float64)\n",
      "tensor([0.6023], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6023, 1.2484, 0.0391], dtype=torch.float64)\n",
      "tensor([0.6032], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6032, 1.2406, 0.0312], dtype=torch.float64)\n",
      "tensor([0.6029], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6029, 1.2306, 0.0234], dtype=torch.float64)\n",
      "tensor([0.6025], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6025, 1.2617, 0.0156], dtype=torch.float64)\n",
      "tensor([0.6037], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6037, 1.2732, 0.0078], dtype=torch.float64)\n",
      "tensor([0.6042], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.6538777006136722e-05 5.198345062331744e-07\n",
      "loss: [156.51417311]\n",
      "episode:  101\n",
      "loss: [187.6368798]\n",
      "episode:  102\n",
      "loss: [199.47105116]\n",
      "episode:  103\n",
      "loss: [208.63682849]\n",
      "episode:  104\n",
      "loss: [230.97685596]\n",
      "episode:  105\n",
      "loss: [272.26227795]\n",
      "episode:  106\n",
      "loss: [183.73901961]\n",
      "episode:  107\n",
      "loss: [188.89322932]\n",
      "episode:  108\n",
      "loss: [234.81643903]\n",
      "episode:  109\n",
      "loss: [192.7049229]\n",
      "episode:  110\n",
      "loss: [174.21607097]\n",
      "episode:  111\n",
      "loss: [244.49542213]\n",
      "episode:  112\n",
      "loss: [286.23472251]\n",
      "episode:  113\n",
      "loss: [231.92159164]\n",
      "episode:  114\n",
      "loss: [269.55980377]\n",
      "episode:  115\n",
      "loss: [179.75773149]\n",
      "episode:  116\n",
      "loss: [150.01970119]\n",
      "episode:  117\n",
      "loss: [170.75285854]\n",
      "episode:  118\n",
      "loss: [206.11696112]\n",
      "episode:  119\n",
      "loss: [182.7614397]\n",
      "episode:  120\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.5859], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5859, 1.0142, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5690], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5690, 1.0144, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5699], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5699, 1.0045, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5688], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5688, 0.9855, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5668], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5668, 0.9815, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5665], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5665, 0.9550, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5636], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5636, 0.9568, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5640], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5640, 0.9391, 0.9375], dtype=torch.float64)\n",
      "tensor([0.5619], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5619, 0.9278, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5607], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5607, 0.9298, 0.9219], dtype=torch.float64)\n",
      "tensor([0.5611], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5611, 0.9368, 0.9141], dtype=torch.float64)\n",
      "tensor([0.5619], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5619, 0.9319, 0.9062], dtype=torch.float64)\n",
      "tensor([0.5614], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5614, 0.9365, 0.8984], dtype=torch.float64)\n",
      "tensor([0.5620], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5620, 0.9563, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5644], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5644, 0.9543, 0.8828], dtype=torch.float64)\n",
      "tensor([0.5641], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5641, 0.9637, 0.8750], dtype=torch.float64)\n",
      "tensor([0.5652], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5652, 0.9494, 0.8672], dtype=torch.float64)\n",
      "tensor([0.5636], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5636, 0.9392, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5625], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5625, 0.9540, 0.8516], dtype=torch.float64)\n",
      "tensor([0.5644], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5644, 0.9408, 0.8438], dtype=torch.float64)\n",
      "tensor([0.5627], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5627, 0.9430, 0.8359], dtype=torch.float64)\n",
      "tensor([0.5631], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5631, 0.9238, 0.8281], dtype=torch.float64)\n",
      "tensor([0.5609], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5609, 0.9187, 0.8203], dtype=torch.float64)\n",
      "tensor([0.5604], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5604, 0.9227, 0.8125], dtype=torch.float64)\n",
      "tensor([0.5610], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5610, 0.9235, 0.8047], dtype=torch.float64)\n",
      "tensor([0.5611], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5611, 0.9247, 0.7969], dtype=torch.float64)\n",
      "tensor([0.5613], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5613, 0.9272, 0.7891], dtype=torch.float64)\n",
      "tensor([0.5617], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5617, 0.9258, 0.7812], dtype=torch.float64)\n",
      "tensor([0.5616], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5616, 0.9522, 0.7734], dtype=torch.float64)\n",
      "tensor([0.5651], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5651, 0.9420, 0.7656], dtype=torch.float64)\n",
      "tensor([0.5637], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5637, 0.9459, 0.7578], dtype=torch.float64)\n",
      "tensor([0.5644], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5644, 0.9316, 0.7500], dtype=torch.float64)\n",
      "tensor([0.5624], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5624, 0.9366, 0.7422], dtype=torch.float64)\n",
      "tensor([0.5633], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5633, 0.9238, 0.7344], dtype=torch.float64)\n",
      "tensor([0.5614], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5614, 0.9227, 0.7266], dtype=torch.float64)\n",
      "tensor([0.5614], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5614, 0.9271, 0.7188], dtype=torch.float64)\n",
      "tensor([0.5621], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5621, 0.9321, 0.7109], dtype=torch.float64)\n",
      "tensor([0.5628], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5628, 0.9155, 0.7031], dtype=torch.float64)\n",
      "tensor([0.5604], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5604, 0.9313, 0.6953], dtype=torch.float64)\n",
      "tensor([0.5629], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5629, 0.9356, 0.6875], dtype=torch.float64)\n",
      "tensor([0.5634], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5634, 0.9225, 0.6797], dtype=torch.float64)\n",
      "tensor([0.5615], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5615, 0.9193, 0.6719], dtype=torch.float64)\n",
      "tensor([0.5612], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5612, 0.9345, 0.6641], dtype=torch.float64)\n",
      "tensor([0.5634], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5634, 0.9343, 0.6562], dtype=torch.float64)\n",
      "tensor([0.5633], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5633, 0.9361, 0.6484], dtype=torch.float64)\n",
      "tensor([0.5636], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5636, 0.9229, 0.6406], dtype=torch.float64)\n",
      "tensor([0.5617], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5617, 0.9333, 0.6328], dtype=torch.float64)\n",
      "tensor([0.5634], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5634, 0.9314, 0.6250], dtype=torch.float64)\n",
      "tensor([0.5630], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5630, 0.9434, 0.6172], dtype=torch.float64)\n",
      "tensor([0.5648], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5648, 0.9533, 0.6094], dtype=torch.float64)\n",
      "tensor([0.5661], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5661, 0.9366, 0.6016], dtype=torch.float64)\n",
      "tensor([0.5637], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5637, 0.9376, 0.5938], dtype=torch.float64)\n",
      "tensor([0.5641], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5641, 0.9237, 0.5859], dtype=torch.float64)\n",
      "tensor([0.5621], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5621, 0.9284, 0.5781], dtype=torch.float64)\n",
      "tensor([0.5629], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5629, 0.9236, 0.5703], dtype=torch.float64)\n",
      "tensor([0.5622], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5622, 0.9273, 0.5625], dtype=torch.float64)\n",
      "tensor([0.5628], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5628, 0.9258, 0.5547], dtype=torch.float64)\n",
      "tensor([0.5626], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5626, 0.9194, 0.5469], dtype=torch.float64)\n",
      "tensor([0.5617], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5617, 0.9222, 0.5391], dtype=torch.float64)\n",
      "tensor([0.5622], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5622, 0.9280, 0.5312], dtype=torch.float64)\n",
      "tensor([0.5631], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5631, 0.8931, 0.5234], dtype=torch.float64)\n",
      "tensor([0.5580], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5580, 0.8958, 0.5156], dtype=torch.float64)\n",
      "tensor([0.5588], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5588, 0.8750, 0.5078], dtype=torch.float64)\n",
      "tensor([0.5558], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5558, 0.8720, 0.5000], dtype=torch.float64)\n",
      "tensor([0.5556], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5556, 0.8646, 0.4922], dtype=torch.float64)\n",
      "tensor([0.5546], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5546, 0.8684, 0.4844], dtype=torch.float64)\n",
      "tensor([0.5553], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5553, 0.8882, 0.4766], dtype=torch.float64)\n",
      "tensor([0.5581], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5581, 0.8935, 0.4688], dtype=torch.float64)\n",
      "tensor([0.5587], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5587, 0.8772, 0.4609], dtype=torch.float64)\n",
      "tensor([0.5564], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5564, 0.8724, 0.4531], dtype=torch.float64)\n",
      "tensor([0.5559], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5559, 0.8805, 0.4453], dtype=torch.float64)\n",
      "tensor([0.5571], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5571, 0.8791, 0.4375], dtype=torch.float64)\n",
      "tensor([0.5568], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5568, 0.8678, 0.4297], dtype=torch.float64)\n",
      "tensor([0.5553], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5553, 0.8673, 0.4219], dtype=torch.float64)\n",
      "tensor([0.5554], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5554, 0.8564, 0.4141], dtype=torch.float64)\n",
      "tensor([0.5538], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5538, 0.8390, 0.4062], dtype=torch.float64)\n",
      "tensor([0.5515], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5515, 0.8361, 0.3984], dtype=torch.float64)\n",
      "tensor([0.5512], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5512, 0.8359, 0.3906], dtype=torch.float64)\n",
      "tensor([0.5513], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5513, 0.8436, 0.3828], dtype=torch.float64)\n",
      "tensor([0.5524], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5524, 0.8563, 0.3750], dtype=torch.float64)\n",
      "tensor([0.5542], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5542, 0.8459, 0.3672], dtype=torch.float64)\n",
      "tensor([0.5526], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5526, 0.8353, 0.3594], dtype=torch.float64)\n",
      "tensor([0.5512], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5512, 0.8480, 0.3516], dtype=torch.float64)\n",
      "tensor([0.5532], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5532, 0.8420, 0.3438], dtype=torch.float64)\n",
      "tensor([0.5522], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5522, 0.8375, 0.3359], dtype=torch.float64)\n",
      "tensor([0.5517], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5517, 0.8424, 0.3281], dtype=torch.float64)\n",
      "tensor([0.5525], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5525, 0.8422, 0.3203], dtype=torch.float64)\n",
      "tensor([0.5524], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5524, 0.8656, 0.3125], dtype=torch.float64)\n",
      "tensor([0.5558], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5558, 0.8638, 0.3047], dtype=torch.float64)\n",
      "tensor([0.5554], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5554, 0.8686, 0.2969], dtype=torch.float64)\n",
      "tensor([0.5561], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5561, 0.8809, 0.2891], dtype=torch.float64)\n",
      "tensor([0.5579], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5579, 0.8896, 0.2812], dtype=torch.float64)\n",
      "tensor([0.5590], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5590, 0.8892, 0.2734], dtype=torch.float64)\n",
      "tensor([0.5589], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5589, 0.8716, 0.2656], dtype=torch.float64)\n",
      "tensor([0.5564], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5564, 0.8839, 0.2578], dtype=torch.float64)\n",
      "tensor([0.5584], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5584, 0.8849, 0.2500], dtype=torch.float64)\n",
      "tensor([0.5585], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5585, 0.8942, 0.2422], dtype=torch.float64)\n",
      "tensor([0.5598], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5598, 0.8992, 0.2344], dtype=torch.float64)\n",
      "tensor([0.5605], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5605, 0.9127, 0.2266], dtype=torch.float64)\n",
      "tensor([0.5624], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5624, 0.9065, 0.2188], dtype=torch.float64)\n",
      "tensor([0.5614], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5614, 0.9164, 0.2109], dtype=torch.float64)\n",
      "tensor([0.5630], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5630, 0.9142, 0.2031], dtype=torch.float64)\n",
      "tensor([0.5626], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5626, 0.9191, 0.1953], dtype=torch.float64)\n",
      "tensor([0.5634], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5634, 0.9181, 0.1875], dtype=torch.float64)\n",
      "tensor([0.5632], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5632, 0.9165, 0.1797], dtype=torch.float64)\n",
      "tensor([0.5630], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5630, 0.9153, 0.1719], dtype=torch.float64)\n",
      "tensor([0.5629], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5629, 0.9099, 0.1641], dtype=torch.float64)\n",
      "tensor([0.5622], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5622, 0.8819, 0.1562], dtype=torch.float64)\n",
      "tensor([0.5582], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5582, 0.8878, 0.1484], dtype=torch.float64)\n",
      "tensor([0.5594], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5594, 0.8728, 0.1406], dtype=torch.float64)\n",
      "tensor([0.5572], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5572, 0.8611, 0.1328], dtype=torch.float64)\n",
      "tensor([0.5557], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5557, 0.8544, 0.1250], dtype=torch.float64)\n",
      "tensor([0.5549], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5549, 0.8602, 0.1172], dtype=torch.float64)\n",
      "tensor([0.5558], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5558, 0.8707, 0.1094], dtype=torch.float64)\n",
      "tensor([0.5573], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5573, 0.8729, 0.1016], dtype=torch.float64)\n",
      "tensor([0.5575], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5575, 0.8860, 0.0938], dtype=torch.float64)\n",
      "tensor([0.5594], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5594, 0.8759, 0.0859], dtype=torch.float64)\n",
      "tensor([0.5579], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5579, 0.8783, 0.0781], dtype=torch.float64)\n",
      "tensor([0.5584], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5584, 0.8683, 0.0703], dtype=torch.float64)\n",
      "tensor([0.5569], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5569, 0.8800, 0.0625], dtype=torch.float64)\n",
      "tensor([0.5587], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5587, 0.8767, 0.0547], dtype=torch.float64)\n",
      "tensor([0.5582], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5582, 0.8988, 0.0469], dtype=torch.float64)\n",
      "tensor([0.5614], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5614, 0.8872, 0.0391], dtype=torch.float64)\n",
      "tensor([0.5596], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5596, 0.8900, 0.0312], dtype=torch.float64)\n",
      "tensor([0.5601], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5601, 0.8935, 0.0234], dtype=torch.float64)\n",
      "tensor([0.5606], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5606, 0.8961, 0.0156], dtype=torch.float64)\n",
      "tensor([0.5610], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5610, 0.8821, 0.0078], dtype=torch.float64)\n",
      "tensor([0.5590], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.2317469984923952e-05 6.422673036252319e-07\n",
      "loss: [191.9618412]\n",
      "episode:  121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [161.79714247]\n",
      "episode:  122\n",
      "loss: [181.86592002]\n",
      "episode:  123\n",
      "loss: [230.75804406]\n",
      "episode:  124\n",
      "loss: [173.53453616]\n",
      "episode:  125\n",
      "loss: [213.88243369]\n",
      "episode:  126\n",
      "loss: [119.84357121]\n",
      "episode:  127\n",
      "loss: [178.8039013]\n",
      "episode:  128\n",
      "loss: [199.92361394]\n",
      "episode:  129\n",
      "loss: [174.77188619]\n",
      "episode:  130\n",
      "loss: [186.33378131]\n",
      "episode:  131\n",
      "loss: [170.53041475]\n",
      "episode:  132\n",
      "loss: [200.28425472]\n",
      "episode:  133\n",
      "loss: [154.72869827]\n",
      "episode:  134\n",
      "loss: [142.3505111]\n",
      "episode:  135\n",
      "loss: [182.96691609]\n",
      "episode:  136\n",
      "loss: [155.95138349]\n",
      "episode:  137\n",
      "loss: [154.94295731]\n",
      "episode:  138\n",
      "loss: [173.38530866]\n",
      "episode:  139\n",
      "loss: [173.82897105]\n",
      "episode:  140\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6365], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6365, 0.9913, 0.9922], dtype=torch.float64)\n",
      "tensor([0.4924], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4924, 0.9823, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5282], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5282, 0.9875, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5197], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5197, 1.0063, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5336], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5336, 1.0154, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5345], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5345, 1.0297, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5427], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5427, 1.0583, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5568], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5568, 1.0350, 0.9375], dtype=torch.float64)\n",
      "tensor([0.5387], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5387, 1.0238, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5383], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5383, 1.0325, 0.9219], dtype=torch.float64)\n",
      "tensor([0.5436], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5436, 1.0254, 0.9141], dtype=torch.float64)\n",
      "tensor([0.5378], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5378, 1.0210, 0.9062], dtype=torch.float64)\n",
      "tensor([0.5373], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5373, 1.0040, 0.8984], dtype=torch.float64)\n",
      "tensor([0.5277], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5277, 0.9978, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5273], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5273, 0.9928, 0.8828], dtype=torch.float64)\n",
      "tensor([0.5247], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5247, 0.9903, 0.8750], dtype=torch.float64)\n",
      "tensor([0.5242], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5242, 0.9952, 0.8672], dtype=torch.float64)\n",
      "tensor([0.5274], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5274, 1.0094, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5347], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5347, 1.0053, 0.8516], dtype=torch.float64)\n",
      "tensor([0.5301], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5301, 1.0215, 0.8438], dtype=torch.float64)\n",
      "tensor([0.5417], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5417, 1.0271, 0.8359], dtype=torch.float64)\n",
      "tensor([0.5418], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5418, 1.0142, 0.8281], dtype=torch.float64)\n",
      "tensor([0.5340], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5340, 1.0227, 0.8203], dtype=torch.float64)\n",
      "tensor([0.5427], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5427, 1.0100, 0.8125], dtype=torch.float64)\n",
      "tensor([0.5321], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5321, 1.0090, 0.8047], dtype=torch.float64)\n",
      "tensor([0.5357], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5357, 1.0147, 0.7969], dtype=torch.float64)\n",
      "tensor([0.5387], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5387, 1.0214, 0.7891], dtype=torch.float64)\n",
      "tensor([0.5425], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5425, 1.0176, 0.7812], dtype=torch.float64)\n",
      "tensor([0.5393], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5393, 1.0135, 0.7734], dtype=torch.float64)\n",
      "tensor([0.5383], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5383, 1.0245, 0.7656], dtype=torch.float64)\n",
      "tensor([0.5463], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5463, 1.0558, 0.7578], dtype=torch.float64)\n",
      "tensor([0.5643], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5643, 1.0387, 0.7500], dtype=torch.float64)\n",
      "tensor([0.5475], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5475, 1.0516, 0.7422], dtype=torch.float64)\n",
      "tensor([0.5623], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5623, 1.0467, 0.7344], dtype=torch.float64)\n",
      "tensor([0.5545], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5545, 1.0555, 0.7266], dtype=torch.float64)\n",
      "tensor([0.5635], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5635, 1.0689, 0.7188], dtype=torch.float64)\n",
      "tensor([0.5695], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5695, 1.0635, 0.7109], dtype=torch.float64)\n",
      "tensor([0.5645], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5645, 1.0539, 0.7031], dtype=torch.float64)\n",
      "tensor([0.5606], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5606, 1.0637, 0.6953], dtype=torch.float64)\n",
      "tensor([0.5688], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5688, 1.0638, 0.6875], dtype=torch.float64)\n",
      "tensor([0.5665], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5665, 1.0422, 0.6797], dtype=torch.float64)\n",
      "tensor([0.5540], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5540, 1.0396, 0.6719], dtype=torch.float64)\n",
      "tensor([0.5573], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5573, 1.0409, 0.6641], dtype=torch.float64)\n",
      "tensor([0.5575], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5575, 1.0415, 0.6562], dtype=torch.float64)\n",
      "tensor([0.5584], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5584, 1.0236, 0.6484], dtype=torch.float64)\n",
      "tensor([0.5471], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5471, 1.0089, 0.6406], dtype=torch.float64)\n",
      "tensor([0.5421], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5421, 1.0199, 0.6328], dtype=torch.float64)\n",
      "tensor([0.5515], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5515, 1.0401, 0.6250], dtype=torch.float64)\n",
      "tensor([0.5618], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5618, 1.0519, 0.6172], dtype=torch.float64)\n",
      "tensor([0.5664], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5664, 1.0695, 0.6094], dtype=torch.float64)\n",
      "tensor([0.5767], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5767, 1.0453, 0.6016], dtype=torch.float64)\n",
      "tensor([0.5580], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5580, 1.0467, 0.5938], dtype=torch.float64)\n",
      "tensor([0.5660], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5660, 1.0340, 0.5859], dtype=torch.float64)\n",
      "tensor([0.5556], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5556, 1.0379, 0.5781], dtype=torch.float64)\n",
      "tensor([0.5623], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5623, 1.0376, 0.5703], dtype=torch.float64)\n",
      "tensor([0.5603], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5603, 1.0705, 0.5625], dtype=torch.float64)\n",
      "tensor([0.5828], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5828, 1.0739, 0.5547], dtype=torch.float64)\n",
      "tensor([0.5777], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5777, 1.0341, 0.5469], dtype=torch.float64)\n",
      "tensor([0.5544], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5544, 1.0358, 0.5391], dtype=torch.float64)\n",
      "tensor([0.5641], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5641, 1.0181, 0.5312], dtype=torch.float64)\n",
      "tensor([0.5499], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5499, 1.0316, 0.5234], dtype=torch.float64)\n",
      "tensor([0.5641], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5641, 1.0204, 0.5156], dtype=torch.float64)\n",
      "tensor([0.5525], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5525, 1.0261, 0.5078], dtype=torch.float64)\n",
      "tensor([0.5608], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5608, 1.0269, 0.5000], dtype=torch.float64)\n",
      "tensor([0.5590], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5590, 1.0172, 0.4922], dtype=torch.float64)\n",
      "tensor([0.5539], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5539, 1.0508, 0.4844], dtype=torch.float64)\n",
      "tensor([0.5779], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5779, 1.0740, 0.4766], dtype=torch.float64)\n",
      "tensor([0.5850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5850, 1.0566, 0.4688], dtype=torch.float64)\n",
      "tensor([0.5718], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5718, 1.0315, 0.4609], dtype=torch.float64)\n",
      "tensor([0.5608], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5608, 1.0386, 0.4531], dtype=torch.float64)\n",
      "tensor([0.5698], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5698, 1.0482, 0.4453], dtype=torch.float64)\n",
      "tensor([0.5734], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5734, 1.0677, 0.4375], dtype=torch.float64)\n",
      "tensor([0.5853], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5853, 1.1044, 0.4297], dtype=torch.float64)\n",
      "tensor([0.6049], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6049, 1.1188, 0.4219], dtype=torch.float64)\n",
      "tensor([0.6078], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6078, 1.1024, 0.4141], dtype=torch.float64)\n",
      "tensor([0.5972], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5972, 1.0923, 0.4062], dtype=torch.float64)\n",
      "tensor([0.5950], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5950, 1.1068, 0.3984], dtype=torch.float64)\n",
      "tensor([0.6052], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6052, 1.0970, 0.3906], dtype=torch.float64)\n",
      "tensor([0.5963], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5963, 1.1047, 0.3828], dtype=torch.float64)\n",
      "tensor([0.6046], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6046, 1.1133, 0.3750], dtype=torch.float64)\n",
      "tensor([0.6077], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6077, 1.1041, 0.3672], dtype=torch.float64)\n",
      "tensor([0.6015], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6015, 1.0904, 0.3594], dtype=torch.float64)\n",
      "tensor([0.5956], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5956, 1.0919, 0.3516], dtype=torch.float64)\n",
      "tensor([0.5990], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5990, 1.0955, 0.3438], dtype=torch.float64)\n",
      "tensor([0.6006], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6006, 1.0989, 0.3359], dtype=torch.float64)\n",
      "tensor([0.6027], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6027, 1.1088, 0.3281], dtype=torch.float64)\n",
      "tensor([0.6087], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6087, 1.1087, 0.3203], dtype=torch.float64)\n",
      "tensor([0.6072], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6072, 1.1250, 0.3125], dtype=torch.float64)\n",
      "tensor([0.6183], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6183, 1.1362, 0.3047], dtype=torch.float64)\n",
      "tensor([0.6220], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6220, 1.1099, 0.2969], dtype=torch.float64)\n",
      "tensor([0.6050], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6050, 1.0982, 0.2891], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6040], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6040, 1.0905, 0.2812], dtype=torch.float64)\n",
      "tensor([0.6001], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6001, 1.0987, 0.2734], dtype=torch.float64)\n",
      "tensor([0.6070], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6070, 1.1004, 0.2656], dtype=torch.float64)\n",
      "tensor([0.6063], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6063, 1.1073, 0.2578], dtype=torch.float64)\n",
      "tensor([0.6114], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6114, 1.1249, 0.2500], dtype=torch.float64)\n",
      "tensor([0.6211], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6211, 1.1418, 0.2422], dtype=torch.float64)\n",
      "tensor([0.6289], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6289, 1.1645, 0.2344], dtype=torch.float64)\n",
      "tensor([0.6410], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6410, 1.1589, 0.2266], dtype=torch.float64)\n",
      "tensor([0.6340], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6340, 1.1566, 0.2188], dtype=torch.float64)\n",
      "tensor([0.6355], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6355, 1.1491, 0.2109], dtype=torch.float64)\n",
      "tensor([0.6308], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6308, 1.1471, 0.2031], dtype=torch.float64)\n",
      "tensor([0.6317], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6317, 1.1594, 0.1953], dtype=torch.float64)\n",
      "tensor([0.6396], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6396, 1.1569, 0.1875], dtype=torch.float64)\n",
      "tensor([0.6359], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6359, 1.1531, 0.1797], dtype=torch.float64)\n",
      "tensor([0.6354], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6354, 1.1453, 0.1719], dtype=torch.float64)\n",
      "tensor([0.6311], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6311, 1.1397, 0.1641], dtype=torch.float64)\n",
      "tensor([0.6296], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6296, 1.1529, 0.1562], dtype=torch.float64)\n",
      "tensor([0.6389], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6389, 1.1890, 0.1484], dtype=torch.float64)\n",
      "tensor([0.6586], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6586, 1.1993, 0.1406], dtype=torch.float64)\n",
      "tensor([0.6589], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6589, 1.2174, 0.1328], dtype=torch.float64)\n",
      "tensor([0.6680], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6680, 1.2365, 0.1250], dtype=torch.float64)\n",
      "tensor([0.6749], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6749, 1.2136, 0.1172], dtype=torch.float64)\n",
      "tensor([0.6631], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6631, 1.2276, 0.1094], dtype=torch.float64)\n",
      "tensor([0.6728], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6728, 1.2231, 0.1016], dtype=torch.float64)\n",
      "tensor([0.6688], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6688, 1.2528, 0.0938], dtype=torch.float64)\n",
      "tensor([0.6838], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6838, 1.2675, 0.0859], dtype=torch.float64)\n",
      "tensor([0.6872], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6872, 1.2691, 0.0781], dtype=torch.float64)\n",
      "tensor([0.6874], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6874, 1.2825, 0.0703], dtype=torch.float64)\n",
      "tensor([0.6940], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6940, 1.2670, 0.0625], dtype=torch.float64)\n",
      "tensor([0.6856], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6856, 1.2687, 0.0547], dtype=torch.float64)\n",
      "tensor([0.6889], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6889, 1.2844, 0.0469], dtype=torch.float64)\n",
      "tensor([0.6957], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6957, 1.2980, 0.0391], dtype=torch.float64)\n",
      "tensor([0.7006], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7006, 1.3135, 0.0312], dtype=torch.float64)\n",
      "tensor([0.7068], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7068, 1.3283, 0.0234], dtype=torch.float64)\n",
      "tensor([0.7121], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7121, 1.3417, 0.0156], dtype=torch.float64)\n",
      "tensor([0.7170], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7170, 1.3287, 0.0078], dtype=torch.float64)\n",
      "tensor([0.7107], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.0676666278434035e-05 6.047248011322186e-07\n",
      "loss: [115.54949104]\n",
      "episode:  141\n",
      "loss: [145.72333106]\n",
      "episode:  142\n",
      "loss: [165.32100112]\n",
      "episode:  143\n",
      "loss: [153.18428839]\n",
      "episode:  144\n",
      "loss: [128.65315656]\n",
      "episode:  145\n",
      "loss: [152.61795873]\n",
      "episode:  146\n",
      "loss: [114.50817419]\n",
      "episode:  147\n",
      "loss: [128.93714569]\n",
      "episode:  148\n",
      "loss: [133.90776367]\n",
      "episode:  149\n",
      "loss: [119.92989719]\n",
      "episode:  150\n",
      "loss: [144.52744339]\n",
      "episode:  151\n",
      "loss: [138.28532254]\n",
      "episode:  152\n",
      "loss: [306.6306793]\n",
      "episode:  153\n",
      "loss: [207.52790335]\n",
      "episode:  154\n",
      "loss: [151.92194672]\n",
      "episode:  155\n",
      "loss: [127.18931084]\n",
      "episode:  156\n",
      "loss: [97.04172732]\n",
      "episode:  157\n",
      "loss: [100.82628143]\n",
      "episode:  158\n",
      "loss: [70.06640593]\n",
      "episode:  159\n",
      "loss: [90.43867328]\n",
      "episode:  160\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6638], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6638, 1.0295, 0.9922], dtype=torch.float64)\n",
      "tensor([0.6111], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6111, 1.0305, 0.9844], dtype=torch.float64)\n",
      "tensor([0.6198], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6198, 1.0132, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5889], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5889, 1.0336, 0.9688], dtype=torch.float64)\n",
      "tensor([0.6356], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6356, 1.0396, 0.9609], dtype=torch.float64)\n",
      "tensor([0.6307], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6307, 1.0390, 0.9531], dtype=torch.float64)\n",
      "tensor([0.6316], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6316, 1.0357, 0.9453], dtype=torch.float64)\n",
      "tensor([0.6230], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6230, 1.0234, 0.9375], dtype=torch.float64)\n",
      "tensor([0.5979], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5979, 1.0089, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5783], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5783, 1.0149, 0.9219], dtype=torch.float64)\n",
      "tensor([0.6022], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6022, 1.0059, 0.9141], dtype=torch.float64)\n",
      "tensor([0.5664], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5664, 0.9927, 0.9062], dtype=torch.float64)\n",
      "tensor([0.5568], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5568, 0.9920, 0.8984], dtype=torch.float64)\n",
      "tensor([0.5604], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5604, 0.9723, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5096], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5096, 0.9802, 0.8828], dtype=torch.float64)\n",
      "tensor([0.5567], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5567, 0.9947, 0.8750], dtype=torch.float64)\n",
      "tensor([0.5642], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5642, 0.9983, 0.8672], dtype=torch.float64)\n",
      "tensor([0.5672], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5672, 0.9832, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5279], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5279, 0.9824, 0.8516], dtype=torch.float64)\n",
      "tensor([0.5506], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5506, 0.9967, 0.8438], dtype=torch.float64)\n",
      "tensor([0.5694], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5694, 0.9974, 0.8359], dtype=torch.float64)\n",
      "tensor([0.5579], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5579, 0.9872, 0.8281], dtype=torch.float64)\n",
      "tensor([0.5398], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5398, 0.9987, 0.8203], dtype=torch.float64)\n",
      "tensor([0.5804], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5804, 0.9975, 0.8125], dtype=torch.float64)\n",
      "tensor([0.5495], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5495, 0.9936, 0.8047], dtype=torch.float64)\n",
      "tensor([0.5611], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5611, 0.9802, 0.7969], dtype=torch.float64)\n",
      "tensor([0.5181], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5181, 0.9886, 0.7891], dtype=torch.float64)\n",
      "tensor([0.5709], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5709, 0.9865, 0.7812], dtype=torch.float64)\n",
      "tensor([0.5293], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5293, 0.9938, 0.7734], dtype=torch.float64)\n",
      "tensor([0.5781], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5781, 0.9875, 0.7656], dtype=torch.float64)\n",
      "tensor([0.5281], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5281, 0.9834, 0.7578], dtype=torch.float64)\n",
      "tensor([0.5526], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5526, 0.9778, 0.7500], dtype=torch.float64)\n",
      "tensor([0.5212], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5212, 0.9812, 0.7422], dtype=torch.float64)\n",
      "tensor([0.5525], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5525, 0.9713, 0.7344], dtype=torch.float64)\n",
      "tensor([0.5054], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5054, 0.9745, 0.7266], dtype=torch.float64)\n",
      "tensor([0.5471], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5471, 0.9694, 0.7188], dtype=torch.float64)\n",
      "tensor([0.5051], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5051, 0.9616, 0.7109], dtype=torch.float64)\n",
      "tensor([0.5141], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5141, 0.9516, 0.7031], dtype=torch.float64)\n",
      "tensor([0.4819], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4819, 0.9395, 0.6953], dtype=torch.float64)\n",
      "tensor([0.4726], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4726, 0.9429, 0.6875], dtype=torch.float64)\n",
      "tensor([0.4887], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4887, 0.9627, 0.6797], dtype=torch.float64)\n",
      "tensor([0.5308], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5308, 0.9643, 0.6719], dtype=torch.float64)\n",
      "tensor([0.5065], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5065, 0.9686, 0.6641], dtype=torch.float64)\n",
      "tensor([0.5354], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5354, 0.9464, 0.6562], dtype=torch.float64)\n",
      "tensor([0.4607], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4607, 0.9428, 0.6484], dtype=torch.float64)\n",
      "tensor([0.4998], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4998, 0.9374, 0.6406], dtype=torch.float64)\n",
      "tensor([0.4595], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4595, 0.9500, 0.6328], dtype=torch.float64)\n",
      "tensor([0.5209], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5209, 0.9475, 0.6250], dtype=torch.float64)\n",
      "tensor([0.4724], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4724, 0.9600, 0.6172], dtype=torch.float64)\n",
      "tensor([0.5396], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5396, 0.9715, 0.6094], dtype=torch.float64)\n",
      "tensor([0.5242], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5242, 0.9788, 0.6016], dtype=torch.float64)\n",
      "tensor([0.5549], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5549, 0.9785, 0.5938], dtype=torch.float64)\n",
      "tensor([0.5334], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5334, 0.9912, 0.5859], dtype=torch.float64)\n",
      "tensor([0.5824], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5824, 0.9952, 0.5781], dtype=torch.float64)\n",
      "tensor([0.5596], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5596, 0.9872, 0.5703], dtype=torch.float64)\n",
      "tensor([0.5548], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5548, 0.9831, 0.5625], dtype=torch.float64)\n",
      "tensor([0.5479], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5479, 1.0105, 0.5547], dtype=torch.float64)\n",
      "tensor([0.6259], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6259, 1.0355, 0.5469], dtype=torch.float64)\n",
      "tensor([0.6390], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6390, 1.0258, 0.5391], dtype=torch.float64)\n",
      "tensor([0.6046], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6046, 1.0413, 0.5312], dtype=torch.float64)\n",
      "tensor([0.6702], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6702, 1.0421, 0.5234], dtype=torch.float64)\n",
      "tensor([0.6274], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6274, 1.0190, 0.5156], dtype=torch.float64)\n",
      "tensor([0.5964], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5964, 1.0201, 0.5078], dtype=torch.float64)\n",
      "tensor([0.6214], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6214, 1.0083, 0.5000], dtype=torch.float64)\n",
      "tensor([0.5733], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5733, 0.9988, 0.4922], dtype=torch.float64)\n",
      "tensor([0.5820], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5820, 1.0013, 0.4844], dtype=torch.float64)\n",
      "tensor([0.5831], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5831, 0.9985, 0.4766], dtype=torch.float64)\n",
      "tensor([0.5756], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5756, 0.9975, 0.4688], dtype=torch.float64)\n",
      "tensor([0.5787], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5787, 0.9617, 0.4609], dtype=torch.float64)\n",
      "tensor([0.4875], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4875, 0.9771, 0.4531], dtype=torch.float64)\n",
      "tensor([0.5869], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5869, 0.9826, 0.4453], dtype=torch.float64)\n",
      "tensor([0.5331], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5331, 0.9899, 0.4375], dtype=torch.float64)\n",
      "tensor([0.5903], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5903, 0.9981, 0.4297], dtype=torch.float64)\n",
      "tensor([0.5732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5732, 0.9921, 0.4219], dtype=torch.float64)\n",
      "tensor([0.5697], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5697, 0.9861, 0.4141], dtype=torch.float64)\n",
      "tensor([0.5568], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5568, 0.9743, 0.4062], dtype=torch.float64)\n",
      "tensor([0.5350], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5350, 0.9719, 0.3984], dtype=torch.float64)\n",
      "tensor([0.5442], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5442, 0.9619, 0.3906], dtype=torch.float64)\n",
      "tensor([0.5121], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5121, 0.9496, 0.3828], dtype=torch.float64)\n",
      "tensor([0.5021], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5021, 0.9627, 0.3750], dtype=torch.float64)\n",
      "tensor([0.5444], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5444, 0.9730, 0.3672], dtype=torch.float64)\n",
      "tensor([0.5432], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5432, 0.9860, 0.3594], dtype=torch.float64)\n",
      "tensor([0.5791], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5791, 0.9934, 0.3516], dtype=torch.float64)\n",
      "tensor([0.5743], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5743, 1.0062, 0.3438], dtype=torch.float64)\n",
      "tensor([0.6123], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6123, 1.0208, 0.3359], dtype=torch.float64)\n",
      "tensor([0.6251], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6251, 1.0040, 0.3281], dtype=torch.float64)\n",
      "tensor([0.5725], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5725, 0.9785, 0.3203], dtype=torch.float64)\n",
      "tensor([0.5416], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5416, 0.9679, 0.3125], dtype=torch.float64)\n",
      "tensor([0.5355], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5355, 0.9569, 0.3047], dtype=torch.float64)\n",
      "tensor([0.5113], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5113, 0.9690, 0.2969], dtype=torch.float64)\n",
      "tensor([0.5606], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5606, 0.9647, 0.2891], dtype=torch.float64)\n",
      "tensor([0.5174], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5174, 0.9744, 0.2812], dtype=torch.float64)\n",
      "tensor([0.5720], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5720, 0.9845, 0.2734], dtype=torch.float64)\n",
      "tensor([0.5615], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5615, 0.9889, 0.2656], dtype=torch.float64)\n",
      "tensor([0.5810], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5810, 0.9674, 0.2578], dtype=torch.float64)\n",
      "tensor([0.5155], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5155, 0.9699, 0.2500], dtype=torch.float64)\n",
      "tensor([0.5636], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5636, 0.9589, 0.2422], dtype=torch.float64)\n",
      "tensor([0.5062], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5062, 0.9657, 0.2344], dtype=torch.float64)\n",
      "tensor([0.5602], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5602, 0.9726, 0.2266], dtype=torch.float64)\n",
      "tensor([0.5417], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5417, 0.9776, 0.2188], dtype=torch.float64)\n",
      "tensor([0.5682], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5682, 0.9875, 0.2109], dtype=torch.float64)\n",
      "tensor([0.5767], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5767, 0.9787, 0.2031], dtype=torch.float64)\n",
      "tensor([0.5481], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5481, 0.9657, 0.1953], dtype=torch.float64)\n",
      "tensor([0.5341], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5341, 0.9644, 0.1875], dtype=torch.float64)\n",
      "tensor([0.5410], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5410, 0.9574, 0.1797], dtype=torch.float64)\n",
      "tensor([0.5194], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5194, 0.9551, 0.1719], dtype=torch.float64)\n",
      "tensor([0.5275], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5275, 0.9772, 0.1641], dtype=torch.float64)\n",
      "tensor([0.5811], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5811, 0.9685, 0.1562], dtype=torch.float64)\n",
      "tensor([0.5261], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5261, 0.9653, 0.1484], dtype=torch.float64)\n",
      "tensor([0.5518], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5518, 0.9775, 0.1406], dtype=torch.float64)\n",
      "tensor([0.5671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5671, 0.9786, 0.1328], dtype=torch.float64)\n",
      "tensor([0.5600], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5600, 0.9715, 0.1250], dtype=torch.float64)\n",
      "tensor([0.5466], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5466, 0.9397, 0.1172], dtype=torch.float64)\n",
      "tensor([0.4811], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4811, 0.9379, 0.1094], dtype=torch.float64)\n",
      "tensor([0.5131], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5131, 0.9591, 0.1016], dtype=torch.float64)\n",
      "tensor([0.5478], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5478, 0.9302, 0.0938], dtype=torch.float64)\n",
      "tensor([0.4615], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4615, 0.9193, 0.0859], dtype=torch.float64)\n",
      "tensor([0.4804], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4804, 0.9160, 0.0781], dtype=torch.float64)\n",
      "tensor([0.4639], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4639, 0.9431, 0.0703], dtype=torch.float64)\n",
      "tensor([0.5417], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5417, 0.9310, 0.0625], dtype=torch.float64)\n",
      "tensor([0.4681], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4681, 0.9454, 0.0547], dtype=torch.float64)\n",
      "tensor([0.5463], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5463, 0.9670, 0.0469], dtype=torch.float64)\n",
      "tensor([0.5499], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5499, 0.9516, 0.0391], dtype=torch.float64)\n",
      "tensor([0.5129], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5129, 0.9513, 0.0312], dtype=torch.float64)\n",
      "tensor([0.5325], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5325, 0.9448, 0.0234], dtype=torch.float64)\n",
      "tensor([0.5075], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5075, 0.9460, 0.0156], dtype=torch.float64)\n",
      "tensor([0.5236], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5236, 0.9393, 0.0078], dtype=torch.float64)\n",
      "tensor([0.5007], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  5.1738718282718396e-06 5.629428410339467e-07\n",
      "loss: [69.20812033]\n",
      "episode:  161\n",
      "loss: [88.31876399]\n",
      "episode:  162\n",
      "loss: [98.97482819]\n",
      "episode:  163\n",
      "loss: [83.20015959]\n",
      "episode:  164\n",
      "loss: [55.02243309]\n",
      "episode:  165\n",
      "loss: [46.6619652]\n",
      "episode:  166\n",
      "loss: [57.87820507]\n",
      "episode:  167\n",
      "loss: [57.82232682]\n",
      "episode:  168\n",
      "loss: [39.15886168]\n",
      "episode:  169\n",
      "loss: [63.33492048]\n",
      "episode:  170\n",
      "loss: [42.52611234]\n",
      "episode:  171\n",
      "loss: [37.14801679]\n",
      "episode:  172\n",
      "loss: [48.64909082]\n",
      "episode:  173\n",
      "loss: [58.21411843]\n",
      "episode:  174\n",
      "loss: [35.98517759]\n",
      "episode:  175\n",
      "loss: [65.67835684]\n",
      "episode:  176\n",
      "loss: [27.44513316]\n",
      "episode:  177\n",
      "loss: [38.98473422]\n",
      "episode:  178\n",
      "loss: [38.15522102]\n",
      "episode:  179\n",
      "loss: [37.6779462]\n",
      "episode:  180\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.2528], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2528, 0.9823, 0.9922], dtype=torch.float64)\n",
      "tensor([0.3741], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3741, 0.9652, 0.9844], dtype=torch.float64)\n",
      "tensor([0.4045], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4045, 0.9797, 0.9766], dtype=torch.float64)\n",
      "tensor([0.4482], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4482, 0.9737, 0.9688], dtype=torch.float64)\n",
      "tensor([0.4411], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4411, 0.9792, 0.9609], dtype=torch.float64)\n",
      "tensor([0.4535], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4535, 0.9802, 0.9531], dtype=torch.float64)\n",
      "tensor([0.4581], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4581, 0.9878, 0.9453], dtype=torch.float64)\n",
      "tensor([0.4783], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4783, 0.9938, 0.9375], dtype=torch.float64)\n",
      "tensor([0.4976], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4976, 0.9923, 0.9297], dtype=torch.float64)\n",
      "tensor([0.4922], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4922, 0.9838, 0.9219], dtype=torch.float64)\n",
      "tensor([0.4655], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4655, 0.9989, 0.9141], dtype=torch.float64)\n",
      "tensor([0.5065], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5065, 1.0056, 0.9062], dtype=torch.float64)\n",
      "tensor([0.5251], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5251, 1.0284, 0.8984], dtype=torch.float64)\n",
      "tensor([0.5888], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5888, 1.0123, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5411], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5411, 1.0279, 0.8828], dtype=torch.float64)\n",
      "tensor([0.5871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5871, 1.0358, 0.8750], dtype=torch.float64)\n",
      "tensor([0.6026], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6026, 1.0388, 0.8672], dtype=torch.float64)\n",
      "tensor([0.6079], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6079, 1.0504, 0.8594], dtype=torch.float64)\n",
      "tensor([0.6288], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6288, 1.0447, 0.8516], dtype=torch.float64)\n",
      "tensor([0.6180], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6180, 1.0608, 0.8438], dtype=torch.float64)\n",
      "tensor([0.6472], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6472, 1.0548, 0.8359], dtype=torch.float64)\n",
      "tensor([0.6358], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6358, 1.0647, 0.8281], dtype=torch.float64)\n",
      "tensor([0.6537], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6537, 1.0588, 0.8203], dtype=torch.float64)\n",
      "tensor([0.6424], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6424, 1.0558, 0.8125], dtype=torch.float64)\n",
      "tensor([0.6369], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6369, 1.0692, 0.8047], dtype=torch.float64)\n",
      "tensor([0.6611], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6611, 1.0791, 0.7969], dtype=torch.float64)\n",
      "tensor([0.6786], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6786, 1.0903, 0.7891], dtype=torch.float64)\n",
      "tensor([0.6972], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6972, 1.0960, 0.7812], dtype=torch.float64)\n",
      "tensor([0.7067], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7067, 1.1058, 0.7734], dtype=torch.float64)\n",
      "tensor([0.7231], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7231, 1.1116, 0.7656], dtype=torch.float64)\n",
      "tensor([0.7326], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7326, 1.1221, 0.7578], dtype=torch.float64)\n",
      "tensor([0.7501], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7501, 1.1396, 0.7500], dtype=torch.float64)\n",
      "tensor([0.7796], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7796, 1.1372, 0.7422], dtype=torch.float64)\n",
      "tensor([0.7751], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7751, 1.1642, 0.7344], dtype=torch.float64)\n",
      "tensor([0.8209], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8209, 1.1644, 0.7266], dtype=torch.float64)\n",
      "tensor([0.8207], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8207, 1.1431, 0.7188], dtype=torch.float64)\n",
      "tensor([0.7841], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7841, 1.1729, 0.7109], dtype=torch.float64)\n",
      "tensor([0.8350], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8350, 1.1515, 0.7031], dtype=torch.float64)\n",
      "tensor([0.7977], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7977, 1.1576, 0.6953], dtype=torch.float64)\n",
      "tensor([0.8084], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8084, 1.1610, 0.6875], dtype=torch.float64)\n",
      "tensor([0.8137], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8137, 1.1514, 0.6797], dtype=torch.float64)\n",
      "tensor([0.7973], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7973, 1.1346, 0.6719], dtype=torch.float64)\n",
      "tensor([0.7685], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7685, 1.1296, 0.6641], dtype=torch.float64)\n",
      "tensor([0.7599], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7599, 1.0996, 0.6562], dtype=torch.float64)\n",
      "tensor([0.7086], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7086, 1.0996, 0.6484], dtype=torch.float64)\n",
      "tensor([0.7090], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7090, 1.1020, 0.6406], dtype=torch.float64)\n",
      "tensor([0.7129], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7129, 1.0938, 0.6328], dtype=torch.float64)\n",
      "tensor([0.6985], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6985, 1.0941, 0.6250], dtype=torch.float64)\n",
      "tensor([0.6990], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6990, 1.0744, 0.6172], dtype=torch.float64)\n",
      "tensor([0.6644], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6644, 1.0899, 0.6094], dtype=torch.float64)\n",
      "tensor([0.6917], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6917, 1.0948, 0.6016], dtype=torch.float64)\n",
      "tensor([0.6996], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6996, 1.0898, 0.5938], dtype=torch.float64)\n",
      "tensor([0.6907], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6907, 1.1169, 0.5859], dtype=torch.float64)\n",
      "tensor([0.7368], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7368, 1.1034, 0.5781], dtype=torch.float64)\n",
      "tensor([0.7133], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7133, 1.0993, 0.5703], dtype=torch.float64)\n",
      "tensor([0.7061], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7061, 1.1159, 0.5625], dtype=torch.float64)\n",
      "tensor([0.7344], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7344, 1.1404, 0.5547], dtype=torch.float64)\n",
      "tensor([0.7756], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7756, 1.1262, 0.5469], dtype=torch.float64)\n",
      "tensor([0.7508], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7508, 1.1337, 0.5391], dtype=torch.float64)\n",
      "tensor([0.7635], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7635, 1.1128, 0.5312], dtype=torch.float64)\n",
      "tensor([0.7277], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7277, 1.1279, 0.5234], dtype=torch.float64)\n",
      "tensor([0.7534], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7534, 1.1290, 0.5156], dtype=torch.float64)\n",
      "tensor([0.7549], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7549, 1.1219, 0.5078], dtype=torch.float64)\n",
      "tensor([0.7426], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7426, 1.1528, 0.5000], dtype=torch.float64)\n",
      "tensor([0.7951], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7951, 1.1493, 0.4922], dtype=torch.float64)\n",
      "tensor([0.7885], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7885, 1.1302, 0.4844], dtype=torch.float64)\n",
      "tensor([0.7558], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7558, 1.1085, 0.4766], dtype=torch.float64)\n",
      "tensor([0.7188], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7188, 1.1257, 0.4688], dtype=torch.float64)\n",
      "tensor([0.7483], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7483, 1.1313, 0.4609], dtype=torch.float64)\n",
      "tensor([0.7574], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7574, 1.1459, 0.4531], dtype=torch.float64)\n",
      "tensor([0.7819], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7819, 1.1377, 0.4453], dtype=torch.float64)\n",
      "tensor([0.7675], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7675, 1.1367, 0.4375], dtype=torch.float64)\n",
      "tensor([0.7658], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7658, 1.1323, 0.4297], dtype=torch.float64)\n",
      "tensor([0.7581], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7581, 1.1093, 0.4219], dtype=torch.float64)\n",
      "tensor([0.7188], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7188, 1.0832, 0.4141], dtype=torch.float64)\n",
      "tensor([0.6744], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6744, 1.0943, 0.4062], dtype=torch.float64)\n",
      "tensor([0.6933], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6933, 1.0911, 0.3984], dtype=torch.float64)\n",
      "tensor([0.6876], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6876, 1.0757, 0.3906], dtype=torch.float64)\n",
      "tensor([0.6602], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6602, 1.0937, 0.3828], dtype=torch.float64)\n",
      "tensor([0.6919], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6919, 1.0756, 0.3750], dtype=torch.float64)\n",
      "tensor([0.6594], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6594, 1.0971, 0.3672], dtype=torch.float64)\n",
      "tensor([0.6972], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6972, 1.0871, 0.3594], dtype=torch.float64)\n",
      "tensor([0.6797], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6797, 1.1040, 0.3516], dtype=torch.float64)\n",
      "tensor([0.7083], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7083, 1.1198, 0.3438], dtype=torch.float64)\n",
      "tensor([0.7348], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7348, 1.1332, 0.3359], dtype=torch.float64)\n",
      "tensor([0.7572], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7572, 1.1329, 0.3281], dtype=torch.float64)\n",
      "tensor([0.7563], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7563, 1.1450, 0.3203], dtype=torch.float64)\n",
      "tensor([0.7767], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7767, 1.1463, 0.3125], dtype=torch.float64)\n",
      "tensor([0.7786], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7786, 1.1489, 0.3047], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7828], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7828, 1.1433, 0.2969], dtype=torch.float64)\n",
      "tensor([0.7729], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7729, 1.1351, 0.2891], dtype=torch.float64)\n",
      "tensor([0.7589], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7589, 1.1375, 0.2812], dtype=torch.float64)\n",
      "tensor([0.7629], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7629, 1.1287, 0.2734], dtype=torch.float64)\n",
      "tensor([0.7476], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7476, 1.1336, 0.2656], dtype=torch.float64)\n",
      "tensor([0.7559], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7559, 1.1489, 0.2578], dtype=torch.float64)\n",
      "tensor([0.7816], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7816, 1.1681, 0.2500], dtype=torch.float64)\n",
      "tensor([0.8139], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8139, 1.1803, 0.2422], dtype=torch.float64)\n",
      "tensor([0.8342], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8342, 1.1582, 0.2344], dtype=torch.float64)\n",
      "tensor([0.7962], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7962, 1.1747, 0.2266], dtype=torch.float64)\n",
      "tensor([0.8243], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8243, 1.1887, 0.2188], dtype=torch.float64)\n",
      "tensor([0.8477], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8477, 1.1972, 0.2109], dtype=torch.float64)\n",
      "tensor([0.8618], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8618, 1.2133, 0.2031], dtype=torch.float64)\n",
      "tensor([0.8888], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8888, 1.2311, 0.1953], dtype=torch.float64)\n",
      "tensor([0.9187], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9187, 1.2172, 0.1875], dtype=torch.float64)\n",
      "tensor([0.8947], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8947, 1.2301, 0.1797], dtype=torch.float64)\n",
      "tensor([0.9167], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9167, 1.2140, 0.1719], dtype=torch.float64)\n",
      "tensor([0.8889], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8889, 1.2326, 0.1641], dtype=torch.float64)\n",
      "tensor([0.9205], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9205, 1.2408, 0.1562], dtype=torch.float64)\n",
      "tensor([0.9339], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9339, 1.2433, 0.1484], dtype=torch.float64)\n",
      "tensor([0.9379], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9379, 1.2479, 0.1406], dtype=torch.float64)\n",
      "tensor([0.9454], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9454, 1.2577, 0.1328], dtype=torch.float64)\n",
      "tensor([0.9619], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9619, 1.2518, 0.1250], dtype=torch.float64)\n",
      "tensor([0.9515], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9515, 1.2467, 0.1172], dtype=torch.float64)\n",
      "tensor([0.9427], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9427, 1.2451, 0.1094], dtype=torch.float64)\n",
      "tensor([0.9398], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9398, 1.2612, 0.1016], dtype=torch.float64)\n",
      "tensor([0.9671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9671, 1.2535, 0.0938], dtype=torch.float64)\n",
      "tensor([0.9535], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9535, 1.2427, 0.0859], dtype=torch.float64)\n",
      "tensor([0.9350], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9350, 1.2453, 0.0781], dtype=torch.float64)\n",
      "tensor([0.9393], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9393, 1.2395, 0.0703], dtype=torch.float64)\n",
      "tensor([0.9293], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9293, 1.2594, 0.0625], dtype=torch.float64)\n",
      "tensor([0.9630], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9630, 1.2527, 0.0547], dtype=torch.float64)\n",
      "tensor([0.9512], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9512, 1.2382, 0.0469], dtype=torch.float64)\n",
      "tensor([0.9263], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9263, 1.2422, 0.0391], dtype=torch.float64)\n",
      "tensor([0.9332], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9332, 1.2488, 0.0312], dtype=torch.float64)\n",
      "tensor([0.9441], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9441, 1.2749, 0.0234], dtype=torch.float64)\n",
      "tensor([0.9882], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9882, 1.2736, 0.0156], dtype=torch.float64)\n",
      "tensor([0.9854], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9854, 1.3098, 0.0078], dtype=torch.float64)\n",
      "tensor([1.0469], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  3.73697359201294e-06 5.673332194910931e-07\n",
      "loss: [38.8695317]\n",
      "episode:  181\n",
      "loss: [31.12417725]\n",
      "episode:  182\n",
      "loss: [26.28104107]\n",
      "episode:  183\n",
      "loss: [26.62317931]\n",
      "episode:  184\n",
      "loss: [22.51020384]\n",
      "episode:  185\n",
      "loss: [19.27392841]\n",
      "episode:  186\n",
      "loss: [22.65724138]\n",
      "episode:  187\n",
      "loss: [37.07768785]\n",
      "episode:  188\n",
      "loss: [31.82458911]\n",
      "episode:  189\n",
      "loss: [18.72326154]\n",
      "episode:  190\n",
      "loss: [39.71235876]\n",
      "episode:  191\n",
      "loss: [20.84828553]\n",
      "episode:  192\n",
      "loss: [33.15526947]\n",
      "episode:  193\n",
      "loss: [22.60930543]\n",
      "episode:  194\n",
      "loss: [29.92965143]\n",
      "episode:  195\n",
      "loss: [34.43562834]\n",
      "episode:  196\n",
      "loss: [29.49853184]\n",
      "episode:  197\n",
      "loss: [26.94922365]\n",
      "episode:  198\n",
      "loss: [18.87976303]\n",
      "episode:  199\n",
      "loss: [29.73983981]\n",
      "episode:  200\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.2344], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2344, 1.0020, 0.9922], dtype=torch.float64)\n",
      "tensor([0.4299], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4299, 1.0259, 0.9844], dtype=torch.float64)\n",
      "tensor([0.6148], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6148, 1.0049, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5686], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5686, 1.0331, 0.9688], dtype=torch.float64)\n",
      "tensor([0.6521], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6521, 1.0254, 0.9609], dtype=torch.float64)\n",
      "tensor([0.6357], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6357, 1.0323, 0.9531], dtype=torch.float64)\n",
      "tensor([0.6539], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6539, 1.0395, 0.9453], dtype=torch.float64)\n",
      "tensor([0.6775], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6775, 1.0289, 0.9375], dtype=torch.float64)\n",
      "tensor([0.6439], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6439, 1.0150, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5923], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5923, 1.0257, 0.9219], dtype=torch.float64)\n",
      "tensor([0.6182], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6182, 1.0233, 0.9141], dtype=torch.float64)\n",
      "tensor([0.6116], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6116, 1.0282, 0.9062], dtype=torch.float64)\n",
      "tensor([0.6249], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6249, 1.0538, 0.8984], dtype=torch.float64)\n",
      "tensor([0.7062], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7062, 1.0233, 0.8906], dtype=torch.float64)\n",
      "tensor([0.6200], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6200, 0.9869, 0.8828], dtype=torch.float64)\n",
      "tensor([0.4848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4848, 0.9977, 0.8750], dtype=torch.float64)\n",
      "tensor([0.5001], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5001, 0.9950, 0.8672], dtype=torch.float64)\n",
      "tensor([0.4912], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4912, 0.9798, 0.8594], dtype=torch.float64)\n",
      "tensor([0.4385], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4385, 0.9850, 0.8516], dtype=torch.float64)\n",
      "tensor([0.4462], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4462, 0.9835, 0.8438], dtype=torch.float64)\n",
      "tensor([0.4410], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4410, 0.9676, 0.8359], dtype=torch.float64)\n",
      "tensor([0.3857], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3857, 0.9505, 0.8281], dtype=torch.float64)\n",
      "tensor([0.3206], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3206, 0.9541, 0.8203], dtype=torch.float64)\n",
      "tensor([0.3217], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3217, 0.9415, 0.8125], dtype=torch.float64)\n",
      "tensor([0.2788], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2788, 0.9356, 0.8047], dtype=torch.float64)\n",
      "tensor([0.2517], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2517, 0.9080, 0.7969], dtype=torch.float64)\n",
      "tensor([0.1583], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1583, 0.8938, 0.7891], dtype=torch.float64)\n",
      "tensor([0.1200], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1200, 0.8824, 0.7812], dtype=torch.float64)\n",
      "tensor([0.1199], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1199, 0.8714, 0.7734], dtype=torch.float64)\n",
      "tensor([0.1198], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1198, 0.8800, 0.7656], dtype=torch.float64)\n",
      "tensor([0.1199], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1199, 0.8592, 0.7578], dtype=torch.float64)\n",
      "tensor([0.1200], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1200, 0.8722, 0.7500], dtype=torch.float64)\n",
      "tensor([0.1198], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1198, 0.8962, 0.7422], dtype=torch.float64)\n",
      "tensor([0.1200], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1200, 0.9056, 0.7344], dtype=torch.float64)\n",
      "tensor([0.1265], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1265, 0.8922, 0.7266], dtype=torch.float64)\n",
      "tensor([0.1200], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1200, 0.8997, 0.7188], dtype=torch.float64)\n",
      "tensor([0.1205], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1205, 0.9087, 0.7109], dtype=torch.float64)\n",
      "tensor([0.1326], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1326, 0.8898, 0.7031], dtype=torch.float64)\n",
      "tensor([0.1200], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1200, 0.8834, 0.6953], dtype=torch.float64)\n",
      "tensor([0.1199], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1199, 0.8840, 0.6875], dtype=torch.float64)\n",
      "tensor([0.1199], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1199, 0.9006, 0.6797], dtype=torch.float64)\n",
      "tensor([0.1209], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1209, 0.8969, 0.6719], dtype=torch.float64)\n",
      "tensor([0.1200], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1200, 0.8982, 0.6641], dtype=torch.float64)\n",
      "tensor([0.1200], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1200, 0.8976, 0.6562], dtype=torch.float64)\n",
      "tensor([0.1200], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1200, 0.9150, 0.6484], dtype=torch.float64)\n",
      "tensor([0.1472], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1472, 0.9100, 0.6406], dtype=torch.float64)\n",
      "tensor([0.1362], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1362, 0.9146, 0.6328], dtype=torch.float64)\n",
      "tensor([0.1473], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1473, 0.9287, 0.6250], dtype=torch.float64)\n",
      "tensor([0.1955], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1955, 0.9206, 0.6172], dtype=torch.float64)\n",
      "tensor([0.1728], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1728, 0.9286, 0.6094], dtype=torch.float64)\n",
      "tensor([0.1978], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1978, 0.9236, 0.6016], dtype=torch.float64)\n",
      "tensor([0.1830], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1830, 0.9356, 0.5938], dtype=torch.float64)\n",
      "tensor([0.2231], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2231, 0.9386, 0.5859], dtype=torch.float64)\n",
      "tensor([0.2390], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2390, 0.9418, 0.5781], dtype=torch.float64)\n",
      "tensor([0.2520], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2520, 0.9440, 0.5703], dtype=torch.float64)\n",
      "tensor([0.2614], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2614, 0.9535, 0.5625], dtype=torch.float64)\n",
      "tensor([0.2958], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2958, 0.9540, 0.5547], dtype=torch.float64)\n",
      "tensor([0.3018], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3018, 0.9660, 0.5469], dtype=torch.float64)\n",
      "tensor([0.3448], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3448, 0.9417, 0.5391], dtype=torch.float64)\n",
      "tensor([0.2644], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2644, 0.9418, 0.5312], dtype=torch.float64)\n",
      "tensor([0.2532], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2532, 0.9417, 0.5234], dtype=torch.float64)\n",
      "tensor([0.2510], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2510, 0.9471, 0.5156], dtype=torch.float64)\n",
      "tensor([0.2694], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2694, 0.9534, 0.5078], dtype=torch.float64)\n",
      "tensor([0.2938], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2938, 0.9609, 0.5000], dtype=torch.float64)\n",
      "tensor([0.3235], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3235, 0.9598, 0.4922], dtype=torch.float64)\n",
      "tensor([0.3233], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3233, 0.9674, 0.4844], dtype=torch.float64)\n",
      "tensor([0.3497], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3497, 0.9837, 0.4766], dtype=torch.float64)\n",
      "tensor([0.4109], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4109, 0.9962, 0.4688], dtype=torch.float64)\n",
      "tensor([0.4630], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4630, 1.0079, 0.4609], dtype=torch.float64)\n",
      "tensor([0.5111], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5111, 1.0134, 0.4531], dtype=torch.float64)\n",
      "tensor([0.5371], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5371, 1.0070, 0.4453], dtype=torch.float64)\n",
      "tensor([0.5176], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5176, 0.9971, 0.4375], dtype=torch.float64)\n",
      "tensor([0.4795], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4795, 1.0094, 0.4297], dtype=torch.float64)\n",
      "tensor([0.5174], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5174, 1.0144, 0.4219], dtype=torch.float64)\n",
      "tensor([0.5400], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5400, 1.0195, 0.4141], dtype=torch.float64)\n",
      "tensor([0.5608], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5608, 1.0188, 0.4062], dtype=torch.float64)\n",
      "tensor([0.5606], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5606, 1.0514, 0.3984], dtype=torch.float64)\n",
      "tensor([0.6722], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6722, 1.0477, 0.3906], dtype=torch.float64)\n",
      "tensor([0.6775], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6775, 1.0341, 0.3828], dtype=torch.float64)\n",
      "tensor([0.6296], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6296, 1.0209, 0.3750], dtype=torch.float64)\n",
      "tensor([0.5761], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5761, 1.0183, 0.3672], dtype=torch.float64)\n",
      "tensor([0.5593], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5593, 1.0119, 0.3594], dtype=torch.float64)\n",
      "tensor([0.5337], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5337, 0.9978, 0.3516], dtype=torch.float64)\n",
      "tensor([0.4800], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4800, 0.9827, 0.3438], dtype=torch.float64)\n",
      "tensor([0.4188], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4188, 0.9740, 0.3359], dtype=torch.float64)\n",
      "tensor([0.3792], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3792, 0.9819, 0.3281], dtype=torch.float64)\n",
      "tensor([0.4012], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4012, 0.9927, 0.3203], dtype=torch.float64)\n",
      "tensor([0.4421], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4421, 0.9742, 0.3125], dtype=torch.float64)\n",
      "tensor([0.3821], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3821, 0.9685, 0.3047], dtype=torch.float64)\n",
      "tensor([0.3531], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3531, 0.9755, 0.2969], dtype=torch.float64)\n",
      "tensor([0.3736], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3736, 0.9627, 0.2891], dtype=torch.float64)\n",
      "tensor([0.3308], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3308, 0.9470, 0.2812], dtype=torch.float64)\n",
      "tensor([0.2688], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2688, 0.9270, 0.2734], dtype=torch.float64)\n",
      "tensor([0.1893], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1893, 0.9469, 0.2656], dtype=torch.float64)\n",
      "tensor([0.2485], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2485, 0.9496, 0.2578], dtype=torch.float64)\n",
      "tensor([0.2658], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2658, 0.9658, 0.2500], dtype=torch.float64)\n",
      "tensor([0.3249], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3249, 0.9508, 0.2422], dtype=torch.float64)\n",
      "tensor([0.2797], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2797, 0.9578, 0.2344], dtype=torch.float64)\n",
      "tensor([0.2979], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2979, 0.9558, 0.2266], dtype=torch.float64)\n",
      "tensor([0.2930], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2930, 0.9434, 0.2188], dtype=torch.float64)\n",
      "tensor([0.2479], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2479, 0.9312, 0.2109], dtype=torch.float64)\n",
      "tensor([0.1983], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1983, 0.9262, 0.2031], dtype=torch.float64)\n",
      "tensor([0.1732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1732, 0.9192, 0.1953], dtype=torch.float64)\n",
      "tensor([0.1521], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1521, 0.9173, 0.1875], dtype=torch.float64)\n",
      "tensor([0.1558], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1558, 0.9017, 0.1797], dtype=torch.float64)\n",
      "tensor([0.1204], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1204, 0.8879, 0.1719], dtype=torch.float64)\n",
      "tensor([0.1200], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1200, 0.8966, 0.1641], dtype=torch.float64)\n",
      "tensor([0.1246], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1246, 0.9183, 0.1562], dtype=torch.float64)\n",
      "tensor([0.1806], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1806, 0.9270, 0.1484], dtype=torch.float64)\n",
      "tensor([0.1740], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1740, 0.9160, 0.1406], dtype=torch.float64)\n",
      "tensor([0.1441], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1441, 0.9266, 0.1328], dtype=torch.float64)\n",
      "tensor([0.1980], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1980, 0.9119, 0.1250], dtype=torch.float64)\n",
      "tensor([0.1286], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1286, 0.9188, 0.1172], dtype=torch.float64)\n",
      "tensor([0.1847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1847, 0.9099, 0.1094], dtype=torch.float64)\n",
      "tensor([0.1269], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1269, 0.9027, 0.1016], dtype=torch.float64)\n",
      "tensor([0.1390], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1390, 0.9269, 0.0938], dtype=torch.float64)\n",
      "tensor([0.2079], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2079, 0.9153, 0.0859], dtype=torch.float64)\n",
      "tensor([0.1369], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1369, 0.9062, 0.0781], dtype=torch.float64)\n",
      "tensor([0.1452], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1452, 0.9163, 0.0703], dtype=torch.float64)\n",
      "tensor([0.1709], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1709, 0.9221, 0.0625], dtype=torch.float64)\n",
      "tensor([0.1732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1732, 0.9222, 0.0547], dtype=torch.float64)\n",
      "tensor([0.1730], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1730, 0.9062, 0.0469], dtype=torch.float64)\n",
      "tensor([0.1295], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1295, 0.9068, 0.0391], dtype=torch.float64)\n",
      "tensor([0.1560], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1560, 0.8966, 0.0312], dtype=torch.float64)\n",
      "tensor([0.1212], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1212, 0.8979, 0.0234], dtype=torch.float64)\n",
      "tensor([0.1379], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1379, 0.8732, 0.0156], dtype=torch.float64)\n",
      "tensor([0.1199], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1199, 0.8878, 0.0078], dtype=torch.float64)\n",
      "tensor([0.1211], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_result:  2.233289686098265e-06 6.395521138966553e-07\n",
      "loss: [18.18508659]\n",
      "episode:  201\n",
      "loss: [17.62307948]\n",
      "episode:  202\n",
      "loss: [21.73383974]\n",
      "episode:  203\n",
      "loss: [23.1750503]\n",
      "episode:  204\n",
      "loss: [20.40685234]\n",
      "episode:  205\n",
      "loss: [31.80062882]\n",
      "episode:  206\n",
      "loss: [19.84233929]\n",
      "episode:  207\n",
      "loss: [34.93088119]\n",
      "episode:  208\n",
      "loss: [30.15686318]\n",
      "episode:  209\n",
      "loss: [32.06555036]\n",
      "episode:  210\n",
      "loss: [35.26755017]\n",
      "episode:  211\n",
      "loss: [31.58238518]\n",
      "episode:  212\n",
      "loss: [27.54524411]\n",
      "episode:  213\n",
      "loss: [24.54250814]\n",
      "episode:  214\n",
      "loss: [26.44944531]\n",
      "episode:  215\n",
      "loss: [38.38382644]\n",
      "episode:  216\n",
      "loss: [22.22223363]\n",
      "episode:  217\n",
      "loss: [18.72540967]\n",
      "episode:  218\n",
      "loss: [116.2931334]\n",
      "episode:  219\n",
      "loss: [60.2403848]\n",
      "episode:  220\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.2751], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2751, 0.9887, 0.9922], dtype=torch.float64)\n",
      "tensor([0.4323], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4323, 0.9965, 0.9844], dtype=torch.float64)\n",
      "tensor([0.4632], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4632, 0.9918, 0.9766], dtype=torch.float64)\n",
      "tensor([0.4519], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4519, 0.9917, 0.9688], dtype=torch.float64)\n",
      "tensor([0.4489], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4489, 0.9814, 0.9609], dtype=torch.float64)\n",
      "tensor([0.4214], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4214, 0.9612, 0.9531], dtype=torch.float64)\n",
      "tensor([0.3675], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3675, 0.9641, 0.9453], dtype=torch.float64)\n",
      "tensor([0.3688], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3688, 0.9690, 0.9375], dtype=torch.float64)\n",
      "tensor([0.3792], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3792, 0.9845, 0.9297], dtype=torch.float64)\n",
      "tensor([0.4167], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4167, 0.9868, 0.9219], dtype=torch.float64)\n",
      "tensor([0.4232], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4232, 0.9837, 0.9141], dtype=torch.float64)\n",
      "tensor([0.4143], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4143, 0.9902, 0.9062], dtype=torch.float64)\n",
      "tensor([0.4278], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4278, 0.9940, 0.8984], dtype=torch.float64)\n",
      "tensor([0.4364], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4364, 0.9890, 0.8906], dtype=torch.float64)\n",
      "tensor([0.4229], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4229, 0.9900, 0.8828], dtype=torch.float64)\n",
      "tensor([0.4226], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4226, 0.9881, 0.8750], dtype=torch.float64)\n",
      "tensor([0.4159], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4159, 0.9835, 0.8672], dtype=torch.float64)\n",
      "tensor([0.4021], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4021, 0.9619, 0.8594], dtype=torch.float64)\n",
      "tensor([0.3457], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3457, 0.9558, 0.8516], dtype=torch.float64)\n",
      "tensor([0.3244], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3244, 0.9629, 0.8438], dtype=torch.float64)\n",
      "tensor([0.3389], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3389, 0.9747, 0.8359], dtype=torch.float64)\n",
      "tensor([0.3674], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3674, 0.9642, 0.8281], dtype=torch.float64)\n",
      "tensor([0.3416], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3416, 0.9826, 0.8203], dtype=torch.float64)\n",
      "tensor([0.3839], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3839, 0.9995, 0.8125], dtype=torch.float64)\n",
      "tensor([0.4324], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4324, 0.9966, 0.8047], dtype=torch.float64)\n",
      "tensor([0.4279], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4279, 1.0076, 0.7969], dtype=torch.float64)\n",
      "tensor([0.4568], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4568, 0.9928, 0.7891], dtype=torch.float64)\n",
      "tensor([0.4188], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4188, 1.0069, 0.7812], dtype=torch.float64)\n",
      "tensor([0.4530], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4530, 1.0176, 0.7734], dtype=torch.float64)\n",
      "tensor([0.4842], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4842, 1.0143, 0.7656], dtype=torch.float64)\n",
      "tensor([0.4774], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4774, 1.0287, 0.7578], dtype=torch.float64)\n",
      "tensor([0.5142], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5142, 1.0234, 0.7500], dtype=torch.float64)\n",
      "tensor([0.5033], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5033, 1.0400, 0.7422], dtype=torch.float64)\n",
      "tensor([0.5391], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5391, 1.0388, 0.7344], dtype=torch.float64)\n",
      "tensor([0.5393], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5393, 1.0323, 0.7266], dtype=torch.float64)\n",
      "tensor([0.5257], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5257, 1.0362, 0.7188], dtype=torch.float64)\n",
      "tensor([0.5331], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5331, 1.0310, 0.7109], dtype=torch.float64)\n",
      "tensor([0.5213], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5213, 1.0398, 0.7031], dtype=torch.float64)\n",
      "tensor([0.5385], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5385, 1.0155, 0.6953], dtype=torch.float64)\n",
      "tensor([0.4803], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4803, 0.9916, 0.6875], dtype=torch.float64)\n",
      "tensor([0.4108], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4108, 0.9924, 0.6797], dtype=torch.float64)\n",
      "tensor([0.4067], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4067, 0.9800, 0.6719], dtype=torch.float64)\n",
      "tensor([0.3724], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3724, 0.9750, 0.6641], dtype=torch.float64)\n",
      "tensor([0.3557], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3557, 0.9754, 0.6562], dtype=torch.float64)\n",
      "tensor([0.3550], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3550, 0.9804, 0.6484], dtype=torch.float64)\n",
      "tensor([0.3678], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3678, 0.9795, 0.6406], dtype=torch.float64)\n",
      "tensor([0.3660], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3660, 0.9940, 0.6328], dtype=torch.float64)\n",
      "tensor([0.4043], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4043, 1.0043, 0.6250], dtype=torch.float64)\n",
      "tensor([0.4347], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4347, 1.0009, 0.6172], dtype=torch.float64)\n",
      "tensor([0.4276], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4276, 1.0058, 0.6094], dtype=torch.float64)\n",
      "tensor([0.4395], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4395, 1.0265, 0.6016], dtype=torch.float64)\n",
      "tensor([0.4957], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4957, 0.9986, 0.5938], dtype=torch.float64)\n",
      "tensor([0.4247], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4247, 1.0047, 0.5859], dtype=torch.float64)\n",
      "tensor([0.4348], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4348, 1.0003, 0.5781], dtype=torch.float64)\n",
      "tensor([0.4235], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4235, 1.0034, 0.5703], dtype=torch.float64)\n",
      "tensor([0.4302], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4302, 1.0272, 0.5625], dtype=torch.float64)\n",
      "tensor([0.4943], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4943, 1.0306, 0.5547], dtype=torch.float64)\n",
      "tensor([0.5082], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5082, 1.0495, 0.5469], dtype=torch.float64)\n",
      "tensor([0.5477], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5477, 1.0646, 0.5391], dtype=torch.float64)\n",
      "tensor([0.5681], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5681, 1.0693, 0.5312], dtype=torch.float64)\n",
      "tensor([0.5718], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5718, 1.0789, 0.5234], dtype=torch.float64)\n",
      "tensor([0.5775], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5775, 1.0697, 0.5156], dtype=torch.float64)\n",
      "tensor([0.5720], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5720, 1.0454, 0.5078], dtype=torch.float64)\n",
      "tensor([0.5439], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5439, 1.0778, 0.5000], dtype=torch.float64)\n",
      "tensor([0.5764], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5764, 1.0931, 0.4922], dtype=torch.float64)\n",
      "tensor([0.5836], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5836, 1.0906, 0.4844], dtype=torch.float64)\n",
      "tensor([0.5825], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5825, 1.0747, 0.4766], dtype=torch.float64)\n",
      "tensor([0.5751], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5751, 1.0884, 0.4688], dtype=torch.float64)\n",
      "tensor([0.5813], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5813, 1.0964, 0.4609], dtype=torch.float64)\n",
      "tensor([0.5849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5849, 1.0943, 0.4531], dtype=torch.float64)\n",
      "tensor([0.5839], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5839, 1.1075, 0.4453], dtype=torch.float64)\n",
      "tensor([0.5897], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5897, 1.1131, 0.4375], dtype=torch.float64)\n",
      "tensor([0.5922], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5922, 1.1253, 0.4297], dtype=torch.float64)\n",
      "tensor([0.5976], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5976, 1.1464, 0.4219], dtype=torch.float64)\n",
      "tensor([0.6071], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6071, 1.1709, 0.4141], dtype=torch.float64)\n",
      "tensor([0.6182], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6182, 1.1604, 0.4062], dtype=torch.float64)\n",
      "tensor([0.6135], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6135, 1.1748, 0.3984], dtype=torch.float64)\n",
      "tensor([0.6198], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6198, 1.1571, 0.3906], dtype=torch.float64)\n",
      "tensor([0.6119], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6119, 1.1844, 0.3828], dtype=torch.float64)\n",
      "tensor([0.6240], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6240, 1.1801, 0.3750], dtype=torch.float64)\n",
      "tensor([0.6221], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6221, 1.1746, 0.3672], dtype=torch.float64)\n",
      "tensor([0.6195], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6195, 1.1768, 0.3594], dtype=torch.float64)\n",
      "tensor([0.6204], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6204, 1.1538, 0.3516], dtype=torch.float64)\n",
      "tensor([0.6101], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6101, 1.1315, 0.3438], dtype=torch.float64)\n",
      "tensor([0.5998], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5998, 1.1258, 0.3359], dtype=torch.float64)\n",
      "tensor([0.5971], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5971, 1.1119, 0.3281], dtype=torch.float64)\n",
      "tensor([0.5907], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5907, 1.1289, 0.3203], dtype=torch.float64)\n",
      "tensor([0.5982], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5982, 1.1273, 0.3125], dtype=torch.float64)\n",
      "tensor([0.5975], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5975, 1.1216, 0.3047], dtype=torch.float64)\n",
      "tensor([0.5948], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5948, 1.1281, 0.2969], dtype=torch.float64)\n",
      "tensor([0.5976], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5976, 1.1476, 0.2891], dtype=torch.float64)\n",
      "tensor([0.6064], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6064, 1.1496, 0.2812], dtype=torch.float64)\n",
      "tensor([0.6073], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6073, 1.1283, 0.2734], dtype=torch.float64)\n",
      "tensor([0.5977], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5977, 1.1190, 0.2656], dtype=torch.float64)\n",
      "tensor([0.5933], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5933, 1.1418, 0.2578], dtype=torch.float64)\n",
      "tensor([0.6034], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6034, 1.1512, 0.2500], dtype=torch.float64)\n",
      "tensor([0.6077], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6077, 1.1697, 0.2422], dtype=torch.float64)\n",
      "tensor([0.6160], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6160, 1.1472, 0.2344], dtype=torch.float64)\n",
      "tensor([0.6059], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6059, 1.1698, 0.2266], dtype=torch.float64)\n",
      "tensor([0.6158], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6158, 1.1783, 0.2188], dtype=torch.float64)\n",
      "tensor([0.6197], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6197, 1.1768, 0.2109], dtype=torch.float64)\n",
      "tensor([0.6190], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6190, 1.1706, 0.2031], dtype=torch.float64)\n",
      "tensor([0.6161], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6161, 1.1664, 0.1953], dtype=torch.float64)\n",
      "tensor([0.6141], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6141, 1.1525, 0.1875], dtype=torch.float64)\n",
      "tensor([0.6078], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6078, 1.1415, 0.1797], dtype=torch.float64)\n",
      "tensor([0.6027], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6027, 1.1500, 0.1719], dtype=torch.float64)\n",
      "tensor([0.6064], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6064, 1.1510, 0.1641], dtype=torch.float64)\n",
      "tensor([0.6068], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6068, 1.1565, 0.1562], dtype=torch.float64)\n",
      "tensor([0.6092], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6092, 1.1358, 0.1484], dtype=torch.float64)\n",
      "tensor([0.5998], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5998, 1.1301, 0.1406], dtype=torch.float64)\n",
      "tensor([0.5971], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5971, 1.1340, 0.1328], dtype=torch.float64)\n",
      "tensor([0.5987], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5987, 1.1343, 0.1250], dtype=torch.float64)\n",
      "tensor([0.5988], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5988, 1.1504, 0.1172], dtype=torch.float64)\n",
      "tensor([0.6059], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6059, 1.1433, 0.1094], dtype=torch.float64)\n",
      "tensor([0.6028], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6028, 1.1356, 0.1016], dtype=torch.float64)\n",
      "tensor([0.5992], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5992, 1.1413, 0.0938], dtype=torch.float64)\n",
      "tensor([0.6017], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6017, 1.1446, 0.0859], dtype=torch.float64)\n",
      "tensor([0.6031], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6031, 1.1425, 0.0781], dtype=torch.float64)\n",
      "tensor([0.6021], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6021, 1.1564, 0.0703], dtype=torch.float64)\n",
      "tensor([0.6083], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6083, 1.1483, 0.0625], dtype=torch.float64)\n",
      "tensor([0.6046], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6046, 1.1592, 0.0547], dtype=torch.float64)\n",
      "tensor([0.6094], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6094, 1.1145, 0.0469], dtype=torch.float64)\n",
      "tensor([0.5893], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5893, 1.1074, 0.0391], dtype=torch.float64)\n",
      "tensor([0.5858], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5858, 1.1174, 0.0312], dtype=torch.float64)\n",
      "tensor([0.5901], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5901, 1.1202, 0.0234], dtype=torch.float64)\n",
      "tensor([0.5914], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5914, 1.1512, 0.0156], dtype=torch.float64)\n",
      "tensor([0.6052], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6052, 1.1800, 0.0078], dtype=torch.float64)\n",
      "tensor([0.6183], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  7.757069185611964e-06 7.337186046363486e-07\n",
      "loss: [73.24338883]\n",
      "episode:  221\n",
      "loss: [74.05390709]\n",
      "episode:  222\n",
      "loss: [60.06581086]\n",
      "episode:  223\n",
      "loss: [53.94996139]\n",
      "episode:  224\n",
      "loss: [63.21760937]\n",
      "episode:  225\n",
      "loss: [49.44102612]\n",
      "episode:  226\n",
      "loss: [28.13358048]\n",
      "episode:  227\n",
      "loss: [49.52153891]\n",
      "episode:  228\n",
      "loss: [35.78343438]\n",
      "episode:  229\n",
      "loss: [77.57081751]\n",
      "episode:  230\n",
      "loss: [22.25184103]\n",
      "episode:  231\n",
      "loss: [20.99181556]\n",
      "episode:  232\n",
      "loss: [28.74633538]\n",
      "episode:  233\n",
      "loss: [25.89440344]\n",
      "episode:  234\n",
      "loss: [25.96626858]\n",
      "episode:  235\n",
      "loss: [24.80092456]\n",
      "episode:  236\n",
      "loss: [26.52238062]\n",
      "episode:  237\n",
      "loss: [27.08089716]\n",
      "episode:  238\n",
      "loss: [26.19084137]\n",
      "episode:  239\n",
      "loss: [31.37182383]\n",
      "episode:  240\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3907], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3907, 0.9865, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5478], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5478, 0.9817, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5360], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5360, 0.9596, 0.9766], dtype=torch.float64)\n",
      "tensor([0.4716], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4716, 0.9609, 0.9688], dtype=torch.float64)\n",
      "tensor([0.4723], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4723, 0.9875, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5456], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5456, 0.9794, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5228], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5228, 0.9884, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5459], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5459, 1.0038, 0.9375], dtype=torch.float64)\n",
      "tensor([0.5884], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5884, 1.0048, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5905], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5905, 1.0079, 0.9219], dtype=torch.float64)\n",
      "tensor([0.5977], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5977, 1.0007, 0.9141], dtype=torch.float64)\n",
      "tensor([0.5758], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5758, 0.9973, 0.9062], dtype=torch.float64)\n",
      "tensor([0.5641], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5641, 0.9970, 0.8984], dtype=torch.float64)\n",
      "tensor([0.5612], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5612, 0.9953, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5547], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5547, 1.0064, 0.8828], dtype=torch.float64)\n",
      "tensor([0.5842], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5842, 1.0018, 0.8750], dtype=torch.float64)\n",
      "tensor([0.5703], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5703, 1.0053, 0.8672], dtype=torch.float64)\n",
      "tensor([0.5781], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5781, 0.9975, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5547], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5547, 0.9987, 0.8516], dtype=torch.float64)\n",
      "tensor([0.5560], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5560, 0.9949, 0.8438], dtype=torch.float64)\n",
      "tensor([0.5440], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5440, 0.9932, 0.8359], dtype=torch.float64)\n",
      "tensor([0.5380], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5380, 0.9925, 0.8281], dtype=torch.float64)\n",
      "tensor([0.5356], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5356, 0.9903, 0.8203], dtype=torch.float64)\n",
      "tensor([0.5286], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5286, 1.0348, 0.8125], dtype=torch.float64)\n",
      "tensor([0.6638], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6638, 1.0264, 0.8047], dtype=torch.float64)\n",
      "tensor([0.6431], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6431, 0.9994, 0.7969], dtype=torch.float64)\n",
      "tensor([0.5595], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5595, 0.9979, 0.7891], dtype=torch.float64)\n",
      "tensor([0.5523], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5523, 1.0177, 0.7812], dtype=torch.float64)\n",
      "tensor([0.6126], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6126, 1.0108, 0.7734], dtype=torch.float64)\n",
      "tensor([0.5931], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5931, 1.0327, 0.7656], dtype=torch.float64)\n",
      "tensor([0.6594], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6594, 0.9893, 0.7578], dtype=torch.float64)\n",
      "tensor([0.5286], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5286, 0.9940, 0.7500], dtype=torch.float64)\n",
      "tensor([0.5387], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5387, 0.9875, 0.7422], dtype=torch.float64)\n",
      "tensor([0.5190], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5190, 1.0131, 0.7344], dtype=torch.float64)\n",
      "tensor([0.5966], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5966, 1.0134, 0.7266], dtype=torch.float64)\n",
      "tensor([0.5997], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5997, 1.0354, 0.7188], dtype=torch.float64)\n",
      "tensor([0.6669], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6669, 1.0264, 0.7109], dtype=torch.float64)\n",
      "tensor([0.6413], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6413, 1.0420, 0.7031], dtype=torch.float64)\n",
      "tensor([0.6873], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6873, 1.0298, 0.6953], dtype=torch.float64)\n",
      "tensor([0.6522], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6522, 1.0275, 0.6875], dtype=torch.float64)\n",
      "tensor([0.6441], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6441, 1.0203, 0.6797], dtype=torch.float64)\n",
      "tensor([0.6216], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6216, 0.9956, 0.6719], dtype=torch.float64)\n",
      "tensor([0.5451], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5451, 0.9787, 0.6641], dtype=torch.float64)\n",
      "tensor([0.4909], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4909, 0.9711, 0.6562], dtype=torch.float64)\n",
      "tensor([0.4658], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4658, 0.9785, 0.6484], dtype=torch.float64)\n",
      "tensor([0.4875], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4875, 0.9726, 0.6406], dtype=torch.float64)\n",
      "tensor([0.4699], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4699, 0.9798, 0.6328], dtype=torch.float64)\n",
      "tensor([0.4913], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4913, 0.9846, 0.6250], dtype=torch.float64)\n",
      "tensor([0.5065], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5065, 0.9692, 0.6172], dtype=torch.float64)\n",
      "tensor([0.4597], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4597, 0.9949, 0.6094], dtype=torch.float64)\n",
      "tensor([0.5369], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5369, 1.0063, 0.6016], dtype=torch.float64)\n",
      "tensor([0.5739], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5739, 1.0333, 0.5938], dtype=torch.float64)\n",
      "tensor([0.6577], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6577, 1.0123, 0.5859], dtype=torch.float64)\n",
      "tensor([0.5960], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5960, 1.0250, 0.5781], dtype=torch.float64)\n",
      "tensor([0.6326], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6326, 1.0405, 0.5703], dtype=torch.float64)\n",
      "tensor([0.6808], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6808, 1.0450, 0.5625], dtype=torch.float64)\n",
      "tensor([0.6950], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6950, 1.0439, 0.5547], dtype=torch.float64)\n",
      "tensor([0.6922], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6922, 1.0589, 0.5469], dtype=torch.float64)\n",
      "tensor([0.7268], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7268, 1.0719, 0.5391], dtype=torch.float64)\n",
      "tensor([0.7576], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7576, 1.0716, 0.5312], dtype=torch.float64)\n",
      "tensor([0.7588], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7588, 1.0833, 0.5234], dtype=torch.float64)\n",
      "tensor([0.7843], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7843, 1.0791, 0.5156], dtype=torch.float64)\n",
      "tensor([0.7767], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7767, 1.0919, 0.5078], dtype=torch.float64)\n",
      "tensor([0.8015], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8015, 1.0975, 0.5000], dtype=torch.float64)\n",
      "tensor([0.8114], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8114, 1.0858, 0.4922], dtype=torch.float64)\n",
      "tensor([0.7929], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7929, 1.0741, 0.4844], dtype=torch.float64)\n",
      "tensor([0.7663], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7663, 1.0758, 0.4766], dtype=torch.float64)\n",
      "tensor([0.7684], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7684, 1.0666, 0.4688], dtype=torch.float64)\n",
      "tensor([0.7483], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7483, 1.0730, 0.4609], dtype=torch.float64)\n",
      "tensor([0.7610], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7610, 1.0779, 0.4531], dtype=torch.float64)\n",
      "tensor([0.7724], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7724, 1.0814, 0.4453], dtype=torch.float64)\n",
      "tensor([0.7810], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7810, 1.0722, 0.4375], dtype=torch.float64)\n",
      "tensor([0.7612], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7612, 1.0692, 0.4297], dtype=torch.float64)\n",
      "tensor([0.7535], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7535, 1.0576, 0.4219], dtype=torch.float64)\n",
      "tensor([0.7275], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7275, 1.0887, 0.4141], dtype=torch.float64)\n",
      "tensor([0.7940], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7940, 1.0910, 0.4062], dtype=torch.float64)\n",
      "tensor([0.8005], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8005, 1.0991, 0.3984], dtype=torch.float64)\n",
      "tensor([0.8136], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8136, 1.1191, 0.3906], dtype=torch.float64)\n",
      "tensor([0.8457], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8457, 1.1228, 0.3828], dtype=torch.float64)\n",
      "tensor([0.8530], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8530, 1.1372, 0.3750], dtype=torch.float64)\n",
      "tensor([0.8759], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8759, 1.1284, 0.3672], dtype=torch.float64)\n",
      "tensor([0.8631], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8631, 1.1426, 0.3594], dtype=torch.float64)\n",
      "tensor([0.8848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8848, 1.1354, 0.3516], dtype=torch.float64)\n",
      "tensor([0.8745], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8745, 1.1475, 0.3438], dtype=torch.float64)\n",
      "tensor([0.8930], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8930, 1.1643, 0.3359], dtype=torch.float64)\n",
      "tensor([0.9203], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9203, 1.1890, 0.3281], dtype=torch.float64)\n",
      "tensor([0.9603], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9603, 1.2169, 0.3203], dtype=torch.float64)\n",
      "tensor([1.0059], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0059, 1.2214, 0.3125], dtype=torch.float64)\n",
      "tensor([1.0151], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0151, 1.2131, 0.3047], dtype=torch.float64)\n",
      "tensor([1.0026], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0026, 1.1962, 0.2969], dtype=torch.float64)\n",
      "tensor([0.9753], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9753, 1.1894, 0.2891], dtype=torch.float64)\n",
      "tensor([0.9633], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9633, 1.2106, 0.2812], dtype=torch.float64)\n",
      "tensor([0.9961], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9961, 1.1866, 0.2734], dtype=torch.float64)\n",
      "tensor([0.9599], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9599, 1.1989, 0.2656], dtype=torch.float64)\n",
      "tensor([0.9775], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9775, 1.1839, 0.2578], dtype=torch.float64)\n",
      "tensor([0.9548], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9548, 1.1692, 0.2500], dtype=torch.float64)\n",
      "tensor([0.9305], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9305, 1.1543, 0.2422], dtype=torch.float64)\n",
      "tensor([0.9060], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9060, 1.1366, 0.2344], dtype=torch.float64)\n",
      "tensor([0.8770], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8770, 1.1668, 0.2266], dtype=torch.float64)\n",
      "tensor([0.9232], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9232, 1.1513, 0.2188], dtype=torch.float64)\n",
      "tensor([0.9009], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9009, 1.1425, 0.2109], dtype=torch.float64)\n",
      "tensor([0.8860], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8860, 1.1497, 0.2031], dtype=torch.float64)\n",
      "tensor([0.8966], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8966, 1.1473, 0.1953], dtype=torch.float64)\n",
      "tensor([0.8933], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8933, 1.1245, 0.1875], dtype=torch.float64)\n",
      "tensor([0.8573], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8573, 1.1136, 0.1797], dtype=torch.float64)\n",
      "tensor([0.8384], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8384, 1.1034, 0.1719], dtype=torch.float64)\n",
      "tensor([0.8216], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8216, 1.1103, 0.1641], dtype=torch.float64)\n",
      "tensor([0.8317], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8317, 1.1202, 0.1562], dtype=torch.float64)\n",
      "tensor([0.8476], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8476, 1.1194, 0.1484], dtype=torch.float64)\n",
      "tensor([0.8471], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8471, 1.1217, 0.1406], dtype=torch.float64)\n",
      "tensor([0.8506], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8506, 1.1376, 0.1328], dtype=torch.float64)\n",
      "tensor([0.8758], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8758, 1.1438, 0.1250], dtype=torch.float64)\n",
      "tensor([0.8867], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8867, 1.1571, 0.1172], dtype=torch.float64)\n",
      "tensor([0.9081], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9081, 1.1628, 0.1094], dtype=torch.float64)\n",
      "tensor([0.9181], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9181, 1.1595, 0.1016], dtype=torch.float64)\n",
      "tensor([0.9133], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9133, 1.1405, 0.0938], dtype=torch.float64)\n",
      "tensor([0.8832], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8832, 1.1251, 0.0859], dtype=torch.float64)\n",
      "tensor([0.8576], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8576, 1.1030, 0.0781], dtype=torch.float64)\n",
      "tensor([0.8215], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8215, 1.1050, 0.0703], dtype=torch.float64)\n",
      "tensor([0.8231], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8231, 1.0996, 0.0625], dtype=torch.float64)\n",
      "tensor([0.8143], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8143, 1.1165, 0.0547], dtype=torch.float64)\n",
      "tensor([0.8408], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8408, 1.1181, 0.0469], dtype=torch.float64)\n",
      "tensor([0.8445], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8445, 1.1252, 0.0391], dtype=torch.float64)\n",
      "tensor([0.8558], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8558, 1.1470, 0.0312], dtype=torch.float64)\n",
      "tensor([0.8906], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8906, 1.1428, 0.0234], dtype=torch.float64)\n",
      "tensor([0.8856], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8856, 1.1594, 0.0156], dtype=torch.float64)\n",
      "tensor([0.9115], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9115, 1.1507, 0.0078], dtype=torch.float64)\n",
      "tensor([0.8989], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.4607591117994787e-06 5.703076108150347e-07\n",
      "loss: [21.48069924]\n",
      "episode:  241\n",
      "loss: [34.51406134]\n",
      "episode:  242\n",
      "loss: [29.06851363]\n",
      "episode:  243\n",
      "loss: [23.39032257]\n",
      "episode:  244\n",
      "loss: [28.48743849]\n",
      "episode:  245\n",
      "loss: [26.40753214]\n",
      "episode:  246\n",
      "loss: [60.82737019]\n",
      "episode:  247\n",
      "loss: [53.82250844]\n",
      "episode:  248\n",
      "loss: [42.55327155]\n",
      "episode:  249\n",
      "loss: [40.50703235]\n",
      "episode:  250\n",
      "loss: [45.68500607]\n",
      "episode:  251\n",
      "loss: [60.68182429]\n",
      "episode:  252\n",
      "loss: [42.7425783]\n",
      "episode:  253\n",
      "loss: [33.56792316]\n",
      "episode:  254\n",
      "loss: [32.01797263]\n",
      "episode:  255\n",
      "loss: [24.39071138]\n",
      "episode:  256\n",
      "loss: [25.22986596]\n",
      "episode:  257\n",
      "loss: [119.31454051]\n",
      "episode:  258\n",
      "loss: [35.11273904]\n",
      "episode:  259\n",
      "loss: [23.74107421]\n",
      "episode:  260\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3728], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3728, 0.9992, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5731], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5731, 1.0119, 0.9844], dtype=torch.float64)\n",
      "tensor([0.6238], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6238, 0.9914, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5695], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5695, 1.0122, 0.9688], dtype=torch.float64)\n",
      "tensor([0.6208], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6208, 1.0035, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5991], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5991, 0.9917, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5622], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5622, 0.9779, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5187], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5187, 0.9774, 0.9375], dtype=torch.float64)\n",
      "tensor([0.5115], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5115, 0.9562, 0.9297], dtype=torch.float64)\n",
      "tensor([0.4497], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4497, 0.9625, 0.9219], dtype=torch.float64)\n",
      "tensor([0.4598], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4598, 0.9519, 0.9141], dtype=torch.float64)\n",
      "tensor([0.4293], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4293, 0.9590, 0.9062], dtype=torch.float64)\n",
      "tensor([0.4445], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4445, 0.9620, 0.8984], dtype=torch.float64)\n",
      "tensor([0.4522], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4522, 0.9668, 0.8906], dtype=torch.float64)\n",
      "tensor([0.4643], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4643, 0.9697, 0.8828], dtype=torch.float64)\n",
      "tensor([0.4715], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4715, 0.9685, 0.8750], dtype=torch.float64)\n",
      "tensor([0.4669], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4669, 0.9750, 0.8672], dtype=torch.float64)\n",
      "tensor([0.4827], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4827, 0.9867, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5146], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5146, 0.9619, 0.8516], dtype=torch.float64)\n",
      "tensor([0.4464], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4464, 0.9578, 0.8438], dtype=torch.float64)\n",
      "tensor([0.4270], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4270, 0.9420, 0.8359], dtype=torch.float64)\n",
      "tensor([0.3792], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3792, 0.9621, 0.8281], dtype=torch.float64)\n",
      "tensor([0.4291], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4291, 0.9600, 0.8203], dtype=torch.float64)\n",
      "tensor([0.4258], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4258, 0.9893, 0.8125], dtype=torch.float64)\n",
      "tensor([0.5053], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5053, 0.9806, 0.8047], dtype=torch.float64)\n",
      "tensor([0.4861], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4861, 0.9982, 0.7969], dtype=torch.float64)\n",
      "tensor([0.5342], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5342, 1.0013, 0.7891], dtype=torch.float64)\n",
      "tensor([0.5479], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5479, 0.9947, 0.7812], dtype=torch.float64)\n",
      "tensor([0.5290], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5290, 0.9847, 0.7734], dtype=torch.float64)\n",
      "tensor([0.4963], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4963, 0.9719, 0.7656], dtype=torch.float64)\n",
      "tensor([0.4539], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4539, 0.9620, 0.7578], dtype=torch.float64)\n",
      "tensor([0.4194], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4194, 0.9612, 0.7500], dtype=torch.float64)\n",
      "tensor([0.4130], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4130, 0.9754, 0.7422], dtype=torch.float64)\n",
      "tensor([0.4548], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4548, 0.9886, 0.7344], dtype=torch.float64)\n",
      "tensor([0.4984], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4984, 0.9849, 0.7266], dtype=torch.float64)\n",
      "tensor([0.4910], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4910, 0.9836, 0.7188], dtype=torch.float64)\n",
      "tensor([0.4859], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4859, 0.9682, 0.7109], dtype=torch.float64)\n",
      "tensor([0.4383], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4383, 0.9625, 0.7031], dtype=torch.float64)\n",
      "tensor([0.4159], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4159, 0.9512, 0.6953], dtype=torch.float64)\n",
      "tensor([0.3790], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3790, 0.9604, 0.6875], dtype=torch.float64)\n",
      "tensor([0.4027], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4027, 0.9631, 0.6797], dtype=torch.float64)\n",
      "tensor([0.4128], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4128, 0.9481, 0.6719], dtype=torch.float64)\n",
      "tensor([0.3679], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3679, 0.9355, 0.6641], dtype=torch.float64)\n",
      "tensor([0.3231], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3231, 0.9491, 0.6562], dtype=torch.float64)\n",
      "tensor([0.3595], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3595, 0.9663, 0.6484], dtype=torch.float64)\n",
      "tensor([0.4160], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4160, 0.9574, 0.6406], dtype=torch.float64)\n",
      "tensor([0.3945], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3945, 0.9414, 0.6328], dtype=torch.float64)\n",
      "tensor([0.3410], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3410, 0.9523, 0.6250], dtype=torch.float64)\n",
      "tensor([0.3681], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3681, 0.9255, 0.6172], dtype=torch.float64)\n",
      "tensor([0.2861], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2861, 0.9236, 0.6094], dtype=torch.float64)\n",
      "tensor([0.2694], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2694, 0.9235, 0.6016], dtype=torch.float64)\n",
      "tensor([0.2660], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2660, 0.8995, 0.5938], dtype=torch.float64)\n",
      "tensor([0.1912], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1912, 0.9151, 0.5859], dtype=torch.float64)\n",
      "tensor([0.2276], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2276, 0.9162, 0.5781], dtype=torch.float64)\n",
      "tensor([0.2344], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2344, 0.9042, 0.5703], dtype=torch.float64)\n",
      "tensor([0.1987], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1987, 0.8902, 0.5625], dtype=torch.float64)\n",
      "tensor([0.1601], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1601, 0.8754, 0.5547], dtype=torch.float64)\n",
      "tensor([0.1366], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1366, 0.8922, 0.5469], dtype=torch.float64)\n",
      "tensor([0.1588], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1588, 0.9080, 0.5391], dtype=torch.float64)\n",
      "tensor([0.1985], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1985, 0.9044, 0.5312], dtype=torch.float64)\n",
      "tensor([0.1915], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1915, 0.8952, 0.5234], dtype=torch.float64)\n",
      "tensor([0.1664], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1664, 0.9154, 0.5156], dtype=torch.float64)\n",
      "tensor([0.2179], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2179, 0.9079, 0.5078], dtype=torch.float64)\n",
      "tensor([0.2011], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2011, 0.9207, 0.5000], dtype=torch.float64)\n",
      "tensor([0.2367], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2367, 0.9398, 0.4922], dtype=torch.float64)\n",
      "tensor([0.3012], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3012, 0.9453, 0.4844], dtype=torch.float64)\n",
      "tensor([0.3250], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3250, 0.9511, 0.4766], dtype=torch.float64)\n",
      "tensor([0.3453], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3453, 0.9523, 0.4688], dtype=torch.float64)\n",
      "tensor([0.3504], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3504, 0.9478, 0.4609], dtype=torch.float64)\n",
      "tensor([0.3358], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3358, 0.9495, 0.4531], dtype=torch.float64)\n",
      "tensor([0.3387], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3387, 0.9234, 0.4453], dtype=torch.float64)\n",
      "tensor([0.2551], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2551, 0.9158, 0.4375], dtype=torch.float64)\n",
      "tensor([0.2206], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2206, 0.9333, 0.4297], dtype=torch.float64)\n",
      "tensor([0.2712], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2712, 0.9230, 0.4219], dtype=torch.float64)\n",
      "tensor([0.2431], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2431, 0.9215, 0.4141], dtype=torch.float64)\n",
      "tensor([0.2339], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2339, 0.9083, 0.4062], dtype=torch.float64)\n",
      "tensor([0.1936], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1936, 0.9007, 0.3984], dtype=torch.float64)\n",
      "tensor([0.1692], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1692, 0.9179, 0.3906], dtype=torch.float64)\n",
      "tensor([0.2125], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2125, 0.9034, 0.3828], dtype=torch.float64)\n",
      "tensor([0.1759], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1759, 0.8941, 0.3750], dtype=torch.float64)\n",
      "tensor([0.1552], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1552, 0.8794, 0.3672], dtype=torch.float64)\n",
      "tensor([0.1328], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1328, 0.8693, 0.3594], dtype=torch.float64)\n",
      "tensor([0.1199], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1199, 0.8556, 0.3516], dtype=torch.float64)\n",
      "tensor([0.1159], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1159, 0.8491, 0.3438], dtype=torch.float64)\n",
      "tensor([0.1151], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1151, 0.8565, 0.3359], dtype=torch.float64)\n",
      "tensor([0.1160], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1160, 0.8637, 0.3281], dtype=torch.float64)\n",
      "tensor([0.1176], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1176, 0.8528, 0.3203], dtype=torch.float64)\n",
      "tensor([0.1152], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1152, 0.8727, 0.3125], dtype=torch.float64)\n",
      "tensor([0.1204], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1204, 0.8451, 0.3047], dtype=torch.float64)\n",
      "tensor([0.1150], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1150, 0.8480, 0.2969], dtype=torch.float64)\n",
      "tensor([0.1150], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1150, 0.8540, 0.2891], dtype=torch.float64)\n",
      "tensor([0.1154], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1154, 0.8379, 0.2812], dtype=torch.float64)\n",
      "tensor([0.1148], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1148, 0.8311, 0.2734], dtype=torch.float64)\n",
      "tensor([0.1146], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1146, 0.8216, 0.2656], dtype=torch.float64)\n",
      "tensor([0.1145], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1145, 0.8260, 0.2578], dtype=torch.float64)\n",
      "tensor([0.1145], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1145, 0.8077, 0.2500], dtype=torch.float64)\n",
      "tensor([0.1145], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1145, 0.8097, 0.2422], dtype=torch.float64)\n",
      "tensor([0.1145], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1145, 0.8298, 0.2344], dtype=torch.float64)\n",
      "tensor([0.1146], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1146, 0.8345, 0.2266], dtype=torch.float64)\n",
      "tensor([0.1147], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1147, 0.8282, 0.2188], dtype=torch.float64)\n",
      "tensor([0.1145], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1145, 0.8243, 0.2109], dtype=torch.float64)\n",
      "tensor([0.1145], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1145, 0.8221, 0.2031], dtype=torch.float64)\n",
      "tensor([0.1145], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1145, 0.8306, 0.1953], dtype=torch.float64)\n",
      "tensor([0.1146], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1146, 0.8329, 0.1875], dtype=torch.float64)\n",
      "tensor([0.1146], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1146, 0.8339, 0.1797], dtype=torch.float64)\n",
      "tensor([0.1147], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1147, 0.8285, 0.1719], dtype=torch.float64)\n",
      "tensor([0.1145], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1145, 0.8332, 0.1641], dtype=torch.float64)\n",
      "tensor([0.1146], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1146, 0.8382, 0.1562], dtype=torch.float64)\n",
      "tensor([0.1148], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1148, 0.8392, 0.1484], dtype=torch.float64)\n",
      "tensor([0.1148], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1148, 0.8204, 0.1406], dtype=torch.float64)\n",
      "tensor([0.1145], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1145, 0.8333, 0.1328], dtype=torch.float64)\n",
      "tensor([0.1146], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1146, 0.8414, 0.1250], dtype=torch.float64)\n",
      "tensor([0.1149], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1149, 0.8466, 0.1172], dtype=torch.float64)\n",
      "tensor([0.1150], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1150, 0.8286, 0.1094], dtype=torch.float64)\n",
      "tensor([0.1145], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1145, 0.8366, 0.1016], dtype=torch.float64)\n",
      "tensor([0.1147], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1147, 0.8384, 0.0938], dtype=torch.float64)\n",
      "tensor([0.1148], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1148, 0.8519, 0.0859], dtype=torch.float64)\n",
      "tensor([0.1151], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1151, 0.8568, 0.0781], dtype=torch.float64)\n",
      "tensor([0.1157], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1157, 0.8585, 0.0703], dtype=torch.float64)\n",
      "tensor([0.1161], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1161, 0.8652, 0.0625], dtype=torch.float64)\n",
      "tensor([0.1177], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1177, 0.8465, 0.0547], dtype=torch.float64)\n",
      "tensor([0.1150], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1150, 0.8429, 0.0469], dtype=torch.float64)\n",
      "tensor([0.1149], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1149, 0.8451, 0.0391], dtype=torch.float64)\n",
      "tensor([0.1150], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1150, 0.8498, 0.0312], dtype=torch.float64)\n",
      "tensor([0.1151], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1151, 0.8515, 0.0234], dtype=torch.float64)\n",
      "tensor([0.1152], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1152, 0.8397, 0.0156], dtype=torch.float64)\n",
      "tensor([0.1149], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1149, 0.8432, 0.0078], dtype=torch.float64)\n",
      "tensor([0.1150], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.839622916122992e-06 7.208335590683857e-07\n",
      "loss: [27.8324123]\n",
      "episode:  261\n",
      "loss: [20.19107189]\n",
      "episode:  262\n",
      "loss: [27.16571177]\n",
      "episode:  263\n",
      "loss: [25.69926147]\n",
      "episode:  264\n",
      "loss: [25.34627715]\n",
      "episode:  265\n",
      "loss: [34.86427703]\n",
      "episode:  266\n",
      "loss: [21.55002461]\n",
      "episode:  267\n",
      "loss: [21.92886299]\n",
      "episode:  268\n",
      "loss: [20.58326624]\n",
      "episode:  269\n",
      "loss: [17.58731228]\n",
      "episode:  270\n",
      "loss: [38.9816834]\n",
      "episode:  271\n",
      "loss: [25.67937569]\n",
      "episode:  272\n",
      "loss: [47.68293265]\n",
      "episode:  273\n",
      "loss: [17.69877329]\n",
      "episode:  274\n",
      "loss: [34.55972704]\n",
      "episode:  275\n",
      "loss: [48.80155668]\n",
      "episode:  276\n",
      "loss: [26.87307265]\n",
      "episode:  277\n",
      "loss: [38.24598423]\n",
      "episode:  278\n",
      "loss: [41.01769902]\n",
      "episode:  279\n",
      "loss: [43.3958938]\n",
      "episode:  280\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.4276], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4276, 0.9931, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5883], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5883, 0.9970, 0.9844], dtype=torch.float64)\n",
      "tensor([0.6104], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6104, 0.9868, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5946], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5946, 0.9836, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5858], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5858, 0.9934, 0.9609], dtype=torch.float64)\n",
      "tensor([0.6015], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6015, 0.9883, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5928], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5928, 0.9777, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5716], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5716, 0.9617, 0.9375], dtype=torch.float64)\n",
      "tensor([0.5391], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5391, 0.9857, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5780], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5780, 0.9678, 0.9219], dtype=torch.float64)\n",
      "tensor([0.5487], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5487, 0.9698, 0.9141], dtype=torch.float64)\n",
      "tensor([0.5481], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5481, 0.9694, 0.9062], dtype=torch.float64)\n",
      "tensor([0.5462], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5462, 0.9747, 0.8984], dtype=torch.float64)\n",
      "tensor([0.5544], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5544, 0.9949, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5910], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5910, 0.9773, 0.8828], dtype=torch.float64)\n",
      "tensor([0.5618], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5618, 1.0044, 0.8750], dtype=torch.float64)\n",
      "tensor([0.6074], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6074, 1.0034, 0.8672], dtype=torch.float64)\n",
      "tensor([0.6104], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6104, 0.9857, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5769], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5769, 0.9988, 0.8516], dtype=torch.float64)\n",
      "tensor([0.5989], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5989, 0.9969, 0.8438], dtype=torch.float64)\n",
      "tensor([0.5976], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5976, 0.9706, 0.8359], dtype=torch.float64)\n",
      "tensor([0.5451], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5451, 0.9694, 0.8281], dtype=torch.float64)\n",
      "tensor([0.5365], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5365, 0.9775, 0.8203], dtype=torch.float64)\n",
      "tensor([0.5513], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5513, 0.9818, 0.8125], dtype=torch.float64)\n",
      "tensor([0.5617], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5617, 0.9800, 0.8047], dtype=torch.float64)\n",
      "tensor([0.5593], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5593, 0.9871, 0.7969], dtype=torch.float64)\n",
      "tensor([0.5730], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5730, 0.9777, 0.7891], dtype=torch.float64)\n",
      "tensor([0.5560], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5560, 0.9768, 0.7812], dtype=torch.float64)\n",
      "tensor([0.5521], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5521, 0.9523, 0.7734], dtype=torch.float64)\n",
      "tensor([0.4891], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4891, 0.9464, 0.7656], dtype=torch.float64)\n",
      "tensor([0.4650], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4650, 0.9599, 0.7578], dtype=torch.float64)\n",
      "tensor([0.5008], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5008, 0.9547, 0.7500], dtype=torch.float64)\n",
      "tensor([0.4899], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4899, 0.9416, 0.7422], dtype=torch.float64)\n",
      "tensor([0.4509], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4509, 0.9307, 0.7344], dtype=torch.float64)\n",
      "tensor([0.4151], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4151, 0.9260, 0.7266], dtype=torch.float64)\n",
      "tensor([0.3970], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3970, 0.9429, 0.7188], dtype=torch.float64)\n",
      "tensor([0.4436], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4436, 0.9303, 0.7109], dtype=torch.float64)\n",
      "tensor([0.4129], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4129, 0.9514, 0.7031], dtype=torch.float64)\n",
      "tensor([0.4697], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4697, 0.9497, 0.6953], dtype=torch.float64)\n",
      "tensor([0.4711], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4711, 0.9324, 0.6875], dtype=torch.float64)\n",
      "tensor([0.4217], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4217, 0.9435, 0.6797], dtype=torch.float64)\n",
      "tensor([0.4476], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4476, 0.9329, 0.6719], dtype=torch.float64)\n",
      "tensor([0.4202], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4202, 0.9318, 0.6641], dtype=torch.float64)\n",
      "tensor([0.4137], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4137, 0.9288, 0.6562], dtype=torch.float64)\n",
      "tensor([0.4041], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4041, 0.9175, 0.6484], dtype=torch.float64)\n",
      "tensor([0.3698], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3698, 0.9131, 0.6406], dtype=torch.float64)\n",
      "tensor([0.3528], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3528, 0.9161, 0.6328], dtype=torch.float64)\n",
      "tensor([0.3594], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3594, 0.9094, 0.6250], dtype=torch.float64)\n",
      "tensor([0.3405], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3405, 0.9041, 0.6172], dtype=torch.float64)\n",
      "tensor([0.3223], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3223, 0.9089, 0.6094], dtype=torch.float64)\n",
      "tensor([0.3338], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3338, 0.9158, 0.6016], dtype=torch.float64)\n",
      "tensor([0.3558], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3558, 0.9112, 0.5938], dtype=torch.float64)\n",
      "tensor([0.3446], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3446, 0.9069, 0.5859], dtype=torch.float64)\n",
      "tensor([0.3292], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3292, 0.9156, 0.5781], dtype=torch.float64)\n",
      "tensor([0.3530], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3530, 0.9151, 0.5703], dtype=torch.float64)\n",
      "tensor([0.3546], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3546, 0.9105, 0.5625], dtype=torch.float64)\n",
      "tensor([0.3402], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3402, 0.9130, 0.5547], dtype=torch.float64)\n",
      "tensor([0.3450], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3450, 0.9299, 0.5469], dtype=torch.float64)\n",
      "tensor([0.3972], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3972, 0.9273, 0.5391], dtype=torch.float64)\n",
      "tensor([0.3964], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3964, 0.9272, 0.5312], dtype=torch.float64)\n",
      "tensor([0.3954], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3954, 0.9291, 0.5234], dtype=torch.float64)\n",
      "tensor([0.4005], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4005, 0.9353, 0.5156], dtype=torch.float64)\n",
      "tensor([0.4197], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4197, 0.9368, 0.5078], dtype=torch.float64)\n",
      "tensor([0.4261], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4261, 0.9356, 0.5000], dtype=torch.float64)\n",
      "tensor([0.4232], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4232, 0.9403, 0.4922], dtype=torch.float64)\n",
      "tensor([0.4364], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4364, 0.9198, 0.4844], dtype=torch.float64)\n",
      "tensor([0.3754], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3754, 0.9093, 0.4766], dtype=torch.float64)\n",
      "tensor([0.3330], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3330, 0.9053, 0.4688], dtype=torch.float64)\n",
      "tensor([0.3139], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3139, 0.9126, 0.4609], dtype=torch.float64)\n",
      "tensor([0.3326], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3326, 0.9024, 0.4531], dtype=torch.float64)\n",
      "tensor([0.3035], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3035, 0.9004, 0.4453], dtype=torch.float64)\n",
      "tensor([0.2925], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2925, 0.9151, 0.4375], dtype=torch.float64)\n",
      "tensor([0.3354], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3354, 0.9179, 0.4297], dtype=torch.float64)\n",
      "tensor([0.3499], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3499, 0.9130, 0.4219], dtype=torch.float64)\n",
      "tensor([0.3364], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3364, 0.9160, 0.4141], dtype=torch.float64)\n",
      "tensor([0.3431], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3431, 0.9268, 0.4062], dtype=torch.float64)\n",
      "tensor([0.3769], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3769, 0.9335, 0.3984], dtype=torch.float64)\n",
      "tensor([0.4019], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4019, 0.9344, 0.3906], dtype=torch.float64)\n",
      "tensor([0.4079], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4079, 0.9315, 0.3828], dtype=torch.float64)\n",
      "tensor([0.3992], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3992, 0.9516, 0.3750], dtype=torch.float64)\n",
      "tensor([0.4579], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4579, 0.9582, 0.3672], dtype=torch.float64)\n",
      "tensor([0.4860], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4860, 0.9628, 0.3594], dtype=torch.float64)\n",
      "tensor([0.5036], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5036, 0.9892, 0.3516], dtype=torch.float64)\n",
      "tensor([0.5772], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5772, 0.9676, 0.3438], dtype=torch.float64)\n",
      "tensor([0.5303], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5303, 0.9622, 0.3359], dtype=torch.float64)\n",
      "tensor([0.5067], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5067, 0.9624, 0.3281], dtype=torch.float64)\n",
      "tensor([0.5030], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5030, 0.9481, 0.3203], dtype=torch.float64)\n",
      "tensor([0.4592], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4592, 0.9631, 0.3125], dtype=torch.float64)\n",
      "tensor([0.4969], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4969, 0.9559, 0.3047], dtype=torch.float64)\n",
      "tensor([0.4802], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4802, 0.9388, 0.2969], dtype=torch.float64)\n",
      "tensor([0.4258], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4258, 0.9307, 0.2891], dtype=torch.float64)\n",
      "tensor([0.3925], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3925, 0.9402, 0.2812], dtype=torch.float64)\n",
      "tensor([0.4160], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4160, 0.9326, 0.2734], dtype=torch.float64)\n",
      "tensor([0.3958], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3958, 0.9581, 0.2656], dtype=torch.float64)\n",
      "tensor([0.4690], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4690, 0.9693, 0.2578], dtype=torch.float64)\n",
      "tensor([0.5129], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5129, 0.9671, 0.2500], dtype=torch.float64)\n",
      "tensor([0.5123], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5123, 0.9865, 0.2422], dtype=torch.float64)\n",
      "tensor([0.5699], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5699, 0.9610, 0.2344], dtype=torch.float64)\n",
      "tensor([0.5015], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5015, 0.9557, 0.2266], dtype=torch.float64)\n",
      "tensor([0.4746], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4746, 0.9679, 0.2188], dtype=torch.float64)\n",
      "tensor([0.5068], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5068, 0.9645, 0.2109], dtype=torch.float64)\n",
      "tensor([0.5007], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5007, 0.9784, 0.2031], dtype=torch.float64)\n",
      "tensor([0.5409], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5409, 0.9758, 0.1953], dtype=torch.float64)\n",
      "tensor([0.5387], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5387, 0.9643, 0.1875], dtype=torch.float64)\n",
      "tensor([0.5033], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5033, 0.9543, 0.1797], dtype=torch.float64)\n",
      "tensor([0.4672], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4672, 0.9504, 0.1719], dtype=torch.float64)\n",
      "tensor([0.4496], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4496, 0.9488, 0.1641], dtype=torch.float64)\n",
      "tensor([0.4415], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4415, 0.9487, 0.1562], dtype=torch.float64)\n",
      "tensor([0.4395], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4395, 0.9392, 0.1484], dtype=torch.float64)\n",
      "tensor([0.4101], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4101, 0.9666, 0.1406], dtype=torch.float64)\n",
      "tensor([0.4876], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4876, 0.9798, 0.1328], dtype=torch.float64)\n",
      "tensor([0.5381], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5381, 0.9727, 0.1250], dtype=torch.float64)\n",
      "tensor([0.5238], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5238, 0.9724, 0.1172], dtype=torch.float64)\n",
      "tensor([0.5203], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5203, 0.9838, 0.1094], dtype=torch.float64)\n",
      "tensor([0.5533], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5533, 0.9641, 0.1016], dtype=torch.float64)\n",
      "tensor([0.4984], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4984, 0.9623, 0.0938], dtype=torch.float64)\n",
      "tensor([0.4844], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4844, 0.9412, 0.0859], dtype=torch.float64)\n",
      "tensor([0.4183], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4183, 0.9328, 0.0781], dtype=torch.float64)\n",
      "tensor([0.3821], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3821, 0.9238, 0.0703], dtype=torch.float64)\n",
      "tensor([0.3482], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3482, 0.9374, 0.0625], dtype=torch.float64)\n",
      "tensor([0.3842], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3842, 0.9217, 0.0547], dtype=torch.float64)\n",
      "tensor([0.3410], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3410, 0.9396, 0.0469], dtype=torch.float64)\n",
      "tensor([0.3888], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3888, 0.9317, 0.0391], dtype=torch.float64)\n",
      "tensor([0.3712], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3712, 0.9474, 0.0312], dtype=torch.float64)\n",
      "tensor([0.4160], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4160, 0.9380, 0.0234], dtype=torch.float64)\n",
      "tensor([0.3935], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3935, 0.9319, 0.0156], dtype=torch.float64)\n",
      "tensor([0.3708], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3708, 0.9396, 0.0078], dtype=torch.float64)\n",
      "tensor([0.3904], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  2.8995275063102885e-06 6.900545603696568e-07\n",
      "loss: [41.95540033]\n",
      "episode:  281\n",
      "loss: [26.27713931]\n",
      "episode:  282\n",
      "loss: [40.50990198]\n",
      "episode:  283\n",
      "loss: [39.6395345]\n",
      "episode:  284\n",
      "loss: [35.01566794]\n",
      "episode:  285\n",
      "loss: [23.79909816]\n",
      "episode:  286\n",
      "loss: [31.31897835]\n",
      "episode:  287\n",
      "loss: [16.27934688]\n",
      "episode:  288\n",
      "loss: [18.94768433]\n",
      "episode:  289\n",
      "loss: [17.3537707]\n",
      "episode:  290\n",
      "loss: [17.4864409]\n",
      "episode:  291\n",
      "loss: [19.0499013]\n",
      "episode:  292\n",
      "loss: [26.72366179]\n",
      "episode:  293\n",
      "loss: [19.61644457]\n",
      "episode:  294\n",
      "loss: [21.11355848]\n",
      "episode:  295\n",
      "loss: [27.27635576]\n",
      "episode:  296\n",
      "loss: [16.68621233]\n",
      "episode:  297\n",
      "loss: [15.88216299]\n",
      "episode:  298\n",
      "loss: [25.27509947]\n",
      "episode:  299\n",
      "loss: [13.80693235]\n",
      "episode:  300\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3548], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3548, 0.9774, 0.9922], dtype=torch.float64)\n",
      "tensor([0.4933], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4933, 0.9871, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5374], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5374, 0.9955, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5653], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5653, 0.9882, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5470], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5470, 0.9975, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5690], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5690, 0.9757, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5094], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5094, 0.9830, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5201], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5201, 1.0173, 0.9375], dtype=torch.float64)\n",
      "tensor([0.6043], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6043, 1.0103, 0.9297], dtype=torch.float64)\n",
      "tensor([0.6010], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6010, 1.0334, 0.9219], dtype=torch.float64)\n",
      "tensor([0.6432], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6432, 1.0418, 0.9141], dtype=torch.float64)\n",
      "tensor([0.6631], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6631, 1.0332, 0.9062], dtype=torch.float64)\n",
      "tensor([0.6490], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6490, 1.0316, 0.8984], dtype=torch.float64)\n",
      "tensor([0.6432], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6432, 1.0354, 0.8906], dtype=torch.float64)\n",
      "tensor([0.6486], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6486, 1.0174, 0.8828], dtype=torch.float64)\n",
      "tensor([0.6143], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6143, 1.0149, 0.8750], dtype=torch.float64)\n",
      "tensor([0.6042], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6042, 1.0099, 0.8672], dtype=torch.float64)\n",
      "tensor([0.5925], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5925, 1.0122, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5944], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5944, 1.0153, 0.8516], dtype=torch.float64)\n",
      "tensor([0.5994], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5994, 1.0094, 0.8438], dtype=torch.float64)\n",
      "tensor([0.5858], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5858, 1.0094, 0.8359], dtype=torch.float64)\n",
      "tensor([0.5824], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5824, 1.0126, 0.8281], dtype=torch.float64)\n",
      "tensor([0.5898], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5898, 1.0301, 0.8203], dtype=torch.float64)\n",
      "tensor([0.6286], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6286, 1.0278, 0.8125], dtype=torch.float64)\n",
      "tensor([0.6296], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6296, 1.0179, 0.8047], dtype=torch.float64)\n",
      "tensor([0.6095], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6095, 1.0091, 0.7969], dtype=torch.float64)\n",
      "tensor([0.5835], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5835, 1.0076, 0.7891], dtype=torch.float64)\n",
      "tensor([0.5752], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5752, 1.0154, 0.7812], dtype=torch.float64)\n",
      "tensor([0.5974], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5974, 0.9911, 0.7734], dtype=torch.float64)\n",
      "tensor([0.5270], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5270, 0.9795, 0.7656], dtype=torch.float64)\n",
      "tensor([0.4809], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4809, 0.9562, 0.7578], dtype=torch.float64)\n",
      "tensor([0.4031], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4031, 0.9783, 0.7500], dtype=torch.float64)\n",
      "tensor([0.4580], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4580, 0.9804, 0.7422], dtype=torch.float64)\n",
      "tensor([0.4732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4732, 0.9876, 0.7344], dtype=torch.float64)\n",
      "tensor([0.4973], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4973, 0.9808, 0.7266], dtype=torch.float64)\n",
      "tensor([0.4806], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4806, 0.9734, 0.7188], dtype=torch.float64)\n",
      "tensor([0.4555], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4555, 0.9914, 0.7109], dtype=torch.float64)\n",
      "tensor([0.5061], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5061, 0.9740, 0.7031], dtype=torch.float64)\n",
      "tensor([0.4613], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4613, 0.9659, 0.6953], dtype=torch.float64)\n",
      "tensor([0.4293], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4293, 0.9887, 0.6875], dtype=torch.float64)\n",
      "tensor([0.4940], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4940, 1.0048, 0.6797], dtype=torch.float64)\n",
      "tensor([0.5533], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5533, 1.0007, 0.6719], dtype=torch.float64)\n",
      "tensor([0.5499], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5499, 0.9838, 0.6641], dtype=torch.float64)\n",
      "tensor([0.4981], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4981, 0.9944, 0.6562], dtype=torch.float64)\n",
      "tensor([0.5223], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5223, 1.0029, 0.6484], dtype=torch.float64)\n",
      "tensor([0.5519], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5519, 1.0064, 0.6406], dtype=torch.float64)\n",
      "tensor([0.5673], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5673, 1.0049, 0.6328], dtype=torch.float64)\n",
      "tensor([0.5650], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5650, 0.9961, 0.6250], dtype=torch.float64)\n",
      "tensor([0.5381], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5381, 0.9926, 0.6172], dtype=torch.float64)\n",
      "tensor([0.5232], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5232, 0.9979, 0.6094], dtype=torch.float64)\n",
      "tensor([0.5370], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5370, 0.9778, 0.6016], dtype=torch.float64)\n",
      "tensor([0.4779], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4779, 0.9946, 0.5938], dtype=torch.float64)\n",
      "tensor([0.5201], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5201, 1.0050, 0.5859], dtype=torch.float64)\n",
      "tensor([0.5583], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5583, 0.9982, 0.5781], dtype=torch.float64)\n",
      "tensor([0.5434], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5434, 1.0066, 0.5703], dtype=torch.float64)\n",
      "tensor([0.5670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5670, 1.0002, 0.5625], dtype=torch.float64)\n",
      "tensor([0.5512], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5512, 1.0027, 0.5547], dtype=torch.float64)\n",
      "tensor([0.5561], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5561, 1.0044, 0.5469], dtype=torch.float64)\n",
      "tensor([0.5624], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5624, 1.0012, 0.5391], dtype=torch.float64)\n",
      "tensor([0.5536], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5536, 1.0094, 0.5312], dtype=torch.float64)\n",
      "tensor([0.5772], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5772, 0.9980, 0.5234], dtype=torch.float64)\n",
      "tensor([0.5460], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5460, 1.0051, 0.5156], dtype=torch.float64)\n",
      "tensor([0.5629], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5629, 0.9953, 0.5078], dtype=torch.float64)\n",
      "tensor([0.5357], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5357, 0.9813, 0.5000], dtype=torch.float64)\n",
      "tensor([0.4891], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4891, 0.9702, 0.4922], dtype=torch.float64)\n",
      "tensor([0.4480], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4480, 0.9694, 0.4844], dtype=torch.float64)\n",
      "tensor([0.4389], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4389, 0.9615, 0.4766], dtype=torch.float64)\n",
      "tensor([0.4133], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4133, 0.9550, 0.4688], dtype=torch.float64)\n",
      "tensor([0.3892], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3892, 0.9643, 0.4609], dtype=torch.float64)\n",
      "tensor([0.4138], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4138, 0.9466, 0.4531], dtype=torch.float64)\n",
      "tensor([0.3613], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3613, 0.9484, 0.4453], dtype=torch.float64)\n",
      "tensor([0.3551], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3551, 0.9487, 0.4375], dtype=torch.float64)\n",
      "tensor([0.3543], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3543, 0.9440, 0.4297], dtype=torch.float64)\n",
      "tensor([0.3383], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3383, 0.9428, 0.4219], dtype=torch.float64)\n",
      "tensor([0.3304], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3304, 0.9352, 0.4141], dtype=torch.float64)\n",
      "tensor([0.3031], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3031, 0.9351, 0.4062], dtype=torch.float64)\n",
      "tensor([0.2966], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2966, 0.9358, 0.3984], dtype=torch.float64)\n",
      "tensor([0.2968], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2968, 0.9286, 0.3906], dtype=torch.float64)\n",
      "tensor([0.2727], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2727, 0.9270, 0.3828], dtype=torch.float64)\n",
      "tensor([0.2618], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2618, 0.9142, 0.3750], dtype=torch.float64)\n",
      "tensor([0.2174], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2174, 0.9025, 0.3672], dtype=torch.float64)\n",
      "tensor([0.1714], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1714, 0.9058, 0.3594], dtype=torch.float64)\n",
      "tensor([0.1718], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1718, 0.8864, 0.3516], dtype=torch.float64)\n",
      "tensor([0.1231], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1231, 0.8892, 0.3438], dtype=torch.float64)\n",
      "tensor([0.1220], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1220, 0.8979, 0.3359], dtype=torch.float64)\n",
      "tensor([0.1392], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1392, 0.9115, 0.3281], dtype=torch.float64)\n",
      "tensor([0.1800], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1800, 0.9082, 0.3203], dtype=torch.float64)\n",
      "tensor([0.1777], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1777, 0.9240, 0.3125], dtype=torch.float64)\n",
      "tensor([0.2263], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2263, 0.9166, 0.3047], dtype=torch.float64)\n",
      "tensor([0.2125], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2125, 0.9443, 0.2969], dtype=torch.float64)\n",
      "tensor([0.2988], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2988, 0.9595, 0.2891], dtype=torch.float64)\n",
      "tensor([0.3660], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3660, 0.9644, 0.2812], dtype=torch.float64)\n",
      "tensor([0.3958], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3958, 0.9808, 0.2734], dtype=torch.float64)\n",
      "tensor([0.4548], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4548, 0.9769, 0.2656], dtype=torch.float64)\n",
      "tensor([0.4543], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4543, 0.9727, 0.2578], dtype=torch.float64)\n",
      "tensor([0.4400], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4400, 0.9650, 0.2500], dtype=torch.float64)\n",
      "tensor([0.4112], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4112, 0.9883, 0.2422], dtype=torch.float64)\n",
      "tensor([0.4798], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4798, 0.9758, 0.2344], dtype=torch.float64)\n",
      "tensor([0.4538], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4538, 0.9742, 0.2266], dtype=torch.float64)\n",
      "tensor([0.4422], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4422, 0.9725, 0.2188], dtype=torch.float64)\n",
      "tensor([0.4338], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4338, 0.9694, 0.2109], dtype=torch.float64)\n",
      "tensor([0.4214], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4214, 0.9422, 0.2031], dtype=torch.float64)\n",
      "tensor([0.3297], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3297, 0.9517, 0.1953], dtype=torch.float64)\n",
      "tensor([0.3402], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3402, 0.9693, 0.1875], dtype=torch.float64)\n",
      "tensor([0.3993], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3993, 0.9626, 0.1797], dtype=torch.float64)\n",
      "tensor([0.3895], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3895, 0.9564, 0.1719], dtype=torch.float64)\n",
      "tensor([0.3668], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3668, 0.9591, 0.1641], dtype=torch.float64)\n",
      "tensor([0.3701], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3701, 0.9726, 0.1562], dtype=torch.float64)\n",
      "tensor([0.4138], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4138, 0.9512, 0.1484], dtype=torch.float64)\n",
      "tensor([0.3531], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3531, 0.9463, 0.1406], dtype=torch.float64)\n",
      "tensor([0.3238], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3238, 0.9473, 0.1328], dtype=torch.float64)\n",
      "tensor([0.3201], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3201, 0.9377, 0.1250], dtype=torch.float64)\n",
      "tensor([0.2876], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2876, 0.9724, 0.1172], dtype=torch.float64)\n",
      "tensor([0.3926], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3926, 0.9854, 0.1094], dtype=torch.float64)\n",
      "tensor([0.4567], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4567, 1.0053, 0.1016], dtype=torch.float64)\n",
      "tensor([0.5333], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5333, 1.0064, 0.0938], dtype=torch.float64)\n",
      "tensor([0.5525], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5525, 0.9961, 0.0859], dtype=torch.float64)\n",
      "tensor([0.5231], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5231, 0.9988, 0.0781], dtype=torch.float64)\n",
      "tensor([0.5249], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5249, 1.0110, 0.0703], dtype=torch.float64)\n",
      "tensor([0.5638], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5638, 0.9945, 0.0625], dtype=torch.float64)\n",
      "tensor([0.5185], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5185, 0.9955, 0.0547], dtype=torch.float64)\n",
      "tensor([0.5116], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5116, 1.0005, 0.0469], dtype=torch.float64)\n",
      "tensor([0.5257], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5257, 1.0017, 0.0391], dtype=torch.float64)\n",
      "tensor([0.5320], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5320, 1.0014, 0.0312], dtype=torch.float64)\n",
      "tensor([0.5317], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5317, 0.9990, 0.0234], dtype=torch.float64)\n",
      "tensor([0.5235], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5235, 1.0262, 0.0156], dtype=torch.float64)\n",
      "tensor([0.6081], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6081, 1.0207, 0.0078], dtype=torch.float64)\n",
      "tensor([0.6079], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.3945548903381885e-06 6.72090610065544e-07\n",
      "loss: [16.57672044]\n",
      "episode:  301\n",
      "loss: [19.92940395]\n",
      "episode:  302\n",
      "loss: [16.40257762]\n",
      "episode:  303\n",
      "loss: [19.07212804]\n",
      "episode:  304\n",
      "loss: [14.26128746]\n",
      "episode:  305\n",
      "loss: [19.56172655]\n",
      "episode:  306\n",
      "loss: [15.55127941]\n",
      "episode:  307\n",
      "loss: [17.93116516]\n",
      "episode:  308\n",
      "loss: [13.03319072]\n",
      "episode:  309\n",
      "loss: [16.13428676]\n",
      "episode:  310\n",
      "loss: [18.83263501]\n",
      "episode:  311\n",
      "loss: [16.46997344]\n",
      "episode:  312\n",
      "loss: [16.85158066]\n",
      "episode:  313\n",
      "loss: [13.27677336]\n",
      "episode:  314\n",
      "loss: [18.93438603]\n",
      "episode:  315\n",
      "loss: [25.11043568]\n",
      "episode:  316\n",
      "loss: [17.07895711]\n",
      "episode:  317\n",
      "loss: [14.9037904]\n",
      "episode:  318\n",
      "loss: [16.92588569]\n",
      "episode:  319\n",
      "loss: [16.12934191]\n",
      "episode:  320\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3492], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3492, 0.9911, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5309], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5309, 0.9797, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5248], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5248, 0.9989, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5775], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5775, 0.9981, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5818], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5818, 1.0090, 0.9609], dtype=torch.float64)\n",
      "tensor([0.6116], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6116, 0.9929, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5686], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5686, 0.9944, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5648], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5648, 0.9948, 0.9375], dtype=torch.float64)\n",
      "tensor([0.5637], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5637, 0.9921, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5543], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5543, 0.9952, 0.9219], dtype=torch.float64)\n",
      "tensor([0.5601], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5601, 1.0087, 0.9141], dtype=torch.float64)\n",
      "tensor([0.5981], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5981, 1.0028, 0.9062], dtype=torch.float64)\n",
      "tensor([0.5853], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5853, 1.0138, 0.8984], dtype=torch.float64)\n",
      "tensor([0.6132], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6132, 0.9971, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5680], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5680, 1.0044, 0.8828], dtype=torch.float64)\n",
      "tensor([0.5804], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5804, 0.9998, 0.8750], dtype=torch.float64)\n",
      "tensor([0.5675], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5675, 1.0166, 0.8672], dtype=torch.float64)\n",
      "tensor([0.6122], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6122, 0.9979, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5638], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5638, 0.9943, 0.8516], dtype=torch.float64)\n",
      "tensor([0.5442], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5442, 1.0103, 0.8438], dtype=torch.float64)\n",
      "tensor([0.5857], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5857, 1.0318, 0.8359], dtype=torch.float64)\n",
      "tensor([0.6436], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6436, 1.0432, 0.8281], dtype=torch.float64)\n",
      "tensor([0.6780], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6780, 1.0388, 0.8203], dtype=torch.float64)\n",
      "tensor([0.6747], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6747, 1.0350, 0.8125], dtype=torch.float64)\n",
      "tensor([0.6662], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6662, 1.0528, 0.8047], dtype=torch.float64)\n",
      "tensor([0.7022], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7022, 1.0489, 0.7969], dtype=torch.float64)\n",
      "tensor([0.7003], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7003, 1.0827, 0.7891], dtype=torch.float64)\n",
      "tensor([0.7648], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7648, 1.0802, 0.7812], dtype=torch.float64)\n",
      "tensor([0.7697], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7697, 1.0709, 0.7734], dtype=torch.float64)\n",
      "tensor([0.7530], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7530, 1.0602, 0.7656], dtype=torch.float64)\n",
      "tensor([0.7304], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7304, 1.0824, 0.7578], dtype=torch.float64)\n",
      "tensor([0.7699], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7699, 1.0941, 0.7500], dtype=torch.float64)\n",
      "tensor([0.7983], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7983, 1.0881, 0.7422], dtype=torch.float64)\n",
      "tensor([0.7912], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7912, 1.0917, 0.7344], dtype=torch.float64)\n",
      "tensor([0.7975], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7975, 1.0766, 0.7266], dtype=torch.float64)\n",
      "tensor([0.7700], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7700, 1.0895, 0.7188], dtype=torch.float64)\n",
      "tensor([0.7909], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7909, 1.0979, 0.7109], dtype=torch.float64)\n",
      "tensor([0.8103], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8103, 1.1007, 0.7031], dtype=torch.float64)\n",
      "tensor([0.8186], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8186, 1.1035, 0.6953], dtype=torch.float64)\n",
      "tensor([0.8242], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8242, 1.1081, 0.6875], dtype=torch.float64)\n",
      "tensor([0.8296], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8296, 1.0954, 0.6797], dtype=torch.float64)\n",
      "tensor([0.8125], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8125, 1.1160, 0.6719], dtype=torch.float64)\n",
      "tensor([0.8371], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8371, 1.1333, 0.6641], dtype=torch.float64)\n",
      "tensor([0.8571], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8571, 1.1322, 0.6562], dtype=torch.float64)\n",
      "tensor([0.8577], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8577, 1.1214, 0.6484], dtype=torch.float64)\n",
      "tensor([0.8468], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8468, 1.1077, 0.6406], dtype=torch.float64)\n",
      "tensor([0.8321], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8321, 1.1049, 0.6328], dtype=torch.float64)\n",
      "tensor([0.8282], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8282, 1.1362, 0.6250], dtype=torch.float64)\n",
      "tensor([0.8602], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8602, 1.1509, 0.6172], dtype=torch.float64)\n",
      "tensor([0.8780], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8780, 1.1523, 0.6094], dtype=torch.float64)\n",
      "tensor([0.8812], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8812, 1.1415, 0.6016], dtype=torch.float64)\n",
      "tensor([0.8705], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8705, 1.1483, 0.5938], dtype=torch.float64)\n",
      "tensor([0.8767], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8767, 1.1607, 0.5859], dtype=torch.float64)\n",
      "tensor([0.8902], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8902, 1.1621, 0.5781], dtype=torch.float64)\n",
      "tensor([0.8929], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8929, 1.1795, 0.5703], dtype=torch.float64)\n",
      "tensor([0.9112], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9112, 1.1906, 0.5625], dtype=torch.float64)\n",
      "tensor([0.9243], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9243, 1.1769, 0.5547], dtype=torch.float64)\n",
      "tensor([0.9114], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9114, 1.1567, 0.5469], dtype=torch.float64)\n",
      "tensor([0.8897], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8897, 1.1502, 0.5391], dtype=torch.float64)\n",
      "tensor([0.8816], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8816, 1.1667, 0.5312], dtype=torch.float64)\n",
      "tensor([0.8981], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8981, 1.1595, 0.5234], dtype=torch.float64)\n",
      "tensor([0.8922], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8922, 1.1446, 0.5156], dtype=torch.float64)\n",
      "tensor([0.8766], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8766, 1.1480, 0.5078], dtype=torch.float64)\n",
      "tensor([0.8790], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8790, 1.1572, 0.5000], dtype=torch.float64)\n",
      "tensor([0.8888], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8888, 1.1481, 0.4922], dtype=torch.float64)\n",
      "tensor([0.8805], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8805, 1.1645, 0.4844], dtype=torch.float64)\n",
      "tensor([0.8968], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8968, 1.1729, 0.4766], dtype=torch.float64)\n",
      "tensor([0.9070], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9070, 1.1789, 0.4688], dtype=torch.float64)\n",
      "tensor([0.9141], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9141, 1.1956, 0.4609], dtype=torch.float64)\n",
      "tensor([0.9318], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9318, 1.1892, 0.4531], dtype=torch.float64)\n",
      "tensor([0.9261], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9261, 1.1762, 0.4453], dtype=torch.float64)\n",
      "tensor([0.9126], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9126, 1.1765, 0.4375], dtype=torch.float64)\n",
      "tensor([0.9124], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9124, 1.1874, 0.4297], dtype=torch.float64)\n",
      "tensor([0.9236], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9236, 1.1779, 0.4219], dtype=torch.float64)\n",
      "tensor([0.9145], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9145, 1.1610, 0.4141], dtype=torch.float64)\n",
      "tensor([0.8967], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8967, 1.1691, 0.4062], dtype=torch.float64)\n",
      "tensor([0.9042], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9042, 1.1511, 0.3984], dtype=torch.float64)\n",
      "tensor([0.8862], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8862, 1.1533, 0.3906], dtype=torch.float64)\n",
      "tensor([0.8876], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8876, 1.1568, 0.3828], dtype=torch.float64)\n",
      "tensor([0.8914], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8914, 1.1376, 0.3750], dtype=torch.float64)\n",
      "tensor([0.8719], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8719, 1.1451, 0.3672], dtype=torch.float64)\n",
      "tensor([0.8788], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8788, 1.1389, 0.3594], dtype=torch.float64)\n",
      "tensor([0.8728], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8728, 1.1291, 0.3516], dtype=torch.float64)\n",
      "tensor([0.8624], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8624, 1.1394, 0.3438], dtype=torch.float64)\n",
      "tensor([0.8727], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8727, 1.1431, 0.3359], dtype=torch.float64)\n",
      "tensor([0.8771], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8771, 1.1561, 0.3281], dtype=torch.float64)\n",
      "tensor([0.8908], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8908, 1.1680, 0.3203], dtype=torch.float64)\n",
      "tensor([0.9037], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9037, 1.1832, 0.3125], dtype=torch.float64)\n",
      "tensor([0.9201], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9201, 1.1897, 0.3047], dtype=torch.float64)\n",
      "tensor([0.9276], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9276, 1.1938, 0.2969], dtype=torch.float64)\n",
      "tensor([0.9323], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9323, 1.1727, 0.2891], dtype=torch.float64)\n",
      "tensor([0.9109], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9109, 1.2006, 0.2812], dtype=torch.float64)\n",
      "tensor([0.9387], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9387, 1.1764, 0.2734], dtype=torch.float64)\n",
      "tensor([0.9151], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9151, 1.1576, 0.2656], dtype=torch.float64)\n",
      "tensor([0.8947], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8947, 1.1549, 0.2578], dtype=torch.float64)\n",
      "tensor([0.8910], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8910, 1.1500, 0.2500], dtype=torch.float64)\n",
      "tensor([0.8859], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8859, 1.1435, 0.2422], dtype=torch.float64)\n",
      "tensor([0.8790], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8790, 1.1459, 0.2344], dtype=torch.float64)\n",
      "tensor([0.8812], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8812, 1.1549, 0.2266], dtype=torch.float64)\n",
      "tensor([0.8907], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8907, 1.1540, 0.2188], dtype=torch.float64)\n",
      "tensor([0.8903], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8903, 1.1306, 0.2109], dtype=torch.float64)\n",
      "tensor([0.8662], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8662, 1.1137, 0.2031], dtype=torch.float64)\n",
      "tensor([0.8478], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8478, 1.1259, 0.1953], dtype=torch.float64)\n",
      "tensor([0.8596], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8596, 1.1201, 0.1875], dtype=torch.float64)\n",
      "tensor([0.8543], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8543, 1.1295, 0.1797], dtype=torch.float64)\n",
      "tensor([0.8638], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8638, 1.1237, 0.1719], dtype=torch.float64)\n",
      "tensor([0.8583], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8583, 1.1086, 0.1641], dtype=torch.float64)\n",
      "tensor([0.8426], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8426, 1.0790, 0.1562], dtype=torch.float64)\n",
      "tensor([0.8018], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8018, 1.0686, 0.1484], dtype=torch.float64)\n",
      "tensor([0.7786], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7786, 1.0659, 0.1406], dtype=torch.float64)\n",
      "tensor([0.7716], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7716, 1.0594, 0.1328], dtype=torch.float64)\n",
      "tensor([0.7587], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7587, 1.0797, 0.1250], dtype=torch.float64)\n",
      "tensor([0.7965], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7965, 1.0763, 0.1172], dtype=torch.float64)\n",
      "tensor([0.7934], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7934, 1.0667, 0.1094], dtype=torch.float64)\n",
      "tensor([0.7750], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7750, 1.0596, 0.1016], dtype=torch.float64)\n",
      "tensor([0.7599], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7599, 1.0687, 0.0938], dtype=torch.float64)\n",
      "tensor([0.7761], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7761, 1.0732, 0.0859], dtype=torch.float64)\n",
      "tensor([0.7864], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7864, 1.0810, 0.0781], dtype=torch.float64)\n",
      "tensor([0.8022], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8022, 1.0632, 0.0703], dtype=torch.float64)\n",
      "tensor([0.7697], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7697, 1.0743, 0.0625], dtype=torch.float64)\n",
      "tensor([0.7882], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7882, 1.0672, 0.0547], dtype=torch.float64)\n",
      "tensor([0.7765], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7765, 1.0656, 0.0469], dtype=torch.float64)\n",
      "tensor([0.7725], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7725, 1.0641, 0.0391], dtype=torch.float64)\n",
      "tensor([0.7694], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7694, 1.0573, 0.0312], dtype=torch.float64)\n",
      "tensor([0.7563], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7563, 1.0633, 0.0234], dtype=torch.float64)\n",
      "tensor([0.7665], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7665, 1.0422, 0.0156], dtype=torch.float64)\n",
      "tensor([0.7110], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7110, 1.0168, 0.0078], dtype=torch.float64)\n",
      "tensor([0.6200], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.1636412784827283e-06 5.641684648420856e-07\n",
      "loss: [15.41566351]\n",
      "episode:  321\n",
      "loss: [17.7685559]\n",
      "episode:  322\n",
      "loss: [14.55516532]\n",
      "episode:  323\n",
      "loss: [16.25555061]\n",
      "episode:  324\n",
      "loss: [21.72705501]\n",
      "episode:  325\n",
      "loss: [46.45208869]\n",
      "episode:  326\n",
      "loss: [14.38712498]\n",
      "episode:  327\n",
      "loss: [12.91100094]\n",
      "episode:  328\n",
      "loss: [12.66203838]\n",
      "episode:  329\n",
      "loss: [14.80203062]\n",
      "episode:  330\n",
      "loss: [11.71209961]\n",
      "episode:  331\n",
      "loss: [14.37401857]\n",
      "episode:  332\n",
      "loss: [16.65917704]\n",
      "episode:  333\n",
      "loss: [15.88869956]\n",
      "episode:  334\n",
      "loss: [13.53298601]\n",
      "episode:  335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [15.93005761]\n",
      "episode:  336\n",
      "loss: [13.53610679]\n",
      "episode:  337\n",
      "loss: [12.56636591]\n",
      "episode:  338\n",
      "loss: [24.30347106]\n",
      "episode:  339\n",
      "loss: [12.40590462]\n",
      "episode:  340\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3457], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3457, 0.9941, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5386], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5386, 1.0136, 0.9844], dtype=torch.float64)\n",
      "tensor([0.6225], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6225, 1.0068, 0.9766], dtype=torch.float64)\n",
      "tensor([0.6205], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6205, 0.9962, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5881], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5881, 1.0069, 0.9609], dtype=torch.float64)\n",
      "tensor([0.6117], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6117, 0.9941, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5774], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5774, 0.9965, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5767], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5767, 0.9881, 0.9375], dtype=torch.float64)\n",
      "tensor([0.5507], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5507, 0.9793, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5193], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5193, 0.9671, 0.9219], dtype=torch.float64)\n",
      "tensor([0.4766], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4766, 0.9692, 0.9141], dtype=torch.float64)\n",
      "tensor([0.4734], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4734, 0.9670, 0.9062], dtype=torch.float64)\n",
      "tensor([0.4648], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4648, 0.9763, 0.8984], dtype=torch.float64)\n",
      "tensor([0.4887], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4887, 0.9578, 0.8906], dtype=torch.float64)\n",
      "tensor([0.4374], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4374, 0.9596, 0.8828], dtype=torch.float64)\n",
      "tensor([0.4319], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4319, 0.9668, 0.8750], dtype=torch.float64)\n",
      "tensor([0.4502], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4502, 0.9798, 0.8672], dtype=torch.float64)\n",
      "tensor([0.4900], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4900, 0.9702, 0.8594], dtype=torch.float64)\n",
      "tensor([0.4673], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4673, 0.9682, 0.8516], dtype=torch.float64)\n",
      "tensor([0.4557], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4557, 0.9433, 0.8438], dtype=torch.float64)\n",
      "tensor([0.3796], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3796, 0.9331, 0.8359], dtype=torch.float64)\n",
      "tensor([0.3347], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3347, 0.9288, 0.8281], dtype=torch.float64)\n",
      "tensor([0.3124], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3124, 0.9293, 0.8203], dtype=torch.float64)\n",
      "tensor([0.3082], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3082, 0.9300, 0.8125], dtype=torch.float64)\n",
      "tensor([0.3080], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3080, 0.9368, 0.8047], dtype=torch.float64)\n",
      "tensor([0.3261], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3261, 0.9425, 0.7969], dtype=torch.float64)\n",
      "tensor([0.3444], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3444, 0.9519, 0.7891], dtype=torch.float64)\n",
      "tensor([0.3735], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3735, 0.9411, 0.7812], dtype=torch.float64)\n",
      "tensor([0.3453], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3453, 0.9528, 0.7734], dtype=torch.float64)\n",
      "tensor([0.3728], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3728, 0.9665, 0.7656], dtype=torch.float64)\n",
      "tensor([0.4160], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4160, 0.9655, 0.7578], dtype=torch.float64)\n",
      "tensor([0.4192], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4192, 0.9628, 0.7500], dtype=torch.float64)\n",
      "tensor([0.4114], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4114, 0.9486, 0.7422], dtype=torch.float64)\n",
      "tensor([0.3658], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3658, 0.9480, 0.7344], dtype=torch.float64)\n",
      "tensor([0.3545], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3545, 0.9460, 0.7266], dtype=torch.float64)\n",
      "tensor([0.3452], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3452, 0.9603, 0.7188], dtype=torch.float64)\n",
      "tensor([0.3885], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3885, 0.9729, 0.7109], dtype=torch.float64)\n",
      "tensor([0.4374], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4374, 0.9767, 0.7031], dtype=torch.float64)\n",
      "tensor([0.4591], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4591, 0.9805, 0.6953], dtype=torch.float64)\n",
      "tensor([0.4756], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4756, 0.9664, 0.6875], dtype=torch.float64)\n",
      "tensor([0.4343], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4343, 0.9660, 0.6797], dtype=torch.float64)\n",
      "tensor([0.4250], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4250, 0.9710, 0.6719], dtype=torch.float64)\n",
      "tensor([0.4388], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4388, 0.9586, 0.6641], dtype=torch.float64)\n",
      "tensor([0.4023], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4023, 0.9625, 0.6562], dtype=torch.float64)\n",
      "tensor([0.4074], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4074, 0.9610, 0.6484], dtype=torch.float64)\n",
      "tensor([0.4037], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4037, 0.9491, 0.6406], dtype=torch.float64)\n",
      "tensor([0.3654], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3654, 0.9480, 0.6328], dtype=torch.float64)\n",
      "tensor([0.3543], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3543, 0.9209, 0.6250], dtype=torch.float64)\n",
      "tensor([0.2656], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2656, 0.9163, 0.6172], dtype=torch.float64)\n",
      "tensor([0.2261], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2261, 0.8904, 0.6094], dtype=torch.float64)\n",
      "tensor([0.1351], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1351, 0.9019, 0.6016], dtype=torch.float64)\n",
      "tensor([0.1472], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1472, 0.9052, 0.5938], dtype=torch.float64)\n",
      "tensor([0.1591], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1591, 0.9111, 0.5859], dtype=torch.float64)\n",
      "tensor([0.1785], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1785, 0.9038, 0.5781], dtype=torch.float64)\n",
      "tensor([0.1611], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1611, 0.8990, 0.5703], dtype=torch.float64)\n",
      "tensor([0.1429], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1429, 0.8973, 0.5625], dtype=torch.float64)\n",
      "tensor([0.1334], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1334, 0.8818, 0.5547], dtype=torch.float64)\n",
      "tensor([0.1024], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1024, 0.8866, 0.5469], dtype=torch.float64)\n",
      "tensor([0.1064], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1064, 0.9066, 0.5391], dtype=torch.float64)\n",
      "tensor([0.1504], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1504, 0.8829, 0.5312], dtype=torch.float64)\n",
      "tensor([0.1061], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1061, 0.8880, 0.5234], dtype=torch.float64)\n",
      "tensor([0.1087], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1087, 0.8618, 0.5156], dtype=torch.float64)\n",
      "tensor([0.0806], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0806, 0.8700, 0.5078], dtype=torch.float64)\n",
      "tensor([0.0811], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0811, 0.8715, 0.5000], dtype=torch.float64)\n",
      "tensor([0.0815], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0815, 0.8745, 0.4922], dtype=torch.float64)\n",
      "tensor([0.0822], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0822, 0.8711, 0.4844], dtype=torch.float64)\n",
      "tensor([0.0813], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0813, 0.8832, 0.4766], dtype=torch.float64)\n",
      "tensor([0.0945], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0945, 0.8812, 0.4688], dtype=torch.float64)\n",
      "tensor([0.0924], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0924, 0.8865, 0.4609], dtype=torch.float64)\n",
      "tensor([0.1016], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1016, 0.8832, 0.4531], dtype=torch.float64)\n",
      "tensor([0.0965], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0965, 0.8897, 0.4453], dtype=torch.float64)\n",
      "tensor([0.1077], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1077, 0.8759, 0.4375], dtype=torch.float64)\n",
      "tensor([0.0838], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0838, 0.8692, 0.4297], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0809], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0809, 0.8684, 0.4219], dtype=torch.float64)\n",
      "tensor([0.0808], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0808, 0.8577, 0.4141], dtype=torch.float64)\n",
      "tensor([0.0806], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0806, 0.8486, 0.4062], dtype=torch.float64)\n",
      "tensor([0.0806], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0806, 0.8445, 0.3984], dtype=torch.float64)\n",
      "tensor([0.0806], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0806, 0.8363, 0.3906], dtype=torch.float64)\n",
      "tensor([0.0806], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0806, 0.8487, 0.3828], dtype=torch.float64)\n",
      "tensor([0.0806], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0806, 0.8541, 0.3750], dtype=torch.float64)\n",
      "tensor([0.0806], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0806, 0.8687, 0.3672], dtype=torch.float64)\n",
      "tensor([0.0808], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0808, 0.8941, 0.3594], dtype=torch.float64)\n",
      "tensor([0.1103], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1103, 0.9022, 0.3516], dtype=torch.float64)\n",
      "tensor([0.1294], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1294, 0.9033, 0.3438], dtype=torch.float64)\n",
      "tensor([0.1347], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1347, 0.8875, 0.3359], dtype=torch.float64)\n",
      "tensor([0.1051], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1051, 0.8895, 0.3281], dtype=torch.float64)\n",
      "tensor([0.1042], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1042, 0.9059, 0.3203], dtype=torch.float64)\n",
      "tensor([0.1350], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1350, 0.9147, 0.3125], dtype=torch.float64)\n",
      "tensor([0.1665], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1665, 0.9161, 0.3047], dtype=torch.float64)\n",
      "tensor([0.1775], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1775, 0.9238, 0.2969], dtype=torch.float64)\n",
      "tensor([0.2024], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2024, 0.9243, 0.2891], dtype=torch.float64)\n",
      "tensor([0.2100], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2100, 0.9303, 0.2812], dtype=torch.float64)\n",
      "tensor([0.2313], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2313, 0.9356, 0.2734], dtype=torch.float64)\n",
      "tensor([0.2541], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2541, 0.9403, 0.2656], dtype=torch.float64)\n",
      "tensor([0.2753], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2753, 0.9546, 0.2578], dtype=torch.float64)\n",
      "tensor([0.3279], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3279, 0.9534, 0.2500], dtype=torch.float64)\n",
      "tensor([0.3370], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3370, 0.9462, 0.2422], dtype=torch.float64)\n",
      "tensor([0.3147], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3147, 0.9240, 0.2344], dtype=torch.float64)\n",
      "tensor([0.2343], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2343, 0.8975, 0.2266], dtype=torch.float64)\n",
      "tensor([0.1342], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1342, 0.8847, 0.2188], dtype=torch.float64)\n",
      "tensor([0.0955], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0955, 0.8977, 0.2109], dtype=torch.float64)\n",
      "tensor([0.1140], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1140, 0.8930, 0.2031], dtype=torch.float64)\n",
      "tensor([0.1074], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1074, 0.8615, 0.1953], dtype=torch.float64)\n",
      "tensor([0.0806], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0806, 0.8776, 0.1875], dtype=torch.float64)\n",
      "tensor([0.0818], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0818, 0.8905, 0.1797], dtype=torch.float64)\n",
      "tensor([0.1030], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1030, 0.8938, 0.1719], dtype=torch.float64)\n",
      "tensor([0.1064], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1064, 0.9058, 0.1641], dtype=torch.float64)\n",
      "tensor([0.1285], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1285, 0.9247, 0.1562], dtype=torch.float64)\n",
      "tensor([0.1845], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1845, 0.9287, 0.1484], dtype=torch.float64)\n",
      "tensor([0.2088], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2088, 0.9090, 0.1406], dtype=torch.float64)\n",
      "tensor([0.1563], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1563, 0.8973, 0.1328], dtype=torch.float64)\n",
      "tensor([0.1189], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1189, 0.8841, 0.1250], dtype=torch.float64)\n",
      "tensor([0.0889], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0889, 0.8728, 0.1172], dtype=torch.float64)\n",
      "tensor([0.0810], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0810, 0.8541, 0.1094], dtype=torch.float64)\n",
      "tensor([0.0806], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0806, 0.8779, 0.1016], dtype=torch.float64)\n",
      "tensor([0.0857], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0857, 0.8966, 0.0938], dtype=torch.float64)\n",
      "tensor([0.1174], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1174, 0.8997, 0.0859], dtype=torch.float64)\n",
      "tensor([0.1160], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1160, 0.9176, 0.0781], dtype=torch.float64)\n",
      "tensor([0.1563], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1563, 0.9215, 0.0703], dtype=torch.float64)\n",
      "tensor([0.1763], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1763, 0.9194, 0.0625], dtype=torch.float64)\n",
      "tensor([0.1742], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1742, 0.9334, 0.0547], dtype=torch.float64)\n",
      "tensor([0.2143], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2143, 0.9225, 0.0469], dtype=torch.float64)\n",
      "tensor([0.1912], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1912, 0.9095, 0.0391], dtype=torch.float64)\n",
      "tensor([0.1475], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1475, 0.9035, 0.0312], dtype=torch.float64)\n",
      "tensor([0.1255], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1255, 0.8832, 0.0234], dtype=torch.float64)\n",
      "tensor([0.0869], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0869, 0.8880, 0.0156], dtype=torch.float64)\n",
      "tensor([0.1080], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1080, 0.8616, 0.0078], dtype=torch.float64)\n",
      "tensor([0.0806], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.1498938026172045e-06 5.608332272190458e-07\n",
      "loss: [21.91066252]\n",
      "episode:  341\n",
      "loss: [16.89191167]\n",
      "episode:  342\n",
      "loss: [14.05441195]\n",
      "episode:  343\n",
      "loss: [13.1372372]\n",
      "episode:  344\n",
      "loss: [12.59116098]\n",
      "episode:  345\n",
      "loss: [17.08853067]\n",
      "episode:  346\n",
      "loss: [16.70978341]\n",
      "episode:  347\n",
      "loss: [19.20554719]\n",
      "episode:  348\n",
      "loss: [16.20453406]\n",
      "episode:  349\n",
      "loss: [12.91660404]\n",
      "episode:  350\n",
      "loss: [13.12611153]\n",
      "episode:  351\n",
      "loss: [17.54337782]\n",
      "episode:  352\n",
      "loss: [16.86493822]\n",
      "episode:  353\n",
      "loss: [16.0775738]\n",
      "episode:  354\n",
      "loss: [13.04311541]\n",
      "episode:  355\n",
      "loss: [13.99589485]\n",
      "episode:  356\n",
      "loss: [11.95487566]\n",
      "episode:  357\n",
      "loss: [11.67428285]\n",
      "episode:  358\n",
      "loss: [13.0929453]\n",
      "episode:  359\n",
      "loss: [12.89229691]\n",
      "episode:  360\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3522], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3522, 0.9969, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5616], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5616, 0.9999, 0.9844], dtype=torch.float64)\n",
      "tensor([0.6090], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6090, 1.0030, 0.9766], dtype=torch.float64)\n",
      "tensor([0.6232], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6232, 1.0002, 0.9688], dtype=torch.float64)\n",
      "tensor([0.6185], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6185, 1.0001, 0.9609], dtype=torch.float64)\n",
      "tensor([0.6157], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6157, 1.0307, 0.9531], dtype=torch.float64)\n",
      "tensor([0.6749], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6749, 1.0175, 0.9453], dtype=torch.float64)\n",
      "tensor([0.6586], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6586, 1.0240, 0.9375], dtype=torch.float64)\n",
      "tensor([0.6673], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6673, 1.0231, 0.9297], dtype=torch.float64)\n",
      "tensor([0.6661], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6661, 1.0394, 0.9219], dtype=torch.float64)\n",
      "tensor([0.6964], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6964, 1.0693, 0.9141], dtype=torch.float64)\n",
      "tensor([0.7545], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7545, 1.0605, 0.9062], dtype=torch.float64)\n",
      "tensor([0.7479], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7479, 1.0490, 0.8984], dtype=torch.float64)\n",
      "tensor([0.7259], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7259, 1.0563, 0.8906], dtype=torch.float64)\n",
      "tensor([0.7357], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7357, 1.0671, 0.8828], dtype=torch.float64)\n",
      "tensor([0.7583], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7583, 1.0794, 0.8750], dtype=torch.float64)\n",
      "tensor([0.7858], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7858, 1.0640, 0.8672], dtype=torch.float64)\n",
      "tensor([0.7617], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7617, 1.0631, 0.8594], dtype=torch.float64)\n",
      "tensor([0.7562], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7562, 1.0606, 0.8516], dtype=torch.float64)\n",
      "tensor([0.7507], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7507, 1.0545, 0.8438], dtype=torch.float64)\n",
      "tensor([0.7385], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7385, 1.0435, 0.8359], dtype=torch.float64)\n",
      "tensor([0.7150], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7150, 1.0872, 0.8281], dtype=torch.float64)\n",
      "tensor([0.7954], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7954, 1.0698, 0.8203], dtype=torch.float64)\n",
      "tensor([0.7765], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7765, 1.0666, 0.8125], dtype=torch.float64)\n",
      "tensor([0.7675], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7675, 1.0460, 0.8047], dtype=torch.float64)\n",
      "tensor([0.7270], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7270, 1.0457, 0.7969], dtype=torch.float64)\n",
      "tensor([0.7193], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7193, 1.0703, 0.7891], dtype=torch.float64)\n",
      "tensor([0.7656], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7656, 1.0731, 0.7812], dtype=torch.float64)\n",
      "tensor([0.7794], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7794, 1.0584, 0.7734], dtype=torch.float64)\n",
      "tensor([0.7540], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7540, 1.0430, 0.7656], dtype=torch.float64)\n",
      "tensor([0.7203], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7203, 1.0658, 0.7578], dtype=torch.float64)\n",
      "tensor([0.7587], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7587, 1.0409, 0.7500], dtype=torch.float64)\n",
      "tensor([0.7175], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7175, 1.0339, 0.7422], dtype=torch.float64)\n",
      "tensor([0.6950], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6950, 1.0390, 0.7344], dtype=torch.float64)\n",
      "tensor([0.7020], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7020, 1.0447, 0.7266], dtype=torch.float64)\n",
      "tensor([0.7159], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7159, 1.0289, 0.7188], dtype=torch.float64)\n",
      "tensor([0.6850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6850, 1.0479, 0.7109], dtype=torch.float64)\n",
      "tensor([0.7202], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7202, 1.0606, 0.7031], dtype=torch.float64)\n",
      "tensor([0.7511], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7511, 1.0558, 0.6953], dtype=torch.float64)\n",
      "tensor([0.7477], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7477, 1.0517, 0.6875], dtype=torch.float64)\n",
      "tensor([0.7396], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7396, 1.0617, 0.6797], dtype=torch.float64)\n",
      "tensor([0.7576], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7576, 1.0592, 0.6719], dtype=torch.float64)\n",
      "tensor([0.7563], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7563, 1.0521, 0.6641], dtype=torch.float64)\n",
      "tensor([0.7429], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7429, 1.0357, 0.6562], dtype=torch.float64)\n",
      "tensor([0.7080], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7080, 1.0380, 0.6484], dtype=torch.float64)\n",
      "tensor([0.7065], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7065, 1.0168, 0.6406], dtype=torch.float64)\n",
      "tensor([0.6564], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6564, 1.0094, 0.6328], dtype=torch.float64)\n",
      "tensor([0.6224], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6224, 1.0062, 0.6250], dtype=torch.float64)\n",
      "tensor([0.6048], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6048, 1.0090, 0.6172], dtype=torch.float64)\n",
      "tensor([0.6097], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6097, 1.0014, 0.6094], dtype=torch.float64)\n",
      "tensor([0.5870], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5870, 1.0094, 0.6016], dtype=torch.float64)\n",
      "tensor([0.6071], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6071, 0.9903, 0.5938], dtype=torch.float64)\n",
      "tensor([0.5520], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5520, 0.9798, 0.5859], dtype=torch.float64)\n",
      "tensor([0.5072], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5072, 0.9811, 0.5781], dtype=torch.float64)\n",
      "tensor([0.5013], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5013, 0.9745, 0.5703], dtype=torch.float64)\n",
      "tensor([0.4791], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4791, 0.9769, 0.5625], dtype=torch.float64)\n",
      "tensor([0.4817], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4817, 0.9823, 0.5547], dtype=torch.float64)\n",
      "tensor([0.4994], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4994, 0.9618, 0.5469], dtype=torch.float64)\n",
      "tensor([0.4386], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4386, 0.9407, 0.5391], dtype=torch.float64)\n",
      "tensor([0.3576], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3576, 0.9321, 0.5312], dtype=torch.float64)\n",
      "tensor([0.3053], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3053, 0.9152, 0.5234], dtype=torch.float64)\n",
      "tensor([0.2339], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2339, 0.9254, 0.5156], dtype=torch.float64)\n",
      "tensor([0.2465], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2465, 0.9315, 0.5078], dtype=torch.float64)\n",
      "tensor([0.2699], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2699, 0.9142, 0.5000], dtype=torch.float64)\n",
      "tensor([0.2182], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2182, 0.9378, 0.4922], dtype=torch.float64)\n",
      "tensor([0.2814], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2814, 0.9225, 0.4844], dtype=torch.float64)\n",
      "tensor([0.2477], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2477, 0.9172, 0.4766], dtype=torch.float64)\n",
      "tensor([0.2199], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2199, 0.9033, 0.4688], dtype=torch.float64)\n",
      "tensor([0.1676], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1676, 0.8930, 0.4609], dtype=torch.float64)\n",
      "tensor([0.1263], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1263, 0.8960, 0.4531], dtype=torch.float64)\n",
      "tensor([0.1250], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1250, 0.8904, 0.4453], dtype=torch.float64)\n",
      "tensor([0.1141], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1141, 0.8741, 0.4375], dtype=torch.float64)\n",
      "tensor([0.0821], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0821, 0.8793, 0.4297], dtype=torch.float64)\n",
      "tensor([0.0862], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0862, 0.8913, 0.4219], dtype=torch.float64)\n",
      "tensor([0.1087], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1087, 0.8954, 0.4141], dtype=torch.float64)\n",
      "tensor([0.1195], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1195, 0.9031, 0.4062], dtype=torch.float64)\n",
      "tensor([0.1378], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1378, 0.9110, 0.3984], dtype=torch.float64)\n",
      "tensor([0.1644], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1644, 0.9172, 0.3906], dtype=torch.float64)\n",
      "tensor([0.1883], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1883, 0.9074, 0.3828], dtype=torch.float64)\n",
      "tensor([0.1652], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1652, 0.9033, 0.3750], dtype=torch.float64)\n",
      "tensor([0.1472], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1472, 0.9174, 0.3672], dtype=torch.float64)\n",
      "tensor([0.1830], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1830, 0.9165, 0.3594], dtype=torch.float64)\n",
      "tensor([0.1886], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1886, 0.9364, 0.3516], dtype=torch.float64)\n",
      "tensor([0.2553], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2553, 0.9411, 0.3438], dtype=torch.float64)\n",
      "tensor([0.2889], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2889, 0.9482, 0.3359], dtype=torch.float64)\n",
      "tensor([0.3214], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3214, 0.9634, 0.3281], dtype=torch.float64)\n",
      "tensor([0.3805], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3805, 0.9598, 0.3203], dtype=torch.float64)\n",
      "tensor([0.3844], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3844, 0.9522, 0.3125], dtype=torch.float64)\n",
      "tensor([0.3597], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3597, 0.9513, 0.3047], dtype=torch.float64)\n",
      "tensor([0.3490], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3490, 0.9573, 0.2969], dtype=torch.float64)\n",
      "tensor([0.3650], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3650, 0.9818, 0.2891], dtype=torch.float64)\n",
      "tensor([0.4503], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4503, 0.9901, 0.2812], dtype=torch.float64)\n",
      "tensor([0.5015], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5015, 1.0002, 0.2734], dtype=torch.float64)\n",
      "tensor([0.5483], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5483, 0.9895, 0.2656], dtype=torch.float64)\n",
      "tensor([0.5254], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5254, 0.9950, 0.2578], dtype=torch.float64)\n",
      "tensor([0.5365], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5365, 0.9857, 0.2500], dtype=torch.float64)\n",
      "tensor([0.5082], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5082, 0.9978, 0.2422], dtype=torch.float64)\n",
      "tensor([0.5393], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5393, 0.9826, 0.2344], dtype=torch.float64)\n",
      "tensor([0.4973], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4973, 0.9870, 0.2266], dtype=torch.float64)\n",
      "tensor([0.4994], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4994, 1.0063, 0.2188], dtype=torch.float64)\n",
      "tensor([0.5627], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5627, 1.0112, 0.2109], dtype=torch.float64)\n",
      "tensor([0.5958], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5958, 1.0135, 0.2031], dtype=torch.float64)\n",
      "tensor([0.6119], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6119, 1.0412, 0.1953], dtype=torch.float64)\n",
      "tensor([0.7063], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7063, 1.0493, 0.1875], dtype=torch.float64)\n",
      "tensor([0.7500], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7500, 1.0665, 0.1797], dtype=torch.float64)\n",
      "tensor([0.7906], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7906, 1.1110, 0.1719], dtype=torch.float64)\n",
      "tensor([0.8582], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8582, 1.1047, 0.1641], dtype=torch.float64)\n",
      "tensor([0.8562], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8562, 1.1109, 0.1562], dtype=torch.float64)\n",
      "tensor([0.8624], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8624, 1.0872, 0.1484], dtype=torch.float64)\n",
      "tensor([0.8389], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8389, 1.0772, 0.1406], dtype=torch.float64)\n",
      "tensor([0.8245], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8245, 1.0815, 0.1328], dtype=torch.float64)\n",
      "tensor([0.8310], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8310, 1.0885, 0.1250], dtype=torch.float64)\n",
      "tensor([0.8386], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8386, 1.1010, 0.1172], dtype=torch.float64)\n",
      "tensor([0.8517], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8517, 1.0946, 0.1094], dtype=torch.float64)\n",
      "tensor([0.8461], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8461, 1.0799, 0.1016], dtype=torch.float64)\n",
      "tensor([0.8310], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8310, 1.0671, 0.0938], dtype=torch.float64)\n",
      "tensor([0.8052], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8052, 1.0511, 0.0859], dtype=torch.float64)\n",
      "tensor([0.7718], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7718, 1.0526, 0.0781], dtype=torch.float64)\n",
      "tensor([0.7698], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7698, 1.0685, 0.0703], dtype=torch.float64)\n",
      "tensor([0.8014], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8014, 1.0836, 0.0625], dtype=torch.float64)\n",
      "tensor([0.8324], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8324, 1.0914, 0.0547], dtype=torch.float64)\n",
      "tensor([0.8422], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8422, 1.0681, 0.0469], dtype=torch.float64)\n",
      "tensor([0.8091], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8091, 1.0673, 0.0391], dtype=torch.float64)\n",
      "tensor([0.8037], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8037, 1.0818, 0.0312], dtype=torch.float64)\n",
      "tensor([0.8310], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8310, 1.0499, 0.0234], dtype=torch.float64)\n",
      "tensor([0.7675], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7675, 1.0511, 0.0156], dtype=torch.float64)\n",
      "tensor([0.7585], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7585, 1.0436, 0.0078], dtype=torch.float64)\n",
      "tensor([0.7342], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.3110746643984342e-06 6.113492714845925e-07\n",
      "loss: [16.91627487]\n",
      "episode:  361\n",
      "loss: [14.51424068]\n",
      "episode:  362\n",
      "loss: [14.6193162]\n",
      "episode:  363\n",
      "loss: [14.21791067]\n",
      "episode:  364\n",
      "loss: [17.48501388]\n",
      "episode:  365\n",
      "loss: [13.26571329]\n",
      "episode:  366\n",
      "loss: [13.79227711]\n",
      "episode:  367\n",
      "loss: [15.89822353]\n",
      "episode:  368\n",
      "loss: [14.6828067]\n",
      "episode:  369\n",
      "loss: [16.04491544]\n",
      "episode:  370\n",
      "loss: [14.42984076]\n",
      "episode:  371\n",
      "loss: [12.28810304]\n",
      "episode:  372\n",
      "loss: [17.40739201]\n",
      "episode:  373\n",
      "loss: [19.14439305]\n",
      "episode:  374\n",
      "loss: [15.73441856]\n",
      "episode:  375\n",
      "loss: [14.73846542]\n",
      "episode:  376\n",
      "loss: [11.44249545]\n",
      "episode:  377\n",
      "loss: [14.80220509]\n",
      "episode:  378\n",
      "loss: [16.00367744]\n",
      "episode:  379\n",
      "loss: [14.53553878]\n",
      "episode:  380\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3445], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3445, 0.9860, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5245], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5245, 0.9859, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5605], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5605, 0.9823, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5560], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5560, 0.9744, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5307], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5307, 0.9706, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5124], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5124, 0.9489, 0.9531], dtype=torch.float64)\n",
      "tensor([0.4436], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4436, 0.9734, 0.9453], dtype=torch.float64)\n",
      "tensor([0.4987], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4987, 0.9855, 0.9375], dtype=torch.float64)\n",
      "tensor([0.5439], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5439, 0.9931, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5736], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5736, 0.9896, 0.9219], dtype=torch.float64)\n",
      "tensor([0.5680], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5680, 0.9770, 0.9141], dtype=torch.float64)\n",
      "tensor([0.5288], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5288, 1.0001, 0.9062], dtype=torch.float64)\n",
      "tensor([0.5853], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5853, 0.9867, 0.8984], dtype=torch.float64)\n",
      "tensor([0.5571], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5571, 0.9719, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5065], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5065, 0.9737, 0.8828], dtype=torch.float64)\n",
      "tensor([0.4992], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4992, 0.9581, 0.8750], dtype=torch.float64)\n",
      "tensor([0.4503], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4503, 0.9540, 0.8672], dtype=torch.float64)\n",
      "tensor([0.4264], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4264, 0.9527, 0.8594], dtype=torch.float64)\n",
      "tensor([0.4159], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4159, 0.9352, 0.8516], dtype=torch.float64)\n",
      "tensor([0.3609], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3609, 0.9250, 0.8438], dtype=torch.float64)\n",
      "tensor([0.3174], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3174, 0.9281, 0.8359], dtype=torch.float64)\n",
      "tensor([0.3156], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3156, 0.9308, 0.8281], dtype=torch.float64)\n",
      "tensor([0.3215], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3215, 0.9300, 0.8203], dtype=torch.float64)\n",
      "tensor([0.3185], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3185, 0.9385, 0.8125], dtype=torch.float64)\n",
      "tensor([0.3410], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3410, 0.9470, 0.8047], dtype=torch.float64)\n",
      "tensor([0.3690], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3690, 0.9487, 0.7969], dtype=torch.float64)\n",
      "tensor([0.3782], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3782, 0.9292, 0.7891], dtype=torch.float64)\n",
      "tensor([0.3215], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3215, 0.9412, 0.7812], dtype=torch.float64)\n",
      "tensor([0.3429], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3429, 0.9457, 0.7734], dtype=torch.float64)\n",
      "tensor([0.3587], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.9237, 0.7656], dtype=torch.float64)\n",
      "tensor([0.2963], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2963, 0.9262, 0.7578], dtype=torch.float64)\n",
      "tensor([0.2884], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2884, 0.9088, 0.7500], dtype=torch.float64)\n",
      "tensor([0.2345], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2345, 0.9131, 0.7422], dtype=torch.float64)\n",
      "tensor([0.2326], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2326, 0.8983, 0.7344], dtype=torch.float64)\n",
      "tensor([0.1840], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1840, 0.8778, 0.7266], dtype=torch.float64)\n",
      "tensor([0.1179], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1179, 0.8756, 0.7188], dtype=torch.float64)\n",
      "tensor([0.1025], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1025, 0.8622, 0.7109], dtype=torch.float64)\n",
      "tensor([0.0785], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0785, 0.8511, 0.7031], dtype=torch.float64)\n",
      "tensor([0.0768], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0768, 0.8619, 0.6953], dtype=torch.float64)\n",
      "tensor([0.0777], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0777, 0.8646, 0.6875], dtype=torch.float64)\n",
      "tensor([0.0782], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0782, 0.8885, 0.6797], dtype=torch.float64)\n",
      "tensor([0.1120], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1120, 0.8979, 0.6719], dtype=torch.float64)\n",
      "tensor([0.1350], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1350, 0.9009, 0.6641], dtype=torch.float64)\n",
      "tensor([0.1466], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1466, 0.8959, 0.6562], dtype=torch.float64)\n",
      "tensor([0.1335], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1335, 0.8825, 0.6484], dtype=torch.float64)\n",
      "tensor([0.1050], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1050, 0.8711, 0.6406], dtype=torch.float64)\n",
      "tensor([0.0799], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0799, 0.8593, 0.6328], dtype=torch.float64)\n",
      "tensor([0.0768], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0768, 0.8421, 0.6250], dtype=torch.float64)\n",
      "tensor([0.0768], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0768, 0.8362, 0.6172], dtype=torch.float64)\n",
      "tensor([0.0768], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0768, 0.8348, 0.6094], dtype=torch.float64)\n",
      "tensor([0.0768], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0768, 0.8542, 0.6016], dtype=torch.float64)\n",
      "tensor([0.0768], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0768, 0.8542, 0.5938], dtype=torch.float64)\n",
      "tensor([0.0768], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0768, 0.8750, 0.5859], dtype=torch.float64)\n",
      "tensor([0.0786], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0786, 0.8909, 0.5781], dtype=torch.float64)\n",
      "tensor([0.1051], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1051, 0.8744, 0.5703], dtype=torch.float64)\n",
      "tensor([0.0790], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0790, 0.8686, 0.5625], dtype=torch.float64)\n",
      "tensor([0.0772], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0772, 0.8802, 0.5547], dtype=torch.float64)\n",
      "tensor([0.0840], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0840, 0.8719, 0.5469], dtype=torch.float64)\n",
      "tensor([0.0776], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0776, 0.8728, 0.5391], dtype=torch.float64)\n",
      "tensor([0.0777], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0777, 0.8870, 0.5312], dtype=torch.float64)\n",
      "tensor([0.0955], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0955, 0.8871, 0.5234], dtype=torch.float64)\n",
      "tensor([0.0985], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0985, 0.8881, 0.5156], dtype=torch.float64)\n",
      "tensor([0.1004], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1004, 0.8909, 0.5078], dtype=torch.float64)\n",
      "tensor([0.1056], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1056, 0.8841, 0.5000], dtype=torch.float64)\n",
      "tensor([0.0935], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0935, 0.9061, 0.4922], dtype=torch.float64)\n",
      "tensor([0.1343], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1343, 0.8991, 0.4844], dtype=torch.float64)\n",
      "tensor([0.1257], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1257, 0.9073, 0.4766], dtype=torch.float64)\n",
      "tensor([0.1453], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1453, 0.8845, 0.4688], dtype=torch.float64)\n",
      "tensor([0.0996], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0996, 0.8887, 0.4609], dtype=torch.float64)\n",
      "tensor([0.0993], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0993, 0.8926, 0.4531], dtype=torch.float64)\n",
      "tensor([0.1063], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1063, 0.8797, 0.4453], dtype=torch.float64)\n",
      "tensor([0.0830], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0830, 0.8752, 0.4375], dtype=torch.float64)\n",
      "tensor([0.0776], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0776, 0.8878, 0.4297], dtype=torch.float64)\n",
      "tensor([0.0926], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0926, 0.8890, 0.4219], dtype=torch.float64)\n",
      "tensor([0.0969], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0969, 0.8953, 0.4141], dtype=torch.float64)\n",
      "tensor([0.1090], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1090, 0.8872, 0.4062], dtype=torch.float64)\n",
      "tensor([0.0957], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0957, 0.8921, 0.3984], dtype=torch.float64)\n",
      "tensor([0.1022], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1022, 0.8836, 0.3906], dtype=torch.float64)\n",
      "tensor([0.0871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0871, 0.8877, 0.3828], dtype=torch.float64)\n",
      "tensor([0.0917], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0917, 0.8783, 0.3750], dtype=torch.float64)\n",
      "tensor([0.0780], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0780, 0.8793, 0.3672], dtype=torch.float64)\n",
      "tensor([0.0780], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0780, 0.8845, 0.3594], dtype=torch.float64)\n",
      "tensor([0.0834], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0834, 0.8758, 0.3516], dtype=torch.float64)\n",
      "tensor([0.0772], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0772, 0.8728, 0.3438], dtype=torch.float64)\n",
      "tensor([0.0770], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0770, 0.8803, 0.3359], dtype=torch.float64)\n",
      "tensor([0.0782], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0782, 0.8763, 0.3281], dtype=torch.float64)\n",
      "tensor([0.0773], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0773, 0.8730, 0.3203], dtype=torch.float64)\n",
      "tensor([0.0770], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0770, 0.8807, 0.3125], dtype=torch.float64)\n",
      "tensor([0.0782], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0782, 0.8833, 0.3047], dtype=torch.float64)\n",
      "tensor([0.0805], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0805, 0.8814, 0.2969], dtype=torch.float64)\n",
      "tensor([0.0783], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0783, 0.8872, 0.2891], dtype=torch.float64)\n",
      "tensor([0.0869], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0869, 0.8925, 0.2812], dtype=torch.float64)\n",
      "tensor([0.0962], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0962, 0.9104, 0.2734], dtype=torch.float64)\n",
      "tensor([0.1316], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1316, 0.9089, 0.2656], dtype=torch.float64)\n",
      "tensor([0.1361], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1361, 0.9127, 0.2578], dtype=torch.float64)\n",
      "tensor([0.1475], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1475, 0.9194, 0.2500], dtype=torch.float64)\n",
      "tensor([0.1693], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1693, 0.9005, 0.2422], dtype=torch.float64)\n",
      "tensor([0.1233], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1233, 0.9074, 0.2344], dtype=torch.float64)\n",
      "tensor([0.1281], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1281, 0.9176, 0.2266], dtype=torch.float64)\n",
      "tensor([0.1575], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1575, 0.9223, 0.2188], dtype=torch.float64)\n",
      "tensor([0.1783], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1783, 0.9302, 0.2109], dtype=torch.float64)\n",
      "tensor([0.2071], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2071, 0.9214, 0.2031], dtype=torch.float64)\n",
      "tensor([0.1876], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1876, 0.9112, 0.1953], dtype=torch.float64)\n",
      "tensor([0.1521], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1521, 0.9311, 0.1875], dtype=torch.float64)\n",
      "tensor([0.2000], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2000, 0.9357, 0.1797], dtype=torch.float64)\n",
      "tensor([0.2291], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2291, 0.9473, 0.1719], dtype=torch.float64)\n",
      "tensor([0.2760], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2760, 0.9444, 0.1641], dtype=torch.float64)\n",
      "tensor([0.2798], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2798, 0.9302, 0.1562], dtype=torch.float64)\n",
      "tensor([0.2327], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2327, 0.9221, 0.1484], dtype=torch.float64)\n",
      "tensor([0.1925], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1925, 0.9061, 0.1406], dtype=torch.float64)\n",
      "tensor([0.1346], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1346, 0.9093, 0.1328], dtype=torch.float64)\n",
      "tensor([0.1290], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1290, 0.9223, 0.1250], dtype=torch.float64)\n",
      "tensor([0.1636], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1636, 0.9166, 0.1172], dtype=torch.float64)\n",
      "tensor([0.1559], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1559, 0.9257, 0.1094], dtype=torch.float64)\n",
      "tensor([0.1797], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1797, 0.9250, 0.1016], dtype=torch.float64)\n",
      "tensor([0.1834], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1834, 0.9076, 0.0938], dtype=torch.float64)\n",
      "tensor([0.1329], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1329, 0.9071, 0.0859], dtype=torch.float64)\n",
      "tensor([0.1226], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1226, 0.9201, 0.0781], dtype=torch.float64)\n",
      "tensor([0.1523], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1523, 0.9121, 0.0703], dtype=torch.float64)\n",
      "tensor([0.1365], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1365, 0.9097, 0.0625], dtype=torch.float64)\n",
      "tensor([0.1269], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1269, 0.9255, 0.0547], dtype=torch.float64)\n",
      "tensor([0.1674], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1674, 0.9243, 0.0469], dtype=torch.float64)\n",
      "tensor([0.1739], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1739, 0.9162, 0.0391], dtype=torch.float64)\n",
      "tensor([0.1515], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1515, 0.9089, 0.0312], dtype=torch.float64)\n",
      "tensor([0.1266], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1266, 0.9006, 0.0234], dtype=torch.float64)\n",
      "tensor([0.1069], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1069, 0.9114, 0.0156], dtype=torch.float64)\n",
      "tensor([0.1309], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1309, 0.9333, 0.0078], dtype=torch.float64)\n",
      "tensor([0.1886], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_result:  9.907781776985475e-07 5.778185205200171e-07\n",
      "loss: [17.86984286]\n",
      "episode:  381\n",
      "loss: [22.35320444]\n",
      "episode:  382\n",
      "loss: [12.6043091]\n",
      "episode:  383\n",
      "loss: [9.98987474]\n",
      "episode:  384\n",
      "loss: [15.28343112]\n",
      "episode:  385\n",
      "loss: [13.78777774]\n",
      "episode:  386\n",
      "loss: [10.98671527]\n",
      "episode:  387\n",
      "loss: [12.85411092]\n",
      "episode:  388\n",
      "loss: [12.87273681]\n",
      "episode:  389\n",
      "loss: [12.05964218]\n",
      "episode:  390\n",
      "loss: [19.43016842]\n",
      "episode:  391\n",
      "loss: [11.3570206]\n",
      "episode:  392\n",
      "loss: [10.5062315]\n",
      "episode:  393\n",
      "loss: [16.44478771]\n",
      "episode:  394\n",
      "loss: [17.95024853]\n",
      "episode:  395\n",
      "loss: [14.88071118]\n",
      "episode:  396\n",
      "loss: [14.57971734]\n",
      "episode:  397\n",
      "loss: [11.4054709]\n",
      "episode:  398\n",
      "loss: [14.85196639]\n",
      "episode:  399\n",
      "loss: [15.61044729]\n",
      "episode:  400\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3428], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3428, 1.0353, 0.9922], dtype=torch.float64)\n",
      "tensor([0.6304], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6304, 1.0351, 0.9844], dtype=torch.float64)\n",
      "tensor([0.6826], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6826, 1.0374, 0.9766], dtype=torch.float64)\n",
      "tensor([0.6955], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6955, 1.0344, 0.9688], dtype=torch.float64)\n",
      "tensor([0.6912], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6912, 1.0352, 0.9609], dtype=torch.float64)\n",
      "tensor([0.6911], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6911, 1.0227, 0.9531], dtype=torch.float64)\n",
      "tensor([0.6663], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6663, 1.0250, 0.9453], dtype=torch.float64)\n",
      "tensor([0.6651], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6651, 1.0464, 0.9375], dtype=torch.float64)\n",
      "tensor([0.7044], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7044, 1.0356, 0.9297], dtype=torch.float64)\n",
      "tensor([0.6904], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6904, 1.0397, 0.9219], dtype=torch.float64)\n",
      "tensor([0.6945], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6945, 1.0328, 0.9141], dtype=torch.float64)\n",
      "tensor([0.6812], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6812, 1.0286, 0.9062], dtype=torch.float64)\n",
      "tensor([0.6697], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6697, 1.0327, 0.8984], dtype=torch.float64)\n",
      "tensor([0.6745], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6745, 1.0388, 0.8906], dtype=torch.float64)\n",
      "tensor([0.6860], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6860, 1.0173, 0.8828], dtype=torch.float64)\n",
      "tensor([0.6463], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6463, 1.0234, 0.8750], dtype=torch.float64)\n",
      "tensor([0.6494], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6494, 1.0426, 0.8672], dtype=torch.float64)\n",
      "tensor([0.6857], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6857, 1.0250, 0.8594], dtype=torch.float64)\n",
      "tensor([0.6578], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6578, 1.0288, 0.8516], dtype=torch.float64)\n",
      "tensor([0.6593], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6593, 1.0248, 0.8438], dtype=torch.float64)\n",
      "tensor([0.6516], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6516, 0.9973, 0.8359], dtype=torch.float64)\n",
      "tensor([0.5874], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5874, 0.9874, 0.8281], dtype=torch.float64)\n",
      "tensor([0.5435], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5435, 0.9783, 0.8203], dtype=torch.float64)\n",
      "tensor([0.5064], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5064, 0.9992, 0.8125], dtype=torch.float64)\n",
      "tensor([0.5563], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5563, 1.0146, 0.8047], dtype=torch.float64)\n",
      "tensor([0.6108], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6108, 1.0159, 0.7969], dtype=torch.float64)\n",
      "tensor([0.6252], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6252, 1.0089, 0.7891], dtype=torch.float64)\n",
      "tensor([0.6125], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6125, 1.0151, 0.7812], dtype=torch.float64)\n",
      "tensor([0.6246], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6246, 1.0108, 0.7734], dtype=torch.float64)\n",
      "tensor([0.6180], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6180, 0.9832, 0.7656], dtype=torch.float64)\n",
      "tensor([0.5312], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5312, 0.9751, 0.7578], dtype=torch.float64)\n",
      "tensor([0.4844], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4844, 0.9640, 0.7500], dtype=torch.float64)\n",
      "tensor([0.4384], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4384, 0.9649, 0.7422], dtype=torch.float64)\n",
      "tensor([0.4297], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4297, 0.9786, 0.7344], dtype=torch.float64)\n",
      "tensor([0.4704], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4704, 0.9695, 0.7266], dtype=torch.float64)\n",
      "tensor([0.4520], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4520, 0.9673, 0.7188], dtype=torch.float64)\n",
      "tensor([0.4404], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4404, 0.9661, 0.7109], dtype=torch.float64)\n",
      "tensor([0.4339], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4339, 0.9493, 0.7031], dtype=torch.float64)\n",
      "tensor([0.3795], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3795, 0.9599, 0.6953], dtype=torch.float64)\n",
      "tensor([0.3994], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3994, 0.9524, 0.6875], dtype=torch.float64)\n",
      "tensor([0.3807], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3807, 0.9492, 0.6797], dtype=torch.float64)\n",
      "tensor([0.3662], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3662, 0.9480, 0.6719], dtype=torch.float64)\n",
      "tensor([0.3587], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.9451, 0.6641], dtype=torch.float64)\n",
      "tensor([0.3478], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3478, 0.9509, 0.6562], dtype=torch.float64)\n",
      "tensor([0.3633], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3633, 0.9514, 0.6484], dtype=torch.float64)\n",
      "tensor([0.3685], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3685, 0.9347, 0.6406], dtype=torch.float64)\n",
      "tensor([0.3174], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3174, 0.9483, 0.6328], dtype=torch.float64)\n",
      "tensor([0.3475], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3475, 0.9518, 0.6250], dtype=torch.float64)\n",
      "tensor([0.3658], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3658, 0.9543, 0.6172], dtype=torch.float64)\n",
      "tensor([0.3780], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3780, 0.9561, 0.6094], dtype=torch.float64)\n",
      "tensor([0.3865], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3865, 0.9647, 0.6016], dtype=torch.float64)\n",
      "tensor([0.4157], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4157, 0.9577, 0.5938], dtype=torch.float64)\n",
      "tensor([0.4008], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4008, 0.9524, 0.5859], dtype=torch.float64)\n",
      "tensor([0.3804], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3804, 0.9403, 0.5781], dtype=torch.float64)\n",
      "tensor([0.3369], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3369, 0.9472, 0.5703], dtype=torch.float64)\n",
      "tensor([0.3454], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3454, 0.9606, 0.5625], dtype=torch.float64)\n",
      "tensor([0.3912], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3912, 0.9533, 0.5547], dtype=torch.float64)\n",
      "tensor([0.3808], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3808, 0.9741, 0.5469], dtype=torch.float64)\n",
      "tensor([0.4432], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4432, 0.9612, 0.5391], dtype=torch.float64)\n",
      "tensor([0.4180], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4180, 0.9573, 0.5312], dtype=torch.float64)\n",
      "tensor([0.3994], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3994, 0.9491, 0.5234], dtype=torch.float64)\n",
      "tensor([0.3662], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3662, 0.9463, 0.5156], dtype=torch.float64)\n",
      "tensor([0.3458], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3458, 0.9395, 0.5078], dtype=torch.float64)\n",
      "tensor([0.3163], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3163, 0.9154, 0.5000], dtype=torch.float64)\n",
      "tensor([0.2270], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2270, 0.8950, 0.4922], dtype=torch.float64)\n",
      "tensor([0.1372], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1372, 0.9067, 0.4844], dtype=torch.float64)\n",
      "tensor([0.1460], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1460, 0.9010, 0.4766], dtype=torch.float64)\n",
      "tensor([0.1313], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1313, 0.8975, 0.4688], dtype=torch.float64)\n",
      "tensor([0.1202], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1202, 0.9087, 0.4609], dtype=torch.float64)\n",
      "tensor([0.1452], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1452, 0.9071, 0.4531], dtype=torch.float64)\n",
      "tensor([0.1467], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1467, 0.8880, 0.4453], dtype=torch.float64)\n",
      "tensor([0.1040], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1040, 0.8992, 0.4375], dtype=torch.float64)\n",
      "tensor([0.1171], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1171, 0.9008, 0.4297], dtype=torch.float64)\n",
      "tensor([0.1218], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1218, 0.8975, 0.4219], dtype=torch.float64)\n",
      "tensor([0.1163], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1163, 0.8929, 0.4141], dtype=torch.float64)\n",
      "tensor([0.1063], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1063, 0.9016, 0.4062], dtype=torch.float64)\n",
      "tensor([0.1204], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1204, 0.9061, 0.3984], dtype=torch.float64)\n",
      "tensor([0.1331], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1331, 0.9005, 0.3906], dtype=torch.float64)\n",
      "tensor([0.1223], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1223, 0.9171, 0.3828], dtype=torch.float64)\n",
      "tensor([0.1639], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1639, 0.9011, 0.3750], dtype=torch.float64)\n",
      "tensor([0.1283], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1283, 0.8997, 0.3672], dtype=torch.float64)\n",
      "tensor([0.1188], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1188, 0.8861, 0.3594], dtype=torch.float64)\n",
      "tensor([0.0915], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0915, 0.8917, 0.3516], dtype=torch.float64)\n",
      "tensor([0.0967], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0967, 0.8890, 0.3438], dtype=torch.float64)\n",
      "tensor([0.0923], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0923, 0.8666, 0.3359], dtype=torch.float64)\n",
      "tensor([0.0747], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0747, 0.8783, 0.3281], dtype=torch.float64)\n",
      "tensor([0.0756], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0756, 0.8830, 0.3203], dtype=torch.float64)\n",
      "tensor([0.0780], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0780, 0.8948, 0.3125], dtype=torch.float64)\n",
      "tensor([0.0983], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0983, 0.9068, 0.3047], dtype=torch.float64)\n",
      "tensor([0.1238], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1238, 0.9059, 0.2969], dtype=torch.float64)\n",
      "tensor([0.1262], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1262, 0.8881, 0.2891], dtype=torch.float64)\n",
      "tensor([0.0931], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0931, 0.8854, 0.2812], dtype=torch.float64)\n",
      "tensor([0.0822], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0822, 0.8798, 0.2734], dtype=torch.float64)\n",
      "tensor([0.0757], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0757, 0.8681, 0.2656], dtype=torch.float64)\n",
      "tensor([0.0747], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0747, 0.8598, 0.2578], dtype=torch.float64)\n",
      "tensor([0.0747], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0747, 0.8552, 0.2500], dtype=torch.float64)\n",
      "tensor([0.0747], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0747, 0.8723, 0.2422], dtype=torch.float64)\n",
      "tensor([0.0748], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0748, 0.8513, 0.2344], dtype=torch.float64)\n",
      "tensor([0.0747], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0747, 0.8751, 0.2266], dtype=torch.float64)\n",
      "tensor([0.0750], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0750, 0.8808, 0.2188], dtype=torch.float64)\n",
      "tensor([0.0758], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0758, 0.8903, 0.2109], dtype=torch.float64)\n",
      "tensor([0.0891], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0891, 0.8794, 0.2031], dtype=torch.float64)\n",
      "tensor([0.0752], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0752, 0.8859, 0.1953], dtype=torch.float64)\n",
      "tensor([0.0820], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0820, 0.8833, 0.1875], dtype=torch.float64)\n",
      "tensor([0.0768], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0768, 0.8832, 0.1797], dtype=torch.float64)\n",
      "tensor([0.0777], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0777, 0.8714, 0.1719], dtype=torch.float64)\n",
      "tensor([0.0747], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0747, 0.8796, 0.1641], dtype=torch.float64)\n",
      "tensor([0.0756], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0756, 0.8753, 0.1562], dtype=torch.float64)\n",
      "tensor([0.0750], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0750, 0.8686, 0.1484], dtype=torch.float64)\n",
      "tensor([0.0747], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0747, 0.8670, 0.1406], dtype=torch.float64)\n",
      "tensor([0.0747], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0747, 0.8720, 0.1328], dtype=torch.float64)\n",
      "tensor([0.0748], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0748, 0.8861, 0.1250], dtype=torch.float64)\n",
      "tensor([0.0856], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0856, 0.9173, 0.1172], dtype=torch.float64)\n",
      "tensor([0.1429], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1429, 0.9193, 0.1094], dtype=torch.float64)\n",
      "tensor([0.1545], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1545, 0.9257, 0.1016], dtype=torch.float64)\n",
      "tensor([0.1754], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1754, 0.9326, 0.0938], dtype=torch.float64)\n",
      "tensor([0.2002], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2002, 0.9399, 0.0859], dtype=torch.float64)\n",
      "tensor([0.2307], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2307, 0.9405, 0.0781], dtype=torch.float64)\n",
      "tensor([0.2413], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2413, 0.9290, 0.0703], dtype=torch.float64)\n",
      "tensor([0.2061], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2061, 0.9245, 0.0625], dtype=torch.float64)\n",
      "tensor([0.1830], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1830, 0.9316, 0.0547], dtype=torch.float64)\n",
      "tensor([0.1965], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1965, 0.9579, 0.0469], dtype=torch.float64)\n",
      "tensor([0.2852], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2852, 0.9479, 0.0391], dtype=torch.float64)\n",
      "tensor([0.2788], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2788, 0.9270, 0.0312], dtype=torch.float64)\n",
      "tensor([0.2074], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2074, 0.9480, 0.0234], dtype=torch.float64)\n",
      "tensor([0.2534], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2534, 0.9583, 0.0156], dtype=torch.float64)\n",
      "tensor([0.3010], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3010, 0.9646, 0.0078], dtype=torch.float64)\n",
      "tensor([0.3358], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  9.389030241218405e-07 5.797743288431022e-07\n",
      "loss: [15.36457767]\n",
      "episode:  401\n",
      "loss: [12.86872515]\n",
      "episode:  402\n",
      "loss: [12.42403004]\n",
      "episode:  403\n",
      "loss: [16.70338066]\n",
      "episode:  404\n",
      "loss: [13.17813508]\n",
      "episode:  405\n",
      "loss: [14.10033315]\n",
      "episode:  406\n",
      "loss: [11.4472461]\n",
      "episode:  407\n",
      "loss: [11.76681279]\n",
      "episode:  408\n",
      "loss: [15.23761924]\n",
      "episode:  409\n",
      "loss: [11.78707836]\n",
      "episode:  410\n",
      "loss: [13.77450636]\n",
      "episode:  411\n",
      "loss: [16.65282936]\n",
      "episode:  412\n",
      "loss: [14.06295005]\n",
      "episode:  413\n",
      "loss: [11.3333405]\n",
      "episode:  414\n",
      "loss: [12.29507959]\n",
      "episode:  415\n",
      "loss: [13.33610578]\n",
      "episode:  416\n",
      "loss: [14.95044066]\n",
      "episode:  417\n",
      "loss: [20.50859623]\n",
      "episode:  418\n",
      "loss: [13.36455627]\n",
      "episode:  419\n",
      "loss: [13.22648036]\n",
      "episode:  420\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3370], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3370, 1.0025, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5592], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5592, 1.0056, 0.9844], dtype=torch.float64)\n",
      "tensor([0.6060], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6060, 0.9989, 0.9766], dtype=torch.float64)\n",
      "tensor([0.6011], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6011, 0.9953, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5902], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5902, 1.0133, 0.9609], dtype=torch.float64)\n",
      "tensor([0.6234], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6234, 1.0243, 0.9531], dtype=torch.float64)\n",
      "tensor([0.6493], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6493, 1.0311, 0.9453], dtype=torch.float64)\n",
      "tensor([0.6661], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6661, 1.0209, 0.9375], dtype=torch.float64)\n",
      "tensor([0.6489], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6489, 1.0238, 0.9297], dtype=torch.float64)\n",
      "tensor([0.6502], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6502, 1.0317, 0.9219], dtype=torch.float64)\n",
      "tensor([0.6645], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6645, 1.0438, 0.9141], dtype=torch.float64)\n",
      "tensor([0.6890], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6890, 1.0518, 0.9062], dtype=torch.float64)\n",
      "tensor([0.7078], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7078, 1.0727, 0.8984], dtype=torch.float64)\n",
      "tensor([0.7516], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7516, 1.0603, 0.8906], dtype=torch.float64)\n",
      "tensor([0.7351], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7351, 1.0390, 0.8828], dtype=torch.float64)\n",
      "tensor([0.6890], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6890, 1.0735, 0.8750], dtype=torch.float64)\n",
      "tensor([0.7508], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7508, 1.0576, 0.8672], dtype=torch.float64)\n",
      "tensor([0.7306], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7306, 1.0781, 0.8594], dtype=torch.float64)\n",
      "tensor([0.7692], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7692, 1.0971, 0.8516], dtype=torch.float64)\n",
      "tensor([0.8123], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8123, 1.0883, 0.8438], dtype=torch.float64)\n",
      "tensor([0.8042], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8042, 1.0727, 0.8359], dtype=torch.float64)\n",
      "tensor([0.7737], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7737, 1.0752, 0.8281], dtype=torch.float64)\n",
      "tensor([0.7733], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7733, 1.0649, 0.8203], dtype=torch.float64)\n",
      "tensor([0.7530], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7530, 1.0777, 0.8125], dtype=torch.float64)\n",
      "tensor([0.7749], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7749, 1.0678, 0.8047], dtype=torch.float64)\n",
      "tensor([0.7602], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7602, 1.0891, 0.7969], dtype=torch.float64)\n",
      "tensor([0.7983], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7983, 1.0996, 0.7891], dtype=torch.float64)\n",
      "tensor([0.8253], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8253, 1.1072, 0.7812], dtype=torch.float64)\n",
      "tensor([0.8450], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8450, 1.0967, 0.7734], dtype=torch.float64)\n",
      "tensor([0.8292], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8292, 1.1067, 0.7656], dtype=torch.float64)\n",
      "tensor([0.8455], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8455, 1.1053, 0.7578], dtype=torch.float64)\n",
      "tensor([0.8461], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8461, 1.1349, 0.7500], dtype=torch.float64)\n",
      "tensor([0.8786], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8786, 1.1233, 0.7422], dtype=torch.float64)\n",
      "tensor([0.8704], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8704, 1.1319, 0.7344], dtype=torch.float64)\n",
      "tensor([0.8784], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8784, 1.1362, 0.7266], dtype=torch.float64)\n",
      "tensor([0.8837], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8837, 1.1219, 0.7188], dtype=torch.float64)\n",
      "tensor([0.8701], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8701, 1.1067, 0.7109], dtype=torch.float64)\n",
      "tensor([0.8538], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8538, 1.1046, 0.7031], dtype=torch.float64)\n",
      "tensor([0.8490], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8490, 1.1102, 0.6953], dtype=torch.float64)\n",
      "tensor([0.8556], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8556, 1.0809, 0.6875], dtype=torch.float64)\n",
      "tensor([0.8057], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8057, 1.0714, 0.6797], dtype=torch.float64)\n",
      "tensor([0.7792], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7792, 1.0900, 0.6719], dtype=torch.float64)\n",
      "tensor([0.8095], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8095, 1.0897, 0.6641], dtype=torch.float64)\n",
      "tensor([0.8150], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8150, 1.1476, 0.6562], dtype=torch.float64)\n",
      "tensor([0.8906], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8906, 1.1397, 0.6484], dtype=torch.float64)\n",
      "tensor([0.8904], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8904, 1.1600, 0.6406], dtype=torch.float64)\n",
      "tensor([0.9109], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9109, 1.1692, 0.6328], dtype=torch.float64)\n",
      "tensor([0.9223], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9223, 1.1592, 0.6250], dtype=torch.float64)\n",
      "tensor([0.9137], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9137, 1.1680, 0.6172], dtype=torch.float64)\n",
      "tensor([0.9218], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9218, 1.1727, 0.6094], dtype=torch.float64)\n",
      "tensor([0.9275], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9275, 1.1683, 0.6016], dtype=torch.float64)\n",
      "tensor([0.9238], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9238, 1.1666, 0.5938], dtype=torch.float64)\n",
      "tensor([0.9220], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9220, 1.1667, 0.5859], dtype=torch.float64)\n",
      "tensor([0.9221], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9221, 1.1581, 0.5781], dtype=torch.float64)\n",
      "tensor([0.9137], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9137, 1.1390, 0.5703], dtype=torch.float64)\n",
      "tensor([0.8939], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8939, 1.1676, 0.5625], dtype=torch.float64)\n",
      "tensor([0.9208], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9208, 1.1665, 0.5547], dtype=torch.float64)\n",
      "tensor([0.9225], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9225, 1.1648, 0.5469], dtype=torch.float64)\n",
      "tensor([0.9213], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9213, 1.1419, 0.5391], dtype=torch.float64)\n",
      "tensor([0.8984], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8984, 1.1334, 0.5312], dtype=torch.float64)\n",
      "tensor([0.8879], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8879, 1.1448, 0.5234], dtype=torch.float64)\n",
      "tensor([0.8985], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8985, 1.1519, 0.5156], dtype=torch.float64)\n",
      "tensor([0.9068], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9068, 1.1516, 0.5078], dtype=torch.float64)\n",
      "tensor([0.9075], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9075, 1.1701, 0.5000], dtype=torch.float64)\n",
      "tensor([0.9263], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9263, 1.1627, 0.4922], dtype=torch.float64)\n",
      "tensor([0.9210], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9210, 1.1652, 0.4844], dtype=torch.float64)\n",
      "tensor([0.9231], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9231, 1.1578, 0.4766], dtype=torch.float64)\n",
      "tensor([0.9161], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9161, 1.1514, 0.4688], dtype=torch.float64)\n",
      "tensor([0.9092], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9092, 1.1721, 0.4609], dtype=torch.float64)\n",
      "tensor([0.9294], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9294, 1.1771, 0.4531], dtype=torch.float64)\n",
      "tensor([0.9366], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9366, 1.1462, 0.4453], dtype=torch.float64)\n",
      "tensor([0.9066], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9066, 1.1639, 0.4375], dtype=torch.float64)\n",
      "tensor([0.9216], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9216, 1.1615, 0.4297], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9208], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9208, 1.1864, 0.4219], dtype=torch.float64)\n",
      "tensor([0.9459], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9459, 1.1874, 0.4141], dtype=torch.float64)\n",
      "tensor([0.9496], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9496, 1.2076, 0.4062], dtype=torch.float64)\n",
      "tensor([0.9702], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9702, 1.2290, 0.3984], dtype=torch.float64)\n",
      "tensor([0.9931], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9931, 1.2169, 0.3906], dtype=torch.float64)\n",
      "tensor([0.9826], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9826, 1.2113, 0.3828], dtype=torch.float64)\n",
      "tensor([0.9764], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9764, 1.2270, 0.3750], dtype=torch.float64)\n",
      "tensor([0.9918], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9918, 1.2316, 0.3672], dtype=torch.float64)\n",
      "tensor([0.9975], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9975, 1.2524, 0.3594], dtype=torch.float64)\n",
      "tensor([1.0189], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0189, 1.2707, 0.3516], dtype=torch.float64)\n",
      "tensor([1.0387], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0387, 1.2745, 0.3438], dtype=torch.float64)\n",
      "tensor([1.0439], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0439, 1.2712, 0.3359], dtype=torch.float64)\n",
      "tensor([1.0410], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0410, 1.3001, 0.3281], dtype=torch.float64)\n",
      "tensor([1.0699], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0699, 1.2787, 0.3203], dtype=torch.float64)\n",
      "tensor([1.0504], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0504, 1.2804, 0.3125], dtype=torch.float64)\n",
      "tensor([1.0509], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0509, 1.2969, 0.3047], dtype=torch.float64)\n",
      "tensor([1.0676], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0676, 1.2971, 0.2969], dtype=torch.float64)\n",
      "tensor([1.0690], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0690, 1.2985, 0.2891], dtype=torch.float64)\n",
      "tensor([1.0706], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0706, 1.3150, 0.2812], dtype=torch.float64)\n",
      "tensor([1.0873], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0873, 1.3217, 0.2734], dtype=torch.float64)\n",
      "tensor([1.0952], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0952, 1.2963, 0.2656], dtype=torch.float64)\n",
      "tensor([1.0704], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0704, 1.2889, 0.2578], dtype=torch.float64)\n",
      "tensor([1.0614], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0614, 1.2888, 0.2500], dtype=torch.float64)\n",
      "tensor([1.0608], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0608, 1.2794, 0.2422], dtype=torch.float64)\n",
      "tensor([1.0514], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0514, 1.3110, 0.2344], dtype=torch.float64)\n",
      "tensor([1.0825], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0825, 1.3210, 0.2266], dtype=torch.float64)\n",
      "tensor([1.0947], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0947, 1.3232, 0.2188], dtype=torch.float64)\n",
      "tensor([1.0979], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0979, 1.3350, 0.2109], dtype=torch.float64)\n",
      "tensor([1.1100], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1100, 1.3453, 0.2031], dtype=torch.float64)\n",
      "tensor([1.1211], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1211, 1.3377, 0.1953], dtype=torch.float64)\n",
      "tensor([1.1144], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1144, 1.3084, 0.1875], dtype=torch.float64)\n",
      "tensor([1.0846], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0846, 1.3155, 0.1797], dtype=torch.float64)\n",
      "tensor([1.0899], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0899, 1.3305, 0.1719], dtype=torch.float64)\n",
      "tensor([1.1054], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1054, 1.3280, 0.1641], dtype=torch.float64)\n",
      "tensor([1.1039], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1039, 1.3084, 0.1562], dtype=torch.float64)\n",
      "tensor([1.0843], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0843, 1.3262, 0.1484], dtype=torch.float64)\n",
      "tensor([1.1009], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1009, 1.3079, 0.1406], dtype=torch.float64)\n",
      "tensor([1.0838], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0838, 1.3363, 0.1328], dtype=torch.float64)\n",
      "tensor([1.1112], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1112, 1.3322, 0.1250], dtype=torch.float64)\n",
      "tensor([1.1090], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1090, 1.3648, 0.1172], dtype=torch.float64)\n",
      "tensor([1.1415], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1415, 1.3680, 0.1094], dtype=torch.float64)\n",
      "tensor([1.1470], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1470, 1.3500, 0.1016], dtype=torch.float64)\n",
      "tensor([1.1295], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1295, 1.3674, 0.0938], dtype=torch.float64)\n",
      "tensor([1.1458], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1458, 1.3723, 0.0859], dtype=torch.float64)\n",
      "tensor([1.1519], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1519, 1.3624, 0.0781], dtype=torch.float64)\n",
      "tensor([1.1425], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1425, 1.3683, 0.0703], dtype=torch.float64)\n",
      "tensor([1.1479], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1479, 1.3620, 0.0625], dtype=torch.float64)\n",
      "tensor([1.1420], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1420, 1.3793, 0.0547], dtype=torch.float64)\n",
      "tensor([1.1591], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1591, 1.3679, 0.0469], dtype=torch.float64)\n",
      "tensor([1.1488], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1488, 1.3675, 0.0391], dtype=torch.float64)\n",
      "tensor([1.1478], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1478, 1.3409, 0.0312], dtype=torch.float64)\n",
      "tensor([1.1212], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1212, 1.3688, 0.0234], dtype=torch.float64)\n",
      "tensor([1.1475], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1475, 1.4002, 0.0156], dtype=torch.float64)\n",
      "tensor([1.1807], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.1807, 1.3908, 0.0078], dtype=torch.float64)\n",
      "tensor([1.1737], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.2327110548078983e-06 6.937878762530069e-07\n",
      "loss: [12.32781523]\n",
      "episode:  421\n",
      "loss: [17.47140831]\n",
      "episode:  422\n",
      "loss: [15.09271294]\n",
      "episode:  423\n",
      "loss: [11.83437258]\n",
      "episode:  424\n",
      "loss: [13.15292053]\n",
      "episode:  425\n",
      "loss: [16.06009867]\n",
      "episode:  426\n",
      "loss: [16.93128384]\n",
      "episode:  427\n",
      "loss: [10.28659342]\n",
      "episode:  428\n",
      "loss: [11.94251672]\n",
      "episode:  429\n",
      "loss: [14.57331262]\n",
      "episode:  430\n",
      "loss: [12.59563371]\n",
      "episode:  431\n",
      "loss: [14.55066595]\n",
      "episode:  432\n",
      "loss: [13.08753027]\n",
      "episode:  433\n",
      "loss: [15.15199567]\n",
      "episode:  434\n",
      "loss: [15.86779362]\n",
      "episode:  435\n",
      "loss: [15.10217789]\n",
      "episode:  436\n",
      "loss: [12.95816788]\n",
      "episode:  437\n",
      "loss: [14.21632544]\n",
      "episode:  438\n",
      "loss: [11.82058633]\n",
      "episode:  439\n",
      "loss: [13.86290123]\n",
      "episode:  440\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3345], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3345, 1.0099, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5709], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5709, 1.0136, 0.9844], dtype=torch.float64)\n",
      "tensor([0.6204], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6204, 1.0157, 0.9766], dtype=torch.float64)\n",
      "tensor([0.6324], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6324, 1.0225, 0.9688], dtype=torch.float64)\n",
      "tensor([0.6466], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6466, 1.0452, 0.9609], dtype=torch.float64)\n",
      "tensor([0.6912], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6912, 1.0534, 0.9531], dtype=torch.float64)\n",
      "tensor([0.7138], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7138, 1.0545, 0.9453], dtype=torch.float64)\n",
      "tensor([0.7192], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7192, 1.0331, 0.9375], dtype=torch.float64)\n",
      "tensor([0.6787], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6787, 1.0274, 0.9297], dtype=torch.float64)\n",
      "tensor([0.6595], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6595, 1.0204, 0.9219], dtype=torch.float64)\n",
      "tensor([0.6418], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6418, 1.0233, 0.9141], dtype=torch.float64)\n",
      "tensor([0.6431], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6431, 1.0119, 0.9062], dtype=torch.float64)\n",
      "tensor([0.6207], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6207, 1.0194, 0.8984], dtype=torch.float64)\n",
      "tensor([0.6299], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6299, 1.0045, 0.8906], dtype=torch.float64)\n",
      "tensor([0.6021], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6021, 1.0214, 0.8828], dtype=torch.float64)\n",
      "tensor([0.6283], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6283, 1.0169, 0.8750], dtype=torch.float64)\n",
      "tensor([0.6238], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6238, 0.9958, 0.8672], dtype=torch.float64)\n",
      "tensor([0.5713], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5713, 0.9879, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5361], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5361, 1.0011, 0.8516], dtype=torch.float64)\n",
      "tensor([0.5643], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5643, 1.0115, 0.8438], dtype=torch.float64)\n",
      "tensor([0.5978], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5978, 0.9895, 0.8359], dtype=torch.float64)\n",
      "tensor([0.5412], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5412, 0.9858, 0.8281], dtype=torch.float64)\n",
      "tensor([0.5167], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5167, 1.0047, 0.8203], dtype=torch.float64)\n",
      "tensor([0.5636], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5636, 1.0040, 0.8125], dtype=torch.float64)\n",
      "tensor([0.5701], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5701, 1.0175, 0.8047], dtype=torch.float64)\n",
      "tensor([0.6095], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6095, 1.0175, 0.7969], dtype=torch.float64)\n",
      "tensor([0.6181], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6181, 1.0082, 0.7891], dtype=torch.float64)\n",
      "tensor([0.5956], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5956, 1.0030, 0.7812], dtype=torch.float64)\n",
      "tensor([0.5741], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5741, 0.9964, 0.7734], dtype=torch.float64)\n",
      "tensor([0.5488], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5488, 0.9763, 0.7656], dtype=torch.float64)\n",
      "tensor([0.4802], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4802, 0.9974, 0.7578], dtype=torch.float64)\n",
      "tensor([0.5294], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5294, 0.9847, 0.7500], dtype=torch.float64)\n",
      "tensor([0.5017], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5017, 0.9877, 0.7422], dtype=torch.float64)\n",
      "tensor([0.5043], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5043, 0.9780, 0.7344], dtype=torch.float64)\n",
      "tensor([0.4747], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4747, 0.9567, 0.7266], dtype=torch.float64)\n",
      "tensor([0.4011], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4011, 0.9699, 0.7188], dtype=torch.float64)\n",
      "tensor([0.4242], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4242, 0.9550, 0.7109], dtype=torch.float64)\n",
      "tensor([0.3834], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3834, 0.9634, 0.7031], dtype=torch.float64)\n",
      "tensor([0.4000], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4000, 0.9534, 0.6953], dtype=torch.float64)\n",
      "tensor([0.3727], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3727, 0.9370, 0.6875], dtype=torch.float64)\n",
      "tensor([0.3151], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3151, 0.9243, 0.6797], dtype=torch.float64)\n",
      "tensor([0.2616], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2616, 0.9229, 0.6719], dtype=torch.float64)\n",
      "tensor([0.2417], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2417, 0.9198, 0.6641], dtype=torch.float64)\n",
      "tensor([0.2246], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2246, 0.9325, 0.6562], dtype=torch.float64)\n",
      "tensor([0.2605], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2605, 0.9401, 0.6484], dtype=torch.float64)\n",
      "tensor([0.2955], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2955, 0.9491, 0.6406], dtype=torch.float64)\n",
      "tensor([0.3338], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3338, 0.9472, 0.6328], dtype=torch.float64)\n",
      "tensor([0.3373], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3373, 0.9559, 0.6250], dtype=torch.float64)\n",
      "tensor([0.3649], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3649, 0.9482, 0.6172], dtype=torch.float64)\n",
      "tensor([0.3479], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3479, 0.9550, 0.6094], dtype=torch.float64)\n",
      "tensor([0.3648], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3648, 0.9762, 0.6016], dtype=torch.float64)\n",
      "tensor([0.4348], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4348, 0.9870, 0.5938], dtype=torch.float64)\n",
      "tensor([0.4853], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4853, 0.9935, 0.5859], dtype=torch.float64)\n",
      "tensor([0.5180], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5180, 1.0153, 0.5781], dtype=torch.float64)\n",
      "tensor([0.5929], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5929, 0.9970, 0.5703], dtype=torch.float64)\n",
      "tensor([0.5545], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5545, 0.9983, 0.5625], dtype=torch.float64)\n",
      "tensor([0.5494], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5494, 0.9800, 0.5547], dtype=torch.float64)\n",
      "tensor([0.4914], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4914, 0.9791, 0.5469], dtype=torch.float64)\n",
      "tensor([0.4746], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4746, 0.9754, 0.5391], dtype=torch.float64)\n",
      "tensor([0.4587], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4587, 0.9756, 0.5312], dtype=torch.float64)\n",
      "tensor([0.4555], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4555, 0.9685, 0.5234], dtype=torch.float64)\n",
      "tensor([0.4327], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4327, 0.9532, 0.5156], dtype=torch.float64)\n",
      "tensor([0.3787], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3787, 0.9357, 0.5078], dtype=torch.float64)\n",
      "tensor([0.3042], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3042, 0.9393, 0.5000], dtype=torch.float64)\n",
      "tensor([0.2921], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2921, 0.9372, 0.4922], dtype=torch.float64)\n",
      "tensor([0.2808], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2808, 0.9266, 0.4844], dtype=torch.float64)\n",
      "tensor([0.2417], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2417, 0.9160, 0.4766], dtype=torch.float64)\n",
      "tensor([0.1943], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1943, 0.9167, 0.4688], dtype=torch.float64)\n",
      "tensor([0.1811], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1811, 0.9261, 0.4609], dtype=torch.float64)\n",
      "tensor([0.2071], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2071, 0.9409, 0.4531], dtype=torch.float64)\n",
      "tensor([0.2629], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2629, 0.9432, 0.4453], dtype=torch.float64)\n",
      "tensor([0.2871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2871, 0.9612, 0.4375], dtype=torch.float64)\n",
      "tensor([0.3529], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3529, 0.9483, 0.4297], dtype=torch.float64)\n",
      "tensor([0.3299], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3299, 0.9498, 0.4219], dtype=torch.float64)\n",
      "tensor([0.3271], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3271, 0.9349, 0.4141], dtype=torch.float64)\n",
      "tensor([0.2765], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2765, 0.9330, 0.4062], dtype=torch.float64)\n",
      "tensor([0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2540, 0.9462, 0.3984], dtype=torch.float64)\n",
      "tensor([0.2897], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2897, 0.9575, 0.3906], dtype=torch.float64)\n",
      "tensor([0.3370], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3370, 0.9588, 0.3828], dtype=torch.float64)\n",
      "tensor([0.3551], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3551, 0.9446, 0.3750], dtype=torch.float64)\n",
      "tensor([0.3133], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3133, 0.9585, 0.3672], dtype=torch.float64)\n",
      "tensor([0.3453], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3453, 0.9529, 0.3594], dtype=torch.float64)\n",
      "tensor([0.3360], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3360, 0.9572, 0.3516], dtype=torch.float64)\n",
      "tensor([0.3465], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3465, 0.9595, 0.3438], dtype=torch.float64)\n",
      "tensor([0.3565], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3565, 0.9549, 0.3359], dtype=torch.float64)\n",
      "tensor([0.3438], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3438, 0.9510, 0.3281], dtype=torch.float64)\n",
      "tensor([0.3262], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3262, 0.9579, 0.3203], dtype=torch.float64)\n",
      "tensor([0.3428], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3428, 0.9500, 0.3125], dtype=torch.float64)\n",
      "tensor([0.3211], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3211, 0.9230, 0.3047], dtype=torch.float64)\n",
      "tensor([0.2252], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2252, 0.9136, 0.2969], dtype=torch.float64)\n",
      "tensor([0.1675], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1675, 0.9101, 0.2891], dtype=torch.float64)\n",
      "tensor([0.1413], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1413, 0.8978, 0.2812], dtype=torch.float64)\n",
      "tensor([0.1086], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1086, 0.8957, 0.2734], dtype=torch.float64)\n",
      "tensor([0.0987], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0987, 0.9056, 0.2656], dtype=torch.float64)\n",
      "tensor([0.1150], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1150, 0.9081, 0.2578], dtype=torch.float64)\n",
      "tensor([0.1221], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1221, 0.8966, 0.2500], dtype=torch.float64)\n",
      "tensor([0.1017], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1017, 0.8841, 0.2422], dtype=torch.float64)\n",
      "tensor([0.0754], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0754, 0.8865, 0.2344], dtype=torch.float64)\n",
      "tensor([0.0787], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0787, 0.8979, 0.2266], dtype=torch.float64)\n",
      "tensor([0.0981], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0981, 0.9085, 0.2188], dtype=torch.float64)\n",
      "tensor([0.1181], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1181, 0.8998, 0.2109], dtype=torch.float64)\n",
      "tensor([0.1052], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1052, 0.8953, 0.2031], dtype=torch.float64)\n",
      "tensor([0.0942], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0942, 0.8980, 0.1953], dtype=torch.float64)\n",
      "tensor([0.0973], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0973, 0.8922, 0.1875], dtype=torch.float64)\n",
      "tensor([0.0867], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0867, 0.8922, 0.1797], dtype=torch.float64)\n",
      "tensor([0.0872], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0872, 0.9062, 0.1719], dtype=torch.float64)\n",
      "tensor([0.1117], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1117, 0.9033, 0.1641], dtype=torch.float64)\n",
      "tensor([0.1083], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1083, 0.8969, 0.1562], dtype=torch.float64)\n",
      "tensor([0.0956], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0956, 0.9155, 0.1484], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1276], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1276, 0.9034, 0.1406], dtype=torch.float64)\n",
      "tensor([0.1103], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1103, 0.9010, 0.1328], dtype=torch.float64)\n",
      "tensor([0.1025], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1025, 0.8898, 0.1250], dtype=torch.float64)\n",
      "tensor([0.0819], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0819, 0.9097, 0.1172], dtype=torch.float64)\n",
      "tensor([0.1216], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1216, 0.9128, 0.1094], dtype=torch.float64)\n",
      "tensor([0.1251], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1251, 0.9110, 0.1016], dtype=torch.float64)\n",
      "tensor([0.1221], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1221, 0.9347, 0.0938], dtype=torch.float64)\n",
      "tensor([0.1849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1849, 0.9272, 0.0859], dtype=torch.float64)\n",
      "tensor([0.1799], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1799, 0.9552, 0.0781], dtype=torch.float64)\n",
      "tensor([0.2655], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2655, 0.9492, 0.0703], dtype=torch.float64)\n",
      "tensor([0.2717], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2717, 0.9573, 0.0625], dtype=torch.float64)\n",
      "tensor([0.2993], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2993, 0.9493, 0.0547], dtype=torch.float64)\n",
      "tensor([0.2807], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2807, 0.9542, 0.0469], dtype=torch.float64)\n",
      "tensor([0.2904], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2904, 0.9841, 0.0391], dtype=torch.float64)\n",
      "tensor([0.3909], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3909, 0.9859, 0.0312], dtype=torch.float64)\n",
      "tensor([0.4270], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4270, 0.9754, 0.0234], dtype=torch.float64)\n",
      "tensor([0.4027], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4027, 0.9718, 0.0156], dtype=torch.float64)\n",
      "tensor([0.3827], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3827, 0.9758, 0.0078], dtype=torch.float64)\n",
      "tensor([0.3889], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.1687066804841264e-06 6.6811100966529e-07\n",
      "loss: [14.10661365]\n",
      "episode:  441\n",
      "loss: [13.85891226]\n",
      "episode:  442\n",
      "loss: [15.40674566]\n",
      "episode:  443\n",
      "loss: [12.01715555]\n",
      "episode:  444\n",
      "loss: [42.16220324]\n",
      "episode:  445\n",
      "loss: [12.25808957]\n",
      "episode:  446\n",
      "loss: [11.12330638]\n",
      "episode:  447\n",
      "loss: [12.33828618]\n",
      "episode:  448\n",
      "loss: [14.68705404]\n",
      "episode:  449\n",
      "loss: [14.47152389]\n",
      "episode:  450\n",
      "loss: [10.22286462]\n",
      "episode:  451\n",
      "loss: [17.12849355]\n",
      "episode:  452\n",
      "loss: [19.59196851]\n",
      "episode:  453\n",
      "loss: [12.83720564]\n",
      "episode:  454\n",
      "loss: [10.64394436]\n",
      "episode:  455\n",
      "loss: [15.18471581]\n",
      "episode:  456\n",
      "loss: [17.34995786]\n",
      "episode:  457\n",
      "loss: [56.29760669]\n",
      "episode:  458\n",
      "loss: [11.00415999]\n",
      "episode:  459\n",
      "loss: [25.31950804]\n",
      "episode:  460\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3333], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3333, 1.0043, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5571], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5571, 0.9932, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5719], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5719, 0.9897, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5636], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5636, 0.9801, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5331], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5331, 0.9562, 0.9609], dtype=torch.float64)\n",
      "tensor([0.4566], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4566, 0.9510, 0.9531], dtype=torch.float64)\n",
      "tensor([0.4236], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4236, 0.9441, 0.9453], dtype=torch.float64)\n",
      "tensor([0.3952], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3952, 0.9510, 0.9375], dtype=torch.float64)\n",
      "tensor([0.4071], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4071, 0.9463, 0.9297], dtype=torch.float64)\n",
      "tensor([0.3945], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3945, 0.9600, 0.9219], dtype=torch.float64)\n",
      "tensor([0.4292], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4292, 0.9477, 0.9141], dtype=torch.float64)\n",
      "tensor([0.4001], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4001, 0.9487, 0.9062], dtype=torch.float64)\n",
      "tensor([0.3950], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3950, 0.9575, 0.8984], dtype=torch.float64)\n",
      "tensor([0.4172], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4172, 0.9671, 0.8906], dtype=torch.float64)\n",
      "tensor([0.4479], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4479, 0.9717, 0.8828], dtype=torch.float64)\n",
      "tensor([0.4659], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4659, 0.9653, 0.8750], dtype=torch.float64)\n",
      "tensor([0.4498], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4498, 0.9734, 0.8672], dtype=torch.float64)\n",
      "tensor([0.4679], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4679, 0.9682, 0.8594], dtype=torch.float64)\n",
      "tensor([0.4553], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4553, 0.9678, 0.8516], dtype=torch.float64)\n",
      "tensor([0.4496], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4496, 0.9565, 0.8438], dtype=torch.float64)\n",
      "tensor([0.4145], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4145, 0.9382, 0.8359], dtype=torch.float64)\n",
      "tensor([0.3530], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3530, 0.9467, 0.8281], dtype=torch.float64)\n",
      "tensor([0.3623], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3623, 0.9480, 0.8203], dtype=torch.float64)\n",
      "tensor([0.3664], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3664, 0.9440, 0.8125], dtype=torch.float64)\n",
      "tensor([0.3541], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3541, 0.9536, 0.8047], dtype=torch.float64)\n",
      "tensor([0.3773], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3773, 0.9530, 0.7969], dtype=torch.float64)\n",
      "tensor([0.3789], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3789, 0.9433, 0.7891], dtype=torch.float64)\n",
      "tensor([0.3499], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3499, 0.9401, 0.7812], dtype=torch.float64)\n",
      "tensor([0.3330], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3330, 0.9431, 0.7734], dtype=torch.float64)\n",
      "tensor([0.3362], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3362, 0.9420, 0.7656], dtype=torch.float64)\n",
      "tensor([0.3321], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3321, 0.9265, 0.7578], dtype=torch.float64)\n",
      "tensor([0.2852], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2852, 0.9418, 0.7500], dtype=torch.float64)\n",
      "tensor([0.3172], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3172, 0.9534, 0.7422], dtype=torch.float64)\n",
      "tensor([0.3555], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3555, 0.9291, 0.7344], dtype=torch.float64)\n",
      "tensor([0.2928], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2928, 0.9519, 0.7266], dtype=torch.float64)\n",
      "tensor([0.3427], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3427, 0.9586, 0.7188], dtype=torch.float64)\n",
      "tensor([0.3736], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3736, 0.9527, 0.7109], dtype=torch.float64)\n",
      "tensor([0.3627], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3627, 0.9713, 0.7031], dtype=torch.float64)\n",
      "tensor([0.4177], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4177, 0.9741, 0.6953], dtype=torch.float64)\n",
      "tensor([0.4395], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4395, 0.9727, 0.6875], dtype=torch.float64)\n",
      "tensor([0.4406], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4406, 0.9703, 0.6797], dtype=torch.float64)\n",
      "tensor([0.4333], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4333, 0.9716, 0.6719], dtype=torch.float64)\n",
      "tensor([0.4355], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4355, 0.9626, 0.6641], dtype=torch.float64)\n",
      "tensor([0.4081], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4081, 0.9632, 0.6562], dtype=torch.float64)\n",
      "tensor([0.4034], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4034, 0.9608, 0.6484], dtype=torch.float64)\n",
      "tensor([0.3949], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3949, 0.9545, 0.6406], dtype=torch.float64)\n",
      "tensor([0.3732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3732, 0.9516, 0.6328], dtype=torch.float64)\n",
      "tensor([0.3590], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3590, 0.9476, 0.6250], dtype=torch.float64)\n",
      "tensor([0.3431], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3431, 0.9574, 0.6172], dtype=torch.float64)\n",
      "tensor([0.3696], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3696, 0.9424, 0.6094], dtype=torch.float64)\n",
      "tensor([0.3294], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3294, 0.9306, 0.6016], dtype=torch.float64)\n",
      "tensor([0.2803], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2803, 0.9441, 0.5938], dtype=torch.float64)\n",
      "tensor([0.3087], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3087, 0.9372, 0.5859], dtype=torch.float64)\n",
      "tensor([0.2940], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2940, 0.9624, 0.5781], dtype=torch.float64)\n",
      "tensor([0.3713], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3713, 0.9621, 0.5703], dtype=torch.float64)\n",
      "tensor([0.3909], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3909, 0.9521, 0.5625], dtype=torch.float64)\n",
      "tensor([0.3646], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3646, 0.9466, 0.5547], dtype=torch.float64)\n",
      "tensor([0.3391], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3391, 0.9719, 0.5469], dtype=torch.float64)\n",
      "tensor([0.4131], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4131, 0.9652, 0.5391], dtype=torch.float64)\n",
      "tensor([0.4105], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4105, 0.9704, 0.5312], dtype=torch.float64)\n",
      "tensor([0.4259], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4259, 0.9565, 0.5234], dtype=torch.float64)\n",
      "tensor([0.3865], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3865, 0.9826, 0.5156], dtype=torch.float64)\n",
      "tensor([0.4581], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4581, 1.0158, 0.5078], dtype=torch.float64)\n",
      "tensor([0.5774], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5774, 1.0162, 0.5000], dtype=torch.float64)\n",
      "tensor([0.6072], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6072, 1.0198, 0.4922], dtype=torch.float64)\n",
      "tensor([0.6253], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6253, 1.0458, 0.4844], dtype=torch.float64)\n",
      "tensor([0.6947], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6947, 1.0324, 0.4766], dtype=torch.float64)\n",
      "tensor([0.6812], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6812, 1.0141, 0.4688], dtype=torch.float64)\n",
      "tensor([0.6254], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6254, 1.0098, 0.4609], dtype=torch.float64)\n",
      "tensor([0.5990], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5990, 1.0288, 0.4531], dtype=torch.float64)\n",
      "tensor([0.6507], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6507, 1.0458, 0.4453], dtype=torch.float64)\n",
      "tensor([0.7020], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7020, 1.0636, 0.4375], dtype=torch.float64)\n",
      "tensor([0.7499], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7499, 1.0610, 0.4297], dtype=torch.float64)\n",
      "tensor([0.7548], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7548, 1.0388, 0.4219], dtype=torch.float64)\n",
      "tensor([0.7100], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7100, 1.0404, 0.4141], dtype=torch.float64)\n",
      "tensor([0.7045], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7045, 1.0476, 0.4062], dtype=torch.float64)\n",
      "tensor([0.7188], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7188, 1.0375, 0.3984], dtype=torch.float64)\n",
      "tensor([0.7012], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7012, 1.0394, 0.3906], dtype=torch.float64)\n",
      "tensor([0.7019], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7019, 1.0395, 0.3828], dtype=torch.float64)\n",
      "tensor([0.7028], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7028, 1.0398, 0.3750], dtype=torch.float64)\n",
      "tensor([0.7039], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7039, 1.0267, 0.3672], dtype=torch.float64)\n",
      "tensor([0.6691], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6691, 1.0302, 0.3594], dtype=torch.float64)\n",
      "tensor([0.6714], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6714, 1.0242, 0.3516], dtype=torch.float64)\n",
      "tensor([0.6536], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6536, 1.0325, 0.3438], dtype=torch.float64)\n",
      "tensor([0.6747], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6747, 1.0481, 0.3359], dtype=torch.float64)\n",
      "tensor([0.7178], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7178, 1.0596, 0.3281], dtype=torch.float64)\n",
      "tensor([0.7510], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7510, 1.0597, 0.3203], dtype=torch.float64)\n",
      "tensor([0.7583], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7583, 1.0663, 0.3125], dtype=torch.float64)\n",
      "tensor([0.7736], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7736, 1.0605, 0.3047], dtype=torch.float64)\n",
      "tensor([0.7656], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7656, 1.0487, 0.2969], dtype=torch.float64)\n",
      "tensor([0.7397], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7397, 1.0610, 0.2891], dtype=torch.float64)\n",
      "tensor([0.7606], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7606, 1.0705, 0.2812], dtype=torch.float64)\n",
      "tensor([0.7833], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7833, 1.0787, 0.2734], dtype=torch.float64)\n",
      "tensor([0.8030], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8030, 1.0823, 0.2656], dtype=torch.float64)\n",
      "tensor([0.8137], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8137, 1.0790, 0.2578], dtype=torch.float64)\n",
      "tensor([0.8099], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8099, 1.0799, 0.2500], dtype=torch.float64)\n",
      "tensor([0.8113], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8113, 1.0799, 0.2422], dtype=torch.float64)\n",
      "tensor([0.8119], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8119, 1.0734, 0.2344], dtype=torch.float64)\n",
      "tensor([0.8003], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8003, 1.0700, 0.2266], dtype=torch.float64)\n",
      "tensor([0.7922], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7922, 1.0550, 0.2188], dtype=torch.float64)\n",
      "tensor([0.7625], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7625, 1.0814, 0.2109], dtype=torch.float64)\n",
      "tensor([0.8073], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8073, 1.0771, 0.2031], dtype=torch.float64)\n",
      "tensor([0.8080], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8080, 1.0570, 0.1953], dtype=torch.float64)\n",
      "tensor([0.7710], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7710, 1.0533, 0.1875], dtype=torch.float64)\n",
      "tensor([0.7565], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7565, 1.0214, 0.1797], dtype=torch.float64)\n",
      "tensor([0.6650], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6650, 1.0338, 0.1719], dtype=torch.float64)\n",
      "tensor([0.6776], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6776, 1.0248, 0.1641], dtype=torch.float64)\n",
      "tensor([0.6518], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6518, 1.0050, 0.1562], dtype=torch.float64)\n",
      "tensor([0.5794], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5794, 0.9963, 0.1484], dtype=torch.float64)\n",
      "tensor([0.5287], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5287, 1.0083, 0.1406], dtype=torch.float64)\n",
      "tensor([0.5512], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5512, 0.9957, 0.1328], dtype=torch.float64)\n",
      "tensor([0.5165], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5165, 1.0037, 0.1250], dtype=torch.float64)\n",
      "tensor([0.5311], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5311, 0.9829, 0.1172], dtype=torch.float64)\n",
      "tensor([0.4671], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4671, 0.9746, 0.1094], dtype=torch.float64)\n",
      "tensor([0.4196], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4196, 0.9822, 0.1016], dtype=torch.float64)\n",
      "tensor([0.4291], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4291, 0.9814, 0.0938], dtype=torch.float64)\n",
      "tensor([0.4288], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4288, 0.9771, 0.0859], dtype=torch.float64)\n",
      "tensor([0.4139], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4139, 0.9676, 0.0781], dtype=torch.float64)\n",
      "tensor([0.3773], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3773, 0.9671, 0.0703], dtype=torch.float64)\n",
      "tensor([0.3639], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3639, 0.9748, 0.0625], dtype=torch.float64)\n",
      "tensor([0.3840], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3840, 0.9956, 0.0547], dtype=torch.float64)\n",
      "tensor([0.4575], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4575, 0.9996, 0.0469], dtype=torch.float64)\n",
      "tensor([0.4923], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4923, 0.9863, 0.0391], dtype=torch.float64)\n",
      "tensor([0.4589], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4589, 1.0016, 0.0312], dtype=torch.float64)\n",
      "tensor([0.4980], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4980, 1.0139, 0.0234], dtype=torch.float64)\n",
      "tensor([0.5491], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5491, 1.0020, 0.0156], dtype=torch.float64)\n",
      "tensor([0.5254], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5254, 1.0175, 0.0078], dtype=torch.float64)\n",
      "tensor([0.5673], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.143851201128583e-06 5.965282020474578e-07\n",
      "loss: [13.93591019]\n",
      "episode:  461\n",
      "loss: [14.70748503]\n",
      "episode:  462\n",
      "loss: [13.76029655]\n",
      "episode:  463\n",
      "loss: [11.24559855]\n",
      "episode:  464\n",
      "loss: [14.6969684]\n",
      "episode:  465\n",
      "loss: [10.90309354]\n",
      "episode:  466\n",
      "loss: [16.92107737]\n",
      "episode:  467\n",
      "loss: [13.02409553]\n",
      "episode:  468\n",
      "loss: [11.19088431]\n",
      "episode:  469\n",
      "loss: [17.36066878]\n",
      "episode:  470\n",
      "loss: [14.61136045]\n",
      "episode:  471\n",
      "loss: [14.47090928]\n",
      "episode:  472\n",
      "loss: [13.03584238]\n",
      "episode:  473\n",
      "loss: [48.85143572]\n",
      "episode:  474\n",
      "loss: [16.81616369]\n",
      "episode:  475\n",
      "loss: [14.95977909]\n",
      "episode:  476\n",
      "loss: [13.29063847]\n",
      "episode:  477\n",
      "loss: [14.34817019]\n",
      "episode:  478\n",
      "loss: [16.17165674]\n",
      "episode:  479\n",
      "loss: [15.331543]\n",
      "episode:  480\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3329], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3329, 0.9946, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5284], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5284, 0.9993, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5807], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5807, 0.9928, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5728], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5728, 0.9994, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5871, 1.0111, 0.9609], dtype=torch.float64)\n",
      "tensor([0.6107], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6107, 1.0150, 0.9531], dtype=torch.float64)\n",
      "tensor([0.6213], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6213, 1.0100, 0.9453], dtype=torch.float64)\n",
      "tensor([0.6130], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6130, 1.0233, 0.9375], dtype=torch.float64)\n",
      "tensor([0.6354], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6354, 1.0161, 0.9297], dtype=torch.float64)\n",
      "tensor([0.6250], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6250, 0.9991, 0.9219], dtype=torch.float64)\n",
      "tensor([0.5885], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5885, 1.0112, 0.9141], dtype=torch.float64)\n",
      "tensor([0.6054], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6054, 0.9993, 0.9062], dtype=torch.float64)\n",
      "tensor([0.5818], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5818, 0.9837, 0.8984], dtype=torch.float64)\n",
      "tensor([0.5313], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5313, 0.9832, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5176], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5176, 0.9623, 0.8828], dtype=torch.float64)\n",
      "tensor([0.4534], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4534, 0.9741, 0.8750], dtype=torch.float64)\n",
      "tensor([0.4716], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4716, 0.9851, 0.8672], dtype=torch.float64)\n",
      "tensor([0.5051], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5051, 1.0143, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5894], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5894, 1.0254, 0.8516], dtype=torch.float64)\n",
      "tensor([0.6247], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6247, 1.0047, 0.8438], dtype=torch.float64)\n",
      "tensor([0.5880], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5880, 1.0231, 0.8359], dtype=torch.float64)\n",
      "tensor([0.6181], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6181, 1.0404, 0.8281], dtype=torch.float64)\n",
      "tensor([0.6606], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6606, 1.0398, 0.8203], dtype=torch.float64)\n",
      "tensor([0.6685], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6685, 1.0540, 0.8125], dtype=torch.float64)\n",
      "tensor([0.7000], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7000, 1.0641, 0.8047], dtype=torch.float64)\n",
      "tensor([0.7278], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7278, 1.0462, 0.7969], dtype=torch.float64)\n",
      "tensor([0.6967], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6967, 1.0226, 0.7891], dtype=torch.float64)\n",
      "tensor([0.6420], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6420, 1.0331, 0.7812], dtype=torch.float64)\n",
      "tensor([0.6530], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6530, 1.0323, 0.7734], dtype=torch.float64)\n",
      "tensor([0.6539], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6539, 1.0376, 0.7656], dtype=torch.float64)\n",
      "tensor([0.6655], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6655, 1.0338, 0.7578], dtype=torch.float64)\n",
      "tensor([0.6605], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6605, 1.0262, 0.7500], dtype=torch.float64)\n",
      "tensor([0.6441], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6441, 1.0484, 0.7422], dtype=torch.float64)\n",
      "tensor([0.6874], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6874, 1.0303, 0.7344], dtype=torch.float64)\n",
      "tensor([0.6590], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6590, 1.0246, 0.7266], dtype=torch.float64)\n",
      "tensor([0.6419], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6419, 1.0261, 0.7188], dtype=torch.float64)\n",
      "tensor([0.6419], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6419, 1.0461, 0.7109], dtype=torch.float64)\n",
      "tensor([0.6838], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6838, 1.0422, 0.7031], dtype=torch.float64)\n",
      "tensor([0.6848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6848, 1.0506, 0.6953], dtype=torch.float64)\n",
      "tensor([0.7027], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7027, 1.0806, 0.6875], dtype=torch.float64)\n",
      "tensor([0.7689], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7689, 1.0974, 0.6797], dtype=torch.float64)\n",
      "tensor([0.8126], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8126, 1.1096, 0.6719], dtype=torch.float64)\n",
      "tensor([0.8436], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8436, 1.1296, 0.6641], dtype=torch.float64)\n",
      "tensor([0.8669], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8669, 1.1350, 0.6562], dtype=torch.float64)\n",
      "tensor([0.8747], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8747, 1.1425, 0.6484], dtype=torch.float64)\n",
      "tensor([0.8829], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8829, 1.1460, 0.6406], dtype=torch.float64)\n",
      "tensor([0.8873], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8873, 1.1507, 0.6328], dtype=torch.float64)\n",
      "tensor([0.8926], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8926, 1.1454, 0.6250], dtype=torch.float64)\n",
      "tensor([0.8881], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8881, 1.1588, 0.6172], dtype=torch.float64)\n",
      "tensor([0.9010], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9010, 1.1634, 0.6094], dtype=torch.float64)\n",
      "tensor([0.9068], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9068, 1.1677, 0.6016], dtype=torch.float64)\n",
      "tensor([0.9119], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9119, 1.1417, 0.5938], dtype=torch.float64)\n",
      "tensor([0.8871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8871, 1.1538, 0.5859], dtype=torch.float64)\n",
      "tensor([0.8968], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8968, 1.1278, 0.5781], dtype=torch.float64)\n",
      "tensor([0.8725], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8725, 1.0911, 0.5703], dtype=torch.float64)\n",
      "tensor([0.8252], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8252, 1.0890, 0.5625], dtype=torch.float64)\n",
      "tensor([0.8130], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8130, 1.0756, 0.5547], dtype=torch.float64)\n",
      "tensor([0.7865], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7865, 1.0653, 0.5469], dtype=torch.float64)\n",
      "tensor([0.7622], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7622, 1.0449, 0.5391], dtype=torch.float64)\n",
      "tensor([0.7154], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7154, 1.0864, 0.5312], dtype=torch.float64)\n",
      "tensor([0.7897], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7897, 1.0817, 0.5234], dtype=torch.float64)\n",
      "tensor([0.7950], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7950, 1.0815, 0.5156], dtype=torch.float64)\n",
      "tensor([0.7960], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7960, 1.0797, 0.5078], dtype=torch.float64)\n",
      "tensor([0.7933], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7933, 1.0861, 0.5000], dtype=torch.float64)\n",
      "tensor([0.8050], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8050, 1.1140, 0.4922], dtype=torch.float64)\n",
      "tensor([0.8524], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8524, 1.1380, 0.4844], dtype=torch.float64)\n",
      "tensor([0.8807], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8807, 1.1503, 0.4766], dtype=torch.float64)\n",
      "tensor([0.8956], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8956, 1.1356, 0.4688], dtype=torch.float64)\n",
      "tensor([0.8829], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8829, 1.1231, 0.4609], dtype=torch.float64)\n",
      "tensor([0.8696], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8696, 1.1208, 0.4531], dtype=torch.float64)\n",
      "tensor([0.8663], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8663, 1.1383, 0.4453], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8833], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8833, 1.1526, 0.4375], dtype=torch.float64)\n",
      "tensor([0.8991], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8991, 1.1190, 0.4297], dtype=torch.float64)\n",
      "tensor([0.8681], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8681, 1.1411, 0.4219], dtype=torch.float64)\n",
      "tensor([0.8868], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8868, 1.1276, 0.4141], dtype=torch.float64)\n",
      "tensor([0.8757], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8757, 1.1049, 0.4062], dtype=torch.float64)\n",
      "tensor([0.8527], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8527, 1.1023, 0.3984], dtype=torch.float64)\n",
      "tensor([0.8481], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8481, 1.0936, 0.3906], dtype=torch.float64)\n",
      "tensor([0.8343], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8343, 1.0896, 0.3828], dtype=torch.float64)\n",
      "tensor([0.8248], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8248, 1.1041, 0.3750], dtype=torch.float64)\n",
      "tensor([0.8478], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8478, 1.1112, 0.3672], dtype=torch.float64)\n",
      "tensor([0.8571], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8571, 1.1008, 0.3594], dtype=torch.float64)\n",
      "tensor([0.8481], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8481, 1.1114, 0.3516], dtype=torch.float64)\n",
      "tensor([0.8578], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8578, 1.1245, 0.3438], dtype=torch.float64)\n",
      "tensor([0.8716], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8716, 1.1161, 0.3359], dtype=torch.float64)\n",
      "tensor([0.8651], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8651, 1.0989, 0.3281], dtype=torch.float64)\n",
      "tensor([0.8478], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8478, 1.1131, 0.3203], dtype=torch.float64)\n",
      "tensor([0.8601], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8601, 1.1190, 0.3125], dtype=torch.float64)\n",
      "tensor([0.8673], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8673, 1.0936, 0.3047], dtype=torch.float64)\n",
      "tensor([0.8420], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8420, 1.1047, 0.2969], dtype=torch.float64)\n",
      "tensor([0.8521], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8521, 1.0889, 0.2891], dtype=torch.float64)\n",
      "tensor([0.8313], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8313, 1.0864, 0.2812], dtype=torch.float64)\n",
      "tensor([0.8233], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8233, 1.0893, 0.2734], dtype=torch.float64)\n",
      "tensor([0.8277], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8277, 1.0597, 0.2656], dtype=torch.float64)\n",
      "tensor([0.7740], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7740, 1.0460, 0.2578], dtype=torch.float64)\n",
      "tensor([0.7356], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7356, 1.0433, 0.2500], dtype=torch.float64)\n",
      "tensor([0.7228], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7228, 1.0494, 0.2422], dtype=torch.float64)\n",
      "tensor([0.7331], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7331, 1.0444, 0.2344], dtype=torch.float64)\n",
      "tensor([0.7253], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7253, 1.0637, 0.2266], dtype=torch.float64)\n",
      "tensor([0.7643], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7643, 1.0360, 0.2188], dtype=torch.float64)\n",
      "tensor([0.7098], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7098, 1.0298, 0.2109], dtype=torch.float64)\n",
      "tensor([0.6779], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6779, 1.0557, 0.2031], dtype=torch.float64)\n",
      "tensor([0.7392], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7392, 1.0616, 0.1953], dtype=torch.float64)\n",
      "tensor([0.7645], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7645, 1.0364, 0.1875], dtype=torch.float64)\n",
      "tensor([0.7111], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7111, 1.0372, 0.1797], dtype=torch.float64)\n",
      "tensor([0.7006], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7006, 1.0251, 0.1719], dtype=torch.float64)\n",
      "tensor([0.6587], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6587, 1.0326, 0.1641], dtype=torch.float64)\n",
      "tensor([0.6695], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6695, 1.0340, 0.1562], dtype=torch.float64)\n",
      "tensor([0.6766], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6766, 1.0493, 0.1484], dtype=torch.float64)\n",
      "tensor([0.7269], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7269, 1.0570, 0.1406], dtype=torch.float64)\n",
      "tensor([0.7553], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7553, 1.0372, 0.1328], dtype=torch.float64)\n",
      "tensor([0.7106], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7106, 1.0269, 0.1250], dtype=torch.float64)\n",
      "tensor([0.6633], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6633, 1.0272, 0.1172], dtype=torch.float64)\n",
      "tensor([0.6491], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6491, 1.0256, 0.1094], dtype=torch.float64)\n",
      "tensor([0.6388], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6388, 1.0152, 0.1016], dtype=torch.float64)\n",
      "tensor([0.6019], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6019, 1.0137, 0.0938], dtype=torch.float64)\n",
      "tensor([0.5852], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5852, 1.0166, 0.0859], dtype=torch.float64)\n",
      "tensor([0.5885], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5885, 1.0247, 0.0781], dtype=torch.float64)\n",
      "tensor([0.6147], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6147, 1.0077, 0.0703], dtype=torch.float64)\n",
      "tensor([0.5675], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5675, 1.0229, 0.0625], dtype=torch.float64)\n",
      "tensor([0.6014], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6014, 1.0239, 0.0547], dtype=torch.float64)\n",
      "tensor([0.6139], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6139, 1.0253, 0.0469], dtype=torch.float64)\n",
      "tensor([0.6215], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6215, 1.0090, 0.0391], dtype=torch.float64)\n",
      "tensor([0.5707], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5707, 1.0083, 0.0312], dtype=torch.float64)\n",
      "tensor([0.5526], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5526, 1.0147, 0.0234], dtype=torch.float64)\n",
      "tensor([0.5667], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5667, 1.0206, 0.0156], dtype=torch.float64)\n",
      "tensor([0.5893], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5893, 1.0129, 0.0078], dtype=torch.float64)\n",
      "tensor([0.5706], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.1345340944343285e-06 7.501388483933532e-07\n",
      "loss: [12.66711029]\n",
      "episode:  481\n",
      "loss: [43.10618102]\n",
      "episode:  482\n",
      "loss: [14.28940513]\n",
      "episode:  483\n",
      "loss: [14.39958222]\n",
      "episode:  484\n",
      "loss: [12.17553418]\n",
      "episode:  485\n",
      "loss: [10.74554467]\n",
      "episode:  486\n",
      "loss: [13.79047211]\n",
      "episode:  487\n",
      "loss: [13.64278389]\n",
      "episode:  488\n",
      "loss: [15.40568275]\n",
      "episode:  489\n",
      "loss: [14.73840651]\n",
      "episode:  490\n",
      "loss: [12.05754853]\n",
      "episode:  491\n",
      "loss: [14.39745371]\n",
      "episode:  492\n",
      "loss: [15.89637537]\n",
      "episode:  493\n",
      "loss: [17.61690899]\n",
      "episode:  494\n",
      "loss: [15.05994168]\n",
      "episode:  495\n",
      "loss: [13.38843588]\n",
      "episode:  496\n",
      "loss: [13.2102665]\n",
      "episode:  497\n",
      "loss: [12.71545713]\n",
      "episode:  498\n",
      "loss: [13.3462131]\n",
      "episode:  499\n",
      "loss: [14.55432106]\n",
      "episode:  500\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3352], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3352, 1.0082, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5651], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5651, 1.0305, 0.9844], dtype=torch.float64)\n",
      "tensor([0.6481], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6481, 1.0266, 0.9766], dtype=torch.float64)\n",
      "tensor([0.6551], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6551, 1.0290, 0.9688], dtype=torch.float64)\n",
      "tensor([0.6600], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6600, 1.0323, 0.9609], dtype=torch.float64)\n",
      "tensor([0.6660], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6660, 1.0192, 0.9531], dtype=torch.float64)\n",
      "tensor([0.6417], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6417, 1.0115, 0.9453], dtype=torch.float64)\n",
      "tensor([0.6220], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6220, 1.0156, 0.9375], dtype=torch.float64)\n",
      "tensor([0.6250], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6250, 1.0021, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5995], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5995, 0.9915, 0.9219], dtype=torch.float64)\n",
      "tensor([0.5657], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5657, 0.9848, 0.9141], dtype=torch.float64)\n",
      "tensor([0.5380], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5380, 0.9802, 0.9062], dtype=torch.float64)\n",
      "tensor([0.5176], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5176, 0.9947, 0.8984], dtype=torch.float64)\n",
      "tensor([0.5522], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5522, 1.0084, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5931], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5931, 1.0149, 0.8828], dtype=torch.float64)\n",
      "tensor([0.6119], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6119, 1.0293, 0.8750], dtype=torch.float64)\n",
      "tensor([0.6412], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6412, 1.0459, 0.8672], dtype=torch.float64)\n",
      "tensor([0.6772], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6772, 1.0327, 0.8594], dtype=torch.float64)\n",
      "tensor([0.6577], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6577, 1.0302, 0.8516], dtype=torch.float64)\n",
      "tensor([0.6488], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6488, 1.0287, 0.8438], dtype=torch.float64)\n",
      "tensor([0.6443], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6443, 1.0239, 0.8359], dtype=torch.float64)\n",
      "tensor([0.6339], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6339, 1.0040, 0.8281], dtype=torch.float64)\n",
      "tensor([0.5885], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5885, 1.0085, 0.8203], dtype=torch.float64)\n",
      "tensor([0.5895], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5895, 1.0145, 0.8125], dtype=torch.float64)\n",
      "tensor([0.6044], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6044, 1.0238, 0.8047], dtype=torch.float64)\n",
      "tensor([0.6271], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6271, 1.0478, 0.7969], dtype=torch.float64)\n",
      "tensor([0.6820], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6820, 1.0546, 0.7891], dtype=torch.float64)\n",
      "tensor([0.7079], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7079, 1.0585, 0.7812], dtype=torch.float64)\n",
      "tensor([0.7218], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7218, 1.0520, 0.7734], dtype=torch.float64)\n",
      "tensor([0.7116], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7116, 1.0570, 0.7656], dtype=torch.float64)\n",
      "tensor([0.7201], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7201, 1.0689, 0.7578], dtype=torch.float64)\n",
      "tensor([0.7470], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7470, 1.0620, 0.7500], dtype=torch.float64)\n",
      "tensor([0.7386], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7386, 1.0517, 0.7422], dtype=torch.float64)\n",
      "tensor([0.7161], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7161, 1.0496, 0.7344], dtype=torch.float64)\n",
      "tensor([0.7076], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7076, 1.0441, 0.7266], dtype=torch.float64)\n",
      "tensor([0.6947], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6947, 1.0805, 0.7188], dtype=torch.float64)\n",
      "tensor([0.7681], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7681, 1.1009, 0.7109], dtype=torch.float64)\n",
      "tensor([0.8199], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8199, 1.0902, 0.7031], dtype=torch.float64)\n",
      "tensor([0.8098], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8098, 1.0970, 0.6953], dtype=torch.float64)\n",
      "tensor([0.8211], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8211, 1.1097, 0.6875], dtype=torch.float64)\n",
      "tensor([0.8458], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8458, 1.0955, 0.6797], dtype=torch.float64)\n",
      "tensor([0.8257], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8257, 1.1101, 0.6719], dtype=torch.float64)\n",
      "tensor([0.8470], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8470, 1.0898, 0.6641], dtype=torch.float64)\n",
      "tensor([0.8161], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8161, 1.1046, 0.6562], dtype=torch.float64)\n",
      "tensor([0.8381], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8381, 1.0889, 0.6484], dtype=torch.float64)\n",
      "tensor([0.8135], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8135, 1.0994, 0.6406], dtype=torch.float64)\n",
      "tensor([0.8288], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8288, 1.1030, 0.6328], dtype=torch.float64)\n",
      "tensor([0.8388], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8388, 1.1100, 0.6250], dtype=torch.float64)\n",
      "tensor([0.8494], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8494, 1.0956, 0.6172], dtype=torch.float64)\n",
      "tensor([0.8297], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8297, 1.0690, 0.6094], dtype=torch.float64)\n",
      "tensor([0.7771], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7771, 1.0648, 0.6016], dtype=torch.float64)\n",
      "tensor([0.7589], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7589, 1.0697, 0.5938], dtype=torch.float64)\n",
      "tensor([0.7658], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7658, 1.0573, 0.5859], dtype=torch.float64)\n",
      "tensor([0.7420], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7420, 1.1031, 0.5781], dtype=torch.float64)\n",
      "tensor([0.8256], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8256, 1.1125, 0.5703], dtype=torch.float64)\n",
      "tensor([0.8520], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8520, 1.1339, 0.5625], dtype=torch.float64)\n",
      "tensor([0.8755], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8755, 1.1718, 0.5547], dtype=torch.float64)\n",
      "tensor([0.9150], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9150, 1.2009, 0.5469], dtype=torch.float64)\n",
      "tensor([0.9473], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9473, 1.1830, 0.5391], dtype=torch.float64)\n",
      "tensor([0.9332], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9332, 1.1884, 0.5312], dtype=torch.float64)\n",
      "tensor([0.9374], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9374, 1.1526, 0.5234], dtype=torch.float64)\n",
      "tensor([0.9031], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9031, 1.1582, 0.5156], dtype=torch.float64)\n",
      "tensor([0.9054], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9054, 1.1450, 0.5078], dtype=torch.float64)\n",
      "tensor([0.8930], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8930, 1.1396, 0.5000], dtype=torch.float64)\n",
      "tensor([0.8868], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8868, 1.1449, 0.4922], dtype=torch.float64)\n",
      "tensor([0.8915], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8915, 1.1517, 0.4844], dtype=torch.float64)\n",
      "tensor([0.8987], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8987, 1.1361, 0.4766], dtype=torch.float64)\n",
      "tensor([0.8845], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8845, 1.1186, 0.4688], dtype=torch.float64)\n",
      "tensor([0.8663], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8663, 1.1405, 0.4609], dtype=torch.float64)\n",
      "tensor([0.8861], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8861, 1.1406, 0.4531], dtype=torch.float64)\n",
      "tensor([0.8882], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8882, 1.1524, 0.4453], dtype=torch.float64)\n",
      "tensor([0.9002], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9002, 1.1359, 0.4375], dtype=torch.float64)\n",
      "tensor([0.8855], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8855, 1.1572, 0.4297], dtype=torch.float64)\n",
      "tensor([0.9050], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9050, 1.1407, 0.4219], dtype=torch.float64)\n",
      "tensor([0.8911], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8911, 1.1551, 0.4141], dtype=torch.float64)\n",
      "tensor([0.9039], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9039, 1.1486, 0.4062], dtype=torch.float64)\n",
      "tensor([0.8990], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8990, 1.1250, 0.3984], dtype=torch.float64)\n",
      "tensor([0.8758], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8758, 1.1140, 0.3906], dtype=torch.float64)\n",
      "tensor([0.8630], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8630, 1.1187, 0.3828], dtype=torch.float64)\n",
      "tensor([0.8666], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8666, 1.1519, 0.3750], dtype=torch.float64)\n",
      "tensor([0.8994], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8994, 1.1247, 0.3672], dtype=torch.float64)\n",
      "tensor([0.8764], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8764, 1.1397, 0.3594], dtype=torch.float64)\n",
      "tensor([0.8889], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8889, 1.1457, 0.3516], dtype=torch.float64)\n",
      "tensor([0.8962], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8962, 1.1304, 0.3438], dtype=torch.float64)\n",
      "tensor([0.8822], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8822, 1.1106, 0.3359], dtype=torch.float64)\n",
      "tensor([0.8618], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8618, 1.1036, 0.3281], dtype=torch.float64)\n",
      "tensor([0.8532], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8532, 1.1136, 0.3203], dtype=torch.float64)\n",
      "tensor([0.8623], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8623, 1.1104, 0.3125], dtype=torch.float64)\n",
      "tensor([0.8603], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8603, 1.1410, 0.3047], dtype=torch.float64)\n",
      "tensor([0.8900], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8900, 1.1677, 0.2969], dtype=torch.float64)\n",
      "tensor([0.9190], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9190, 1.1913, 0.2891], dtype=torch.float64)\n",
      "tensor([0.9444], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9444, 1.1804, 0.2812], dtype=torch.float64)\n",
      "tensor([0.9355], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9355, 1.1979, 0.2734], dtype=torch.float64)\n",
      "tensor([0.9521], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9521, 1.2215, 0.2656], dtype=torch.float64)\n",
      "tensor([0.9763], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9763, 1.2035, 0.2578], dtype=torch.float64)\n",
      "tensor([0.9604], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9604, 1.2161, 0.2500], dtype=torch.float64)\n",
      "tensor([0.9718], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9718, 1.2452, 0.2422], dtype=torch.float64)\n",
      "tensor([1.0010], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0010, 1.2399, 0.2344], dtype=torch.float64)\n",
      "tensor([0.9979], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9979, 1.2133, 0.2266], dtype=torch.float64)\n",
      "tensor([0.9718], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9718, 1.1873, 0.2188], dtype=torch.float64)\n",
      "tensor([0.9448], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9448, 1.1810, 0.2109], dtype=torch.float64)\n",
      "tensor([0.9369], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9369, 1.1854, 0.2031], dtype=torch.float64)\n",
      "tensor([0.9408], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9408, 1.1912, 0.1953], dtype=torch.float64)\n",
      "tensor([0.9468], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9468, 1.1800, 0.1875], dtype=torch.float64)\n",
      "tensor([0.9364], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9364, 1.1934, 0.1797], dtype=torch.float64)\n",
      "tensor([0.9488], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9488, 1.2088, 0.1719], dtype=torch.float64)\n",
      "tensor([0.9648], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9648, 1.2162, 0.1641], dtype=torch.float64)\n",
      "tensor([0.9732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9732, 1.2377, 0.1562], dtype=torch.float64)\n",
      "tensor([0.9948], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9948, 1.2453, 0.1484], dtype=torch.float64)\n",
      "tensor([1.0037], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0037, 1.2377, 0.1406], dtype=torch.float64)\n",
      "tensor([0.9970], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9970, 1.2223, 0.1328], dtype=torch.float64)\n",
      "tensor([0.9816], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9816, 1.2218, 0.1250], dtype=torch.float64)\n",
      "tensor([0.9802], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9802, 1.1848, 0.1172], dtype=torch.float64)\n",
      "tensor([0.9441], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9441, 1.1864, 0.1094], dtype=torch.float64)\n",
      "tensor([0.9434], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9434, 1.1844, 0.1016], dtype=torch.float64)\n",
      "tensor([0.9415], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9415, 1.1814, 0.0938], dtype=torch.float64)\n",
      "tensor([0.9385], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9385, 1.1624, 0.0859], dtype=torch.float64)\n",
      "tensor([0.9199], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9199, 1.1480, 0.0781], dtype=torch.float64)\n",
      "tensor([0.9047], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9047, 1.1465, 0.0703], dtype=torch.float64)\n",
      "tensor([0.9023], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9023, 1.1454, 0.0625], dtype=torch.float64)\n",
      "tensor([0.9012], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9012, 1.1449, 0.0547], dtype=torch.float64)\n",
      "tensor([0.9006], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9006, 1.1542, 0.0469], dtype=torch.float64)\n",
      "tensor([0.9098], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9098, 1.1580, 0.0391], dtype=torch.float64)\n",
      "tensor([0.9143], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9143, 1.1728, 0.0312], dtype=torch.float64)\n",
      "tensor([0.9291], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9291, 1.1721, 0.0234], dtype=torch.float64)\n",
      "tensor([0.9294], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9294, 1.1706, 0.0156], dtype=torch.float64)\n",
      "tensor([0.9281], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9281, 1.1575, 0.0078], dtype=torch.float64)\n",
      "tensor([0.9153], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.1672801331643367e-06 6.672548834503553e-07\n",
      "loss: [14.56843971]\n",
      "episode:  501\n",
      "loss: [12.87975409]\n",
      "episode:  502\n",
      "loss: [17.84983054]\n",
      "episode:  503\n",
      "loss: [16.80128428]\n",
      "episode:  504\n",
      "loss: [16.04611452]\n",
      "episode:  505\n",
      "loss: [19.25985579]\n",
      "episode:  506\n",
      "loss: [13.9246604]\n",
      "episode:  507\n",
      "loss: [9.64250345]\n",
      "episode:  508\n",
      "loss: [13.92566146]\n",
      "episode:  509\n",
      "loss: [12.18751555]\n",
      "episode:  510\n",
      "loss: [12.10427831]\n",
      "episode:  511\n",
      "loss: [14.56047422]\n",
      "episode:  512\n",
      "loss: [16.46271812]\n",
      "episode:  513\n",
      "loss: [14.19746129]\n",
      "episode:  514\n",
      "loss: [12.66688835]\n",
      "episode:  515\n",
      "loss: [19.44287499]\n",
      "episode:  516\n",
      "loss: [10.04356619]\n",
      "episode:  517\n",
      "loss: [32.78932358]\n",
      "episode:  518\n",
      "loss: [16.15279792]\n",
      "episode:  519\n",
      "loss: [15.09275818]\n",
      "episode:  520\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3355], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3355, 1.0168, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5816], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5816, 1.0199, 0.9844], dtype=torch.float64)\n",
      "tensor([0.6318], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6318, 1.0362, 0.9766], dtype=torch.float64)\n",
      "tensor([0.6705], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6705, 1.0229, 0.9688], dtype=torch.float64)\n",
      "tensor([0.6517], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6517, 1.0301, 0.9609], dtype=torch.float64)\n",
      "tensor([0.6608], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6608, 1.0355, 0.9531], dtype=torch.float64)\n",
      "tensor([0.6716], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6716, 1.0434, 0.9453], dtype=torch.float64)\n",
      "tensor([0.6874], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6874, 1.0525, 0.9375], dtype=torch.float64)\n",
      "tensor([0.7063], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7063, 1.0864, 0.9297], dtype=torch.float64)\n",
      "tensor([0.7726], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7726, 1.1124, 0.9219], dtype=torch.float64)\n",
      "tensor([0.8329], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8329, 1.1067, 0.9141], dtype=torch.float64)\n",
      "tensor([0.8337], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8337, 1.1090, 0.9062], dtype=torch.float64)\n",
      "tensor([0.8382], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8382, 1.1004, 0.8984], dtype=torch.float64)\n",
      "tensor([0.8232], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8232, 1.0846, 0.8906], dtype=torch.float64)\n",
      "tensor([0.7913], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7913, 1.0809, 0.8828], dtype=torch.float64)\n",
      "tensor([0.7791], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7791, 1.0744, 0.8750], dtype=torch.float64)\n",
      "tensor([0.7646], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7646, 1.0743, 0.8672], dtype=torch.float64)\n",
      "tensor([0.7618], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7618, 1.0691, 0.8594], dtype=torch.float64)\n",
      "tensor([0.7508], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7508, 1.0692, 0.8516], dtype=torch.float64)\n",
      "tensor([0.7491], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7491, 1.0631, 0.8438], dtype=torch.float64)\n",
      "tensor([0.7367], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7367, 1.0499, 0.8359], dtype=torch.float64)\n",
      "tensor([0.7071], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7071, 1.0437, 0.8281], dtype=torch.float64)\n",
      "tensor([0.6887], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6887, 1.0388, 0.8203], dtype=torch.float64)\n",
      "tensor([0.6752], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6752, 1.0573, 0.8125], dtype=torch.float64)\n",
      "tensor([0.7112], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7112, 1.0458, 0.8047], dtype=torch.float64)\n",
      "tensor([0.6953], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6953, 1.0490, 0.7969], dtype=torch.float64)\n",
      "tensor([0.6989], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6989, 1.0614, 0.7891], dtype=torch.float64)\n",
      "tensor([0.7258], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7258, 1.0553, 0.7812], dtype=torch.float64)\n",
      "tensor([0.7191], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7191, 1.0472, 0.7734], dtype=torch.float64)\n",
      "tensor([0.7014], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7014, 1.0326, 0.7656], dtype=torch.float64)\n",
      "tensor([0.6679], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6679, 1.0600, 0.7578], dtype=torch.float64)\n",
      "tensor([0.7182], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7182, 1.0665, 0.7500], dtype=torch.float64)\n",
      "tensor([0.7425], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7425, 1.0590, 0.7422], dtype=torch.float64)\n",
      "tensor([0.7324], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7324, 1.0563, 0.7344], dtype=torch.float64)\n",
      "tensor([0.7253], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7253, 1.0601, 0.7266], dtype=torch.float64)\n",
      "tensor([0.7321], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7321, 1.0657, 0.7188], dtype=torch.float64)\n",
      "tensor([0.7455], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7455, 1.0339, 0.7109], dtype=torch.float64)\n",
      "tensor([0.6828], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6828, 1.0366, 0.7031], dtype=torch.float64)\n",
      "tensor([0.6758], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6758, 1.0429, 0.6953], dtype=torch.float64)\n",
      "tensor([0.6879], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6879, 1.0405, 0.6875], dtype=torch.float64)\n",
      "tensor([0.6858], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6858, 1.0199, 0.6797], dtype=torch.float64)\n",
      "tensor([0.6433], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6433, 1.0296, 0.6719], dtype=torch.float64)\n",
      "tensor([0.6549], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6549, 1.0129, 0.6641], dtype=torch.float64)\n",
      "tensor([0.6191], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6191, 1.0335, 0.6562], dtype=torch.float64)\n",
      "tensor([0.6589], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6589, 1.0416, 0.6484], dtype=torch.float64)\n",
      "tensor([0.6844], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6844, 1.0340, 0.6406], dtype=torch.float64)\n",
      "tensor([0.6742], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6742, 1.0249, 0.6328], dtype=torch.float64)\n",
      "tensor([0.6538], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6538, 1.0390, 0.6250], dtype=torch.float64)\n",
      "tensor([0.6792], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6792, 1.0295, 0.6172], dtype=torch.float64)\n",
      "tensor([0.6652], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6652, 1.0348, 0.6094], dtype=torch.float64)\n",
      "tensor([0.6737], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6737, 1.0417, 0.6016], dtype=torch.float64)\n",
      "tensor([0.6902], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6902, 1.0484, 0.5938], dtype=torch.float64)\n",
      "tensor([0.7080], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7080, 1.0655, 0.5859], dtype=torch.float64)\n",
      "tensor([0.7475], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7475, 1.0552, 0.5781], dtype=torch.float64)\n",
      "tensor([0.7347], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7347, 1.0596, 0.5703], dtype=torch.float64)\n",
      "tensor([0.7415], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7415, 1.0701, 0.5625], dtype=torch.float64)\n",
      "tensor([0.7652], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7652, 1.0806, 0.5547], dtype=torch.float64)\n",
      "tensor([0.7898], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7898, 1.0913, 0.5469], dtype=torch.float64)\n",
      "tensor([0.8146], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8146, 1.0968, 0.5391], dtype=torch.float64)\n",
      "tensor([0.8298], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8298, 1.0984, 0.5312], dtype=torch.float64)\n",
      "tensor([0.8358], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8358, 1.0914, 0.5234], dtype=torch.float64)\n",
      "tensor([0.8244], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8244, 1.0937, 0.5156], dtype=torch.float64)\n",
      "tensor([0.8270], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8270, 1.0812, 0.5078], dtype=torch.float64)\n",
      "tensor([0.8047], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8047, 1.0798, 0.5000], dtype=torch.float64)\n",
      "tensor([0.7985], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7985, 1.0720, 0.4922], dtype=torch.float64)\n",
      "tensor([0.7832], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7832, 1.0648, 0.4844], dtype=torch.float64)\n",
      "tensor([0.7672], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7672, 1.0606, 0.4766], dtype=torch.float64)\n",
      "tensor([0.7556], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7556, 1.0665, 0.4688], dtype=torch.float64)\n",
      "tensor([0.7659], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7659, 1.0749, 0.4609], dtype=torch.float64)\n",
      "tensor([0.7842], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7842, 1.0706, 0.4531], dtype=torch.float64)\n",
      "tensor([0.7799], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7799, 1.0360, 0.4453], dtype=torch.float64)\n",
      "tensor([0.7090], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7090, 1.0252, 0.4375], dtype=torch.float64)\n",
      "tensor([0.6688], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6688, 1.0081, 0.4297], dtype=torch.float64)\n",
      "tensor([0.6072], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6072, 1.0255, 0.4219], dtype=torch.float64)\n",
      "tensor([0.6453], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6453, 1.0039, 0.4141], dtype=torch.float64)\n",
      "tensor([0.5886], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5886, 0.9925, 0.4062], dtype=torch.float64)\n",
      "tensor([0.5404], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5404, 0.9978, 0.3984], dtype=torch.float64)\n",
      "tensor([0.5449], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5449, 0.9898, 0.3906], dtype=torch.float64)\n",
      "tensor([0.5215], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5215, 0.9991, 0.3828], dtype=torch.float64)\n",
      "tensor([0.5443], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5443, 0.9969, 0.3750], dtype=torch.float64)\n",
      "tensor([0.5431], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5431, 0.9906, 0.3672], dtype=torch.float64)\n",
      "tensor([0.5233], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5233, 1.0047, 0.3594], dtype=torch.float64)\n",
      "tensor([0.5617], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5617, 0.9961, 0.3516], dtype=torch.float64)\n",
      "tensor([0.5447], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5447, 0.9903, 0.3438], dtype=torch.float64)\n",
      "tensor([0.5206], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5206, 1.0072, 0.3359], dtype=torch.float64)\n",
      "tensor([0.5667], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5667, 1.0068, 0.3281], dtype=torch.float64)\n",
      "tensor([0.5785], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5785, 1.0051, 0.3203], dtype=torch.float64)\n",
      "tensor([0.5760], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5760, 1.0019, 0.3125], dtype=torch.float64)\n",
      "tensor([0.5644], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5644, 0.9987, 0.3047], dtype=torch.float64)\n",
      "tensor([0.5497], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5497, 0.9957, 0.2969], dtype=torch.float64)\n",
      "tensor([0.5350], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5350, 0.9814, 0.2891], dtype=torch.float64)\n",
      "tensor([0.4834], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4834, 0.9975, 0.2812], dtype=torch.float64)\n",
      "tensor([0.5191], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5191, 1.0052, 0.2734], dtype=torch.float64)\n",
      "tensor([0.5540], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5540, 0.9943, 0.2656], dtype=torch.float64)\n",
      "tensor([0.5289], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5289, 1.0041, 0.2578], dtype=torch.float64)\n",
      "tensor([0.5520], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5520, 0.9916, 0.2500], dtype=torch.float64)\n",
      "tensor([0.5180], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5180, 0.9920, 0.2422], dtype=torch.float64)\n",
      "tensor([0.5080], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5080, 0.9895, 0.2344], dtype=torch.float64)\n",
      "tensor([0.4961], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4961, 1.0012, 0.2266], dtype=torch.float64)\n",
      "tensor([0.5297], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5297, 1.0183, 0.2188], dtype=torch.float64)\n",
      "tensor([0.5940], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5940, 1.0104, 0.2109], dtype=torch.float64)\n",
      "tensor([0.5876], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5876, 1.0327, 0.2031], dtype=torch.float64)\n",
      "tensor([0.6562], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6562, 1.0427, 0.1953], dtype=torch.float64)\n",
      "tensor([0.7086], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7086, 1.0404, 0.1875], dtype=torch.float64)\n",
      "tensor([0.7142], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7142, 1.0382, 0.1797], dtype=torch.float64)\n",
      "tensor([0.7090], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7090, 1.0534, 0.1719], dtype=torch.float64)\n",
      "tensor([0.7457], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7457, 1.0420, 0.1641], dtype=torch.float64)\n",
      "tensor([0.7282], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7282, 1.0368, 0.1562], dtype=torch.float64)\n",
      "tensor([0.7080], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7080, 1.0246, 0.1484], dtype=torch.float64)\n",
      "tensor([0.6618], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6618, 1.0275, 0.1406], dtype=torch.float64)\n",
      "tensor([0.6564], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6564, 1.0322, 0.1328], dtype=torch.float64)\n",
      "tensor([0.6692], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6692, 1.0161, 0.1250], dtype=torch.float64)\n",
      "tensor([0.6207], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6207, 1.0430, 0.1172], dtype=torch.float64)\n",
      "tensor([0.6914], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6914, 1.0500, 0.1094], dtype=torch.float64)\n",
      "tensor([0.7348], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7348, 1.0516, 0.1016], dtype=torch.float64)\n",
      "tensor([0.7494], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7494, 1.0373, 0.0938], dtype=torch.float64)\n",
      "tensor([0.7102], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7102, 1.0413, 0.0859], dtype=torch.float64)\n",
      "tensor([0.7103], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7103, 1.0270, 0.0781], dtype=torch.float64)\n",
      "tensor([0.6637], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6637, 1.0275, 0.0703], dtype=torch.float64)\n",
      "tensor([0.6502], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6502, 1.0133, 0.0625], dtype=torch.float64)\n",
      "tensor([0.6000], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6000, 1.0080, 0.0547], dtype=torch.float64)\n",
      "tensor([0.5668], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5668, 0.9988, 0.0469], dtype=torch.float64)\n",
      "tensor([0.5265], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5265, 1.0143, 0.0391], dtype=torch.float64)\n",
      "tensor([0.5630], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5630, 1.0170, 0.0312], dtype=torch.float64)\n",
      "tensor([0.5824], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5824, 1.0169, 0.0234], dtype=torch.float64)\n",
      "tensor([0.5873], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5873, 1.0202, 0.0156], dtype=torch.float64)\n",
      "tensor([0.5983], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5983, 1.0003, 0.0078], dtype=torch.float64)\n",
      "tensor([0.5374], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.0379932397602214e-06 7.11961365604363e-07\n",
      "loss: [12.77567581]\n",
      "episode:  521\n",
      "loss: [12.30193212]\n",
      "episode:  522\n",
      "loss: [11.47337589]\n",
      "episode:  523\n",
      "loss: [13.64472311]\n",
      "episode:  524\n",
      "loss: [14.25558819]\n",
      "episode:  525\n",
      "loss: [12.3754734]\n",
      "episode:  526\n",
      "loss: [15.32737043]\n",
      "episode:  527\n",
      "loss: [13.88140646]\n",
      "episode:  528\n",
      "loss: [13.13115586]\n",
      "episode:  529\n",
      "loss: [17.49736761]\n",
      "episode:  530\n",
      "loss: [14.42244576]\n",
      "episode:  531\n",
      "loss: [21.13923337]\n",
      "episode:  532\n",
      "loss: [15.79197954]\n",
      "episode:  533\n",
      "loss: [16.69736579]\n",
      "episode:  534\n",
      "loss: [14.05020571]\n",
      "episode:  535\n",
      "loss: [14.01104787]\n",
      "episode:  536\n",
      "loss: [15.52747527]\n",
      "episode:  537\n",
      "loss: [16.25520327]\n",
      "episode:  538\n",
      "loss: [16.26834162]\n",
      "episode:  539\n",
      "loss: [15.13174635]\n",
      "episode:  540\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3356], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3356, 1.0172, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5824], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5824, 1.0263, 0.9844], dtype=torch.float64)\n",
      "tensor([0.6438], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6438, 1.0250, 0.9766], dtype=torch.float64)\n",
      "tensor([0.6517], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6517, 1.0230, 0.9688], dtype=torch.float64)\n",
      "tensor([0.6485], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6485, 1.0306, 0.9609], dtype=torch.float64)\n",
      "tensor([0.6612], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6612, 1.0217, 0.9531], dtype=torch.float64)\n",
      "tensor([0.6460], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6460, 1.0530, 0.9453], dtype=torch.float64)\n",
      "tensor([0.7007], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7007, 1.0521, 0.9375], dtype=torch.float64)\n",
      "tensor([0.7082], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7082, 1.0699, 0.9297], dtype=torch.float64)\n",
      "tensor([0.7420], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7420, 1.0729, 0.9219], dtype=torch.float64)\n",
      "tensor([0.7531], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7531, 1.0943, 0.9141], dtype=torch.float64)\n",
      "tensor([0.7957], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7957, 1.0959, 0.9062], dtype=torch.float64)\n",
      "tensor([0.8067], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8067, 1.0773, 0.8984], dtype=torch.float64)\n",
      "tensor([0.7745], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7745, 1.0734, 0.8906], dtype=torch.float64)\n",
      "tensor([0.7606], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7606, 1.0827, 0.8828], dtype=torch.float64)\n",
      "tensor([0.7767], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7767, 1.1125, 0.8750], dtype=torch.float64)\n",
      "tensor([0.8352], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8352, 1.1279, 0.8672], dtype=torch.float64)\n",
      "tensor([0.8605], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8605, 1.1208, 0.8594], dtype=torch.float64)\n",
      "tensor([0.8563], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8563, 1.1351, 0.8516], dtype=torch.float64)\n",
      "tensor([0.8700], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8700, 1.1349, 0.8438], dtype=torch.float64)\n",
      "tensor([0.8713], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8713, 1.1122, 0.8359], dtype=torch.float64)\n",
      "tensor([0.8497], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8497, 1.1226, 0.8281], dtype=torch.float64)\n",
      "tensor([0.8579], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8579, 1.1177, 0.8203], dtype=torch.float64)\n",
      "tensor([0.8541], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8541, 1.1079, 0.8125], dtype=torch.float64)\n",
      "tensor([0.8441], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8441, 1.0985, 0.8047], dtype=torch.float64)\n",
      "tensor([0.8251], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8251, 1.0873, 0.7969], dtype=torch.float64)\n",
      "tensor([0.8014], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8014, 1.0659, 0.7891], dtype=torch.float64)\n",
      "tensor([0.7563], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7563, 1.0633, 0.7812], dtype=torch.float64)\n",
      "tensor([0.7421], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7421, 1.0803, 0.7734], dtype=torch.float64)\n",
      "tensor([0.7743], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7743, 1.0797, 0.7656], dtype=torch.float64)\n",
      "tensor([0.7796], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7796, 1.0548, 0.7578], dtype=torch.float64)\n",
      "tensor([0.7306], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7306, 1.0627, 0.7500], dtype=torch.float64)\n",
      "tensor([0.7373], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7373, 1.0468, 0.7422], dtype=torch.float64)\n",
      "tensor([0.7062], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7062, 1.0322, 0.7344], dtype=torch.float64)\n",
      "tensor([0.6699], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6699, 1.0403, 0.7266], dtype=torch.float64)\n",
      "tensor([0.6797], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6797, 1.0296, 0.7188], dtype=torch.float64)\n",
      "tensor([0.6599], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6599, 1.0477, 0.7109], dtype=torch.float64)\n",
      "tensor([0.6937], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6937, 1.0455, 0.7031], dtype=torch.float64)\n",
      "tensor([0.6967], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6967, 1.0778, 0.6953], dtype=torch.float64)\n",
      "tensor([0.7647], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7647, 1.0697, 0.6875], dtype=torch.float64)\n",
      "tensor([0.7622], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7622, 1.0917, 0.6797], dtype=torch.float64)\n",
      "tensor([0.8038], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8038, 1.0888, 0.6719], dtype=torch.float64)\n",
      "tensor([0.8064], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8064, 1.0854, 0.6641], dtype=torch.float64)\n",
      "tensor([0.8010], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8010, 1.0932, 0.6562], dtype=torch.float64)\n",
      "tensor([0.8149], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8149, 1.0888, 0.6484], dtype=torch.float64)\n",
      "tensor([0.8096], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8096, 1.1049, 0.6406], dtype=torch.float64)\n",
      "tensor([0.8389], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8389, 1.1024, 0.6328], dtype=torch.float64)\n",
      "tensor([0.8401], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8401, 1.1084, 0.6250], dtype=torch.float64)\n",
      "tensor([0.8484], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8484, 1.1231, 0.6172], dtype=torch.float64)\n",
      "tensor([0.8637], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8637, 1.1233, 0.6094], dtype=torch.float64)\n",
      "tensor([0.8656], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8656, 1.1256, 0.6016], dtype=torch.float64)\n",
      "tensor([0.8683], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8683, 1.1314, 0.5938], dtype=torch.float64)\n",
      "tensor([0.8744], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8744, 1.1148, 0.5859], dtype=torch.float64)\n",
      "tensor([0.8590], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8590, 1.1120, 0.5781], dtype=torch.float64)\n",
      "tensor([0.8550], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8550, 1.1417, 0.5703], dtype=torch.float64)\n",
      "tensor([0.8837], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8837, 1.1605, 0.5625], dtype=torch.float64)\n",
      "tensor([0.9050], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9050, 1.1186, 0.5547], dtype=torch.float64)\n",
      "tensor([0.8665], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8665, 1.1300, 0.5469], dtype=torch.float64)\n",
      "tensor([0.8740], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8740, 1.1555, 0.5391], dtype=torch.float64)\n",
      "tensor([0.8998], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8998, 1.1468, 0.5312], dtype=torch.float64)\n",
      "tensor([0.8941], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8941, 1.1316, 0.5234], dtype=torch.float64)\n",
      "tensor([0.8789], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8789, 1.1561, 0.5156], dtype=torch.float64)\n",
      "tensor([0.9014], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9014, 1.1439, 0.5078], dtype=torch.float64)\n",
      "tensor([0.8920], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8920, 1.1262, 0.5000], dtype=torch.float64)\n",
      "tensor([0.8741], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8741, 1.1347, 0.4922], dtype=torch.float64)\n",
      "tensor([0.8808], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8808, 1.1634, 0.4844], dtype=torch.float64)\n",
      "tensor([0.9095], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9095, 1.1760, 0.4766], dtype=torch.float64)\n",
      "tensor([0.9249], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9249, 1.1778, 0.4688], dtype=torch.float64)\n",
      "tensor([0.9283], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9283, 1.1517, 0.4609], dtype=torch.float64)\n",
      "tensor([0.9035], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9035, 1.1396, 0.4531], dtype=torch.float64)\n",
      "tensor([0.8894], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8894, 1.1361, 0.4453], dtype=torch.float64)\n",
      "tensor([0.8848], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8848, 1.1316, 0.4375], dtype=torch.float64)\n",
      "tensor([0.8802], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8802, 1.1349, 0.4297], dtype=torch.float64)\n",
      "tensor([0.8832], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8832, 1.1348, 0.4219], dtype=torch.float64)\n",
      "tensor([0.8836], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8836, 1.1553, 0.4141], dtype=torch.float64)\n",
      "tensor([0.9038], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9038, 1.1609, 0.4062], dtype=torch.float64)\n",
      "tensor([0.9114], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9114, 1.1578, 0.3984], dtype=torch.float64)\n",
      "tensor([0.9094], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9094, 1.1882, 0.3906], dtype=torch.float64)\n",
      "tensor([0.9389], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9389, 1.1751, 0.3828], dtype=torch.float64)\n",
      "tensor([0.9292], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9292, 1.1416, 0.3750], dtype=torch.float64)\n",
      "tensor([0.8960], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8960, 1.1343, 0.3672], dtype=torch.float64)\n",
      "tensor([0.8858], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8858, 1.1378, 0.3594], dtype=torch.float64)\n",
      "tensor([0.8884], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8884, 1.1499, 0.3516], dtype=torch.float64)\n",
      "tensor([0.9006], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9006, 1.1318, 0.3438], dtype=torch.float64)\n",
      "tensor([0.8844], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8844, 1.1231, 0.3359], dtype=torch.float64)\n",
      "tensor([0.8746], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8746, 1.1499, 0.3281], dtype=torch.float64)\n",
      "tensor([0.8999], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8999, 1.1162, 0.3203], dtype=torch.float64)\n",
      "tensor([0.8698], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8698, 1.1403, 0.3125], dtype=torch.float64)\n",
      "tensor([0.8905], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8905, 1.1458, 0.3047], dtype=torch.float64)\n",
      "tensor([0.8981], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8981, 1.1639, 0.2969], dtype=torch.float64)\n",
      "tensor([0.9167], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9167, 1.1895, 0.2891], dtype=torch.float64)\n",
      "tensor([0.9430], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9430, 1.1880, 0.2812], dtype=torch.float64)\n",
      "tensor([0.9433], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9433, 1.1815, 0.2734], dtype=torch.float64)\n",
      "tensor([0.9371], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9371, 1.1879, 0.2656], dtype=torch.float64)\n",
      "tensor([0.9430], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9430, 1.1787, 0.2578], dtype=torch.float64)\n",
      "tensor([0.9345], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9345, 1.1605, 0.2500], dtype=torch.float64)\n",
      "tensor([0.9163], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9163, 1.1644, 0.2422], dtype=torch.float64)\n",
      "tensor([0.9190], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9190, 1.1878, 0.2344], dtype=torch.float64)\n",
      "tensor([0.9421], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9421, 1.2032, 0.2266], dtype=torch.float64)\n",
      "tensor([0.9587], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9587, 1.1934, 0.2188], dtype=torch.float64)\n",
      "tensor([0.9504], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9504, 1.1925, 0.2109], dtype=torch.float64)\n",
      "tensor([0.9490], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9490, 1.2116, 0.2031], dtype=torch.float64)\n",
      "tensor([0.9677], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9677, 1.2139, 0.1953], dtype=torch.float64)\n",
      "tensor([0.9712], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9712, 1.2121, 0.1875], dtype=torch.float64)\n",
      "tensor([0.9699], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9699, 1.2203, 0.1797], dtype=torch.float64)\n",
      "tensor([0.9778], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9778, 1.2326, 0.1719], dtype=torch.float64)\n",
      "tensor([0.9904], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9904, 1.2308, 0.1641], dtype=torch.float64)\n",
      "tensor([0.9896], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9896, 1.1986, 0.1562], dtype=torch.float64)\n",
      "tensor([0.9582], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9582, 1.2044, 0.1484], dtype=torch.float64)\n",
      "tensor([0.9619], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9619, 1.1840, 0.1406], dtype=torch.float64)\n",
      "tensor([0.9423], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9423, 1.1938, 0.1328], dtype=torch.float64)\n",
      "tensor([0.9507], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9507, 1.2111, 0.1250], dtype=torch.float64)\n",
      "tensor([0.9682], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9682, 1.2123, 0.1172], dtype=torch.float64)\n",
      "tensor([0.9706], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9706, 1.2242, 0.1094], dtype=torch.float64)\n",
      "tensor([0.9825], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9825, 1.2373, 0.1016], dtype=torch.float64)\n",
      "tensor([0.9962], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9962, 1.2368, 0.0938], dtype=torch.float64)\n",
      "tensor([0.9966], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9966, 1.2440, 0.0859], dtype=torch.float64)\n",
      "tensor([1.0038], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0038, 1.2151, 0.0781], dtype=torch.float64)\n",
      "tensor([0.9761], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9761, 1.2268, 0.0703], dtype=torch.float64)\n",
      "tensor([0.9859], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9859, 1.2247, 0.0625], dtype=torch.float64)\n",
      "tensor([0.9846], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9846, 1.2271, 0.0547], dtype=torch.float64)\n",
      "tensor([0.9869], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9869, 1.2262, 0.0469], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9863], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9863, 1.2491, 0.0391], dtype=torch.float64)\n",
      "tensor([1.0087], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0087, 1.2292, 0.0312], dtype=torch.float64)\n",
      "tensor([0.9908], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9908, 1.2286, 0.0234], dtype=torch.float64)\n",
      "tensor([0.9891], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9891, 1.2137, 0.0156], dtype=torch.float64)\n",
      "tensor([0.9746], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9746, 1.2093, 0.0078], dtype=torch.float64)\n",
      "tensor([0.9694], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.2825378366395432e-06 7.999931626317338e-07\n",
      "loss: [16.61998542]\n",
      "episode:  541\n",
      "loss: [13.05256797]\n",
      "episode:  542\n",
      "loss: [11.39977289]\n",
      "episode:  543\n",
      "loss: [15.31322427]\n",
      "episode:  544\n",
      "loss: [14.56775378]\n",
      "episode:  545\n",
      "loss: [16.11253044]\n",
      "episode:  546\n",
      "loss: [14.1209702]\n",
      "episode:  547\n",
      "loss: [12.18604123]\n",
      "episode:  548\n",
      "loss: [13.88216062]\n",
      "episode:  549\n",
      "loss: [13.78458372]\n",
      "episode:  550\n",
      "loss: [17.02374767]\n",
      "episode:  551\n",
      "loss: [10.36767904]\n",
      "episode:  552\n",
      "loss: [15.67745005]\n",
      "episode:  553\n",
      "loss: [14.00472644]\n",
      "episode:  554\n",
      "loss: [11.79205634]\n",
      "episode:  555\n",
      "loss: [13.10422494]\n",
      "episode:  556\n",
      "loss: [13.85310195]\n",
      "episode:  557\n",
      "loss: [13.22119078]\n",
      "episode:  558\n",
      "loss: [13.67790811]\n",
      "episode:  559\n",
      "loss: [16.84296602]\n",
      "episode:  560\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3356], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3356, 1.0276, 0.9922], dtype=torch.float64)\n",
      "tensor([0.6018], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6018, 1.0503, 0.9844], dtype=torch.float64)\n",
      "tensor([0.6923], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6923, 1.0648, 0.9766], dtype=torch.float64)\n",
      "tensor([0.7359], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7359, 1.0671, 0.9688], dtype=torch.float64)\n",
      "tensor([0.7475], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7475, 1.0723, 0.9609], dtype=torch.float64)\n",
      "tensor([0.7584], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7584, 1.0622, 0.9531], dtype=torch.float64)\n",
      "tensor([0.7402], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7402, 1.0464, 0.9453], dtype=torch.float64)\n",
      "tensor([0.7056], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7056, 1.0463, 0.9375], dtype=torch.float64)\n",
      "tensor([0.6981], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6981, 1.0431, 0.9297], dtype=torch.float64)\n",
      "tensor([0.6899], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6899, 1.0627, 0.9219], dtype=torch.float64)\n",
      "tensor([0.7240], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7240, 1.0695, 0.9141], dtype=torch.float64)\n",
      "tensor([0.7421], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7421, 1.0612, 0.9062], dtype=torch.float64)\n",
      "tensor([0.7290], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7290, 1.0839, 0.8984], dtype=torch.float64)\n",
      "tensor([0.7723], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7723, 1.1094, 0.8906], dtype=torch.float64)\n",
      "tensor([0.8279], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8279, 1.0842, 0.8828], dtype=torch.float64)\n",
      "tensor([0.7919], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7919, 1.0734, 0.8750], dtype=torch.float64)\n",
      "tensor([0.7651], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7651, 1.0801, 0.8672], dtype=torch.float64)\n",
      "tensor([0.7736], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7736, 1.0506, 0.8594], dtype=torch.float64)\n",
      "tensor([0.7149], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7149, 1.0359, 0.8516], dtype=torch.float64)\n",
      "tensor([0.6729], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6729, 1.0364, 0.8438], dtype=torch.float64)\n",
      "tensor([0.6657], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6657, 1.0171, 0.8359], dtype=torch.float64)\n",
      "tensor([0.6246], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6246, 1.0328, 0.8281], dtype=torch.float64)\n",
      "tensor([0.6492], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6492, 1.0401, 0.8203], dtype=torch.float64)\n",
      "tensor([0.6697], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6697, 1.0462, 0.8125], dtype=torch.float64)\n",
      "tensor([0.6871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6871, 1.0350, 0.8047], dtype=torch.float64)\n",
      "tensor([0.6679], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6679, 1.0344, 0.7969], dtype=torch.float64)\n",
      "tensor([0.6632], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6632, 1.0333, 0.7891], dtype=torch.float64)\n",
      "tensor([0.6602], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6602, 1.0439, 0.7812], dtype=torch.float64)\n",
      "tensor([0.6821], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6821, 1.0342, 0.7734], dtype=torch.float64)\n",
      "tensor([0.6669], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6669, 1.0095, 0.7656], dtype=torch.float64)\n",
      "tensor([0.6121], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6121, 1.0127, 0.7578], dtype=torch.float64)\n",
      "tensor([0.6084], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6084, 1.0330, 0.7500], dtype=torch.float64)\n",
      "tensor([0.6506], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6506, 1.0203, 0.7422], dtype=torch.float64)\n",
      "tensor([0.6334], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6334, 1.0240, 0.7344], dtype=torch.float64)\n",
      "tensor([0.6380], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6380, 1.0205, 0.7266], dtype=torch.float64)\n",
      "tensor([0.6321], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6321, 1.0026, 0.7188], dtype=torch.float64)\n",
      "tensor([0.5826], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5826, 1.0263, 0.7109], dtype=torch.float64)\n",
      "tensor([0.6335], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6335, 1.0158, 0.7031], dtype=torch.float64)\n",
      "tensor([0.6228], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6228, 1.0117, 0.6953], dtype=torch.float64)\n",
      "tensor([0.6077], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6077, 1.0287, 0.6875], dtype=torch.float64)\n",
      "tensor([0.6450], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6450, 1.0405, 0.6797], dtype=torch.float64)\n",
      "tensor([0.6775], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6775, 1.0537, 0.6719], dtype=torch.float64)\n",
      "tensor([0.7121], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7121, 1.0556, 0.6641], dtype=torch.float64)\n",
      "tensor([0.7236], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7236, 1.0640, 0.6562], dtype=torch.float64)\n",
      "tensor([0.7439], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7439, 1.0533, 0.6484], dtype=torch.float64)\n",
      "tensor([0.7262], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7262, 1.0624, 0.6406], dtype=torch.float64)\n",
      "tensor([0.7419], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7419, 1.0503, 0.6328], dtype=torch.float64)\n",
      "tensor([0.7205], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7205, 1.0572, 0.6250], dtype=torch.float64)\n",
      "tensor([0.7308], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7308, 1.0455, 0.6172], dtype=torch.float64)\n",
      "tensor([0.7091], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7091, 1.0330, 0.6094], dtype=torch.float64)\n",
      "tensor([0.6791], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6791, 1.0484, 0.6016], dtype=torch.float64)\n",
      "tensor([0.7053], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7053, 1.0613, 0.5938], dtype=torch.float64)\n",
      "tensor([0.7378], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7378, 1.0723, 0.5859], dtype=torch.float64)\n",
      "tensor([0.7679], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7679, 1.0997, 0.5781], dtype=torch.float64)\n",
      "tensor([0.8248], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8248, 1.0907, 0.5703], dtype=torch.float64)\n",
      "tensor([0.8188], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8188, 1.0896, 0.5625], dtype=torch.float64)\n",
      "tensor([0.8162], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8162, 1.0992, 0.5547], dtype=torch.float64)\n",
      "tensor([0.8339], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8339, 1.0964, 0.5469], dtype=torch.float64)\n",
      "tensor([0.8322], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8322, 1.1153, 0.5391], dtype=torch.float64)\n",
      "tensor([0.8566], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8566, 1.1252, 0.5312], dtype=torch.float64)\n",
      "tensor([0.8688], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8688, 1.1397, 0.5234], dtype=torch.float64)\n",
      "tensor([0.8844], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8844, 1.1526, 0.5156], dtype=torch.float64)\n",
      "tensor([0.8986], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8986, 1.1632, 0.5078], dtype=torch.float64)\n",
      "tensor([0.9106], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9106, 1.1692, 0.5000], dtype=torch.float64)\n",
      "tensor([0.9177], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9177, 1.1891, 0.4922], dtype=torch.float64)\n",
      "tensor([0.9381], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9381, 1.1850, 0.4844], dtype=torch.float64)\n",
      "tensor([0.9363], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9363, 1.1929, 0.4766], dtype=torch.float64)\n",
      "tensor([0.9440], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9440, 1.1983, 0.4688], dtype=torch.float64)\n",
      "tensor([0.9502], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9502, 1.1871, 0.4609], dtype=torch.float64)\n",
      "tensor([0.9400], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9400, 1.1933, 0.4531], dtype=torch.float64)\n",
      "tensor([0.9453], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9453, 1.1821, 0.4453], dtype=torch.float64)\n",
      "tensor([0.9352], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9352, 1.1480, 0.4375], dtype=torch.float64)\n",
      "tensor([0.9012], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9012, 1.1451, 0.4297], dtype=torch.float64)\n",
      "tensor([0.8952], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8952, 1.1414, 0.4219], dtype=torch.float64)\n",
      "tensor([0.8912], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8912, 1.1290, 0.4141], dtype=torch.float64)\n",
      "tensor([0.8790], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8790, 1.1117, 0.4062], dtype=torch.float64)\n",
      "tensor([0.8612], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8612, 1.0989, 0.3984], dtype=torch.float64)\n",
      "tensor([0.8472], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8472, 1.0720, 0.3906], dtype=torch.float64)\n",
      "tensor([0.7974], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7974, 1.1042, 0.3828], dtype=torch.float64)\n",
      "tensor([0.8465], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8465, 1.0950, 0.3750], dtype=torch.float64)\n",
      "tensor([0.8406], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8406, 1.0882, 0.3672], dtype=torch.float64)\n",
      "tensor([0.8273], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8273, 1.1094, 0.3594], dtype=torch.float64)\n",
      "tensor([0.8551], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8551, 1.1110, 0.3516], dtype=torch.float64)\n",
      "tensor([0.8596], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8596, 1.1171, 0.3438], dtype=torch.float64)\n",
      "tensor([0.8662], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8662, 1.1123, 0.3359], dtype=torch.float64)\n",
      "tensor([0.8623], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8623, 1.0917, 0.3281], dtype=torch.float64)\n",
      "tensor([0.8398], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8398, 1.1144, 0.3203], dtype=torch.float64)\n",
      "tensor([0.8622], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8622, 1.0922, 0.3125], dtype=torch.float64)\n",
      "tensor([0.8414], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8414, 1.1139, 0.3047], dtype=torch.float64)\n",
      "tensor([0.8623], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8623, 1.0820, 0.2969], dtype=torch.float64)\n",
      "tensor([0.8232], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8232, 1.0527, 0.2891], dtype=torch.float64)\n",
      "tensor([0.7614], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7614, 1.0581, 0.2812], dtype=torch.float64)\n",
      "tensor([0.7601], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7601, 1.0483, 0.2734], dtype=torch.float64)\n",
      "tensor([0.7400], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7400, 1.0405, 0.2656], dtype=torch.float64)\n",
      "tensor([0.7202], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7202, 1.0356, 0.2578], dtype=torch.float64)\n",
      "tensor([0.7029], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7029, 1.0384, 0.2500], dtype=torch.float64)\n",
      "tensor([0.7072], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7072, 1.0412, 0.2422], dtype=torch.float64)\n",
      "tensor([0.7162], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7162, 1.0394, 0.2344], dtype=torch.float64)\n",
      "tensor([0.7134], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7134, 1.0342, 0.2266], dtype=torch.float64)\n",
      "tensor([0.6969], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6969, 1.0418, 0.2188], dtype=torch.float64)\n",
      "tensor([0.7161], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7161, 1.0372, 0.2109], dtype=torch.float64)\n",
      "tensor([0.7067], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7067, 1.0375, 0.2031], dtype=torch.float64)\n",
      "tensor([0.7053], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7053, 1.0542, 0.1953], dtype=torch.float64)\n",
      "tensor([0.7453], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7453, 1.0545, 0.1875], dtype=torch.float64)\n",
      "tensor([0.7546], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7546, 1.0477, 0.1797], dtype=torch.float64)\n",
      "tensor([0.7430], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7430, 1.0483, 0.1719], dtype=torch.float64)\n",
      "tensor([0.7421], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7421, 1.0302, 0.1641], dtype=torch.float64)\n",
      "tensor([0.6917], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6917, 1.0157, 0.1562], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6294], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6294, 1.0230, 0.1484], dtype=torch.float64)\n",
      "tensor([0.6331], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6331, 1.0286, 0.1406], dtype=torch.float64)\n",
      "tensor([0.6513], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6513, 1.0459, 0.1328], dtype=torch.float64)\n",
      "tensor([0.7117], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7117, 1.0038, 0.1250], dtype=torch.float64)\n",
      "tensor([0.5943], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5943, 0.9979, 0.1172], dtype=torch.float64)\n",
      "tensor([0.5389], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5389, 0.9911, 0.1094], dtype=torch.float64)\n",
      "tensor([0.4990], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4990, 0.9871, 0.1016], dtype=torch.float64)\n",
      "tensor([0.4730], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4730, 0.9861, 0.0938], dtype=torch.float64)\n",
      "tensor([0.4608], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4608, 0.9804, 0.0859], dtype=torch.float64)\n",
      "tensor([0.4379], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4379, 0.9984, 0.0781], dtype=torch.float64)\n",
      "tensor([0.4886], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4886, 1.0263, 0.0703], dtype=torch.float64)\n",
      "tensor([0.5932], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5932, 1.0285, 0.0625], dtype=torch.float64)\n",
      "tensor([0.6314], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6314, 1.0465, 0.0547], dtype=torch.float64)\n",
      "tensor([0.7001], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7001, 1.0403, 0.0469], dtype=torch.float64)\n",
      "tensor([0.7006], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7006, 1.0461, 0.0391], dtype=torch.float64)\n",
      "tensor([0.7185], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7185, 1.0318, 0.0312], dtype=torch.float64)\n",
      "tensor([0.6776], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6776, 1.0081, 0.0234], dtype=torch.float64)\n",
      "tensor([0.5883], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5883, 1.0157, 0.0156], dtype=torch.float64)\n",
      "tensor([0.5846], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5846, 1.0160, 0.0078], dtype=torch.float64)\n",
      "tensor([0.5836], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.2056793133030061e-06 6.849048009239764e-07\n",
      "loss: [14.00955903]\n",
      "episode:  561\n",
      "loss: [11.88521904]\n",
      "episode:  562\n",
      "loss: [16.66918704]\n",
      "episode:  563\n",
      "loss: [15.58881334]\n",
      "episode:  564\n",
      "loss: [16.80105353]\n",
      "episode:  565\n",
      "loss: [14.70829382]\n",
      "episode:  566\n",
      "loss: [14.07372023]\n",
      "episode:  567\n",
      "loss: [14.10410217]\n",
      "episode:  568\n",
      "loss: [14.12744401]\n",
      "episode:  569\n",
      "loss: [13.33482769]\n",
      "episode:  570\n",
      "loss: [14.41142372]\n",
      "episode:  571\n",
      "loss: [11.89731232]\n",
      "episode:  572\n",
      "loss: [16.2425521]\n",
      "episode:  573\n",
      "loss: [15.0999168]\n",
      "episode:  574\n",
      "loss: [14.45528977]\n",
      "episode:  575\n",
      "loss: [9.93776384]\n",
      "episode:  576\n",
      "loss: [15.491923]\n",
      "episode:  577\n",
      "loss: [13.1208449]\n",
      "episode:  578\n",
      "loss: [11.99990708]\n",
      "episode:  579\n",
      "loss: [13.28007813]\n",
      "episode:  580\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3356], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3356, 0.9973, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5404], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5404, 0.9914, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5666], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5666, 0.9894, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5649], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5649, 0.9788, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5331], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5331, 0.9725, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5068], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5068, 0.9803, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5215], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5215, 0.9902, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5510], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5510, 0.9932, 0.9375], dtype=torch.float64)\n",
      "tensor([0.5640], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5640, 0.9757, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5159], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5159, 0.9689, 0.9219], dtype=torch.float64)\n",
      "tensor([0.4846], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4846, 0.9655, 0.9141], dtype=torch.float64)\n",
      "tensor([0.4663], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4663, 0.9613, 0.9062], dtype=torch.float64)\n",
      "tensor([0.4487], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4487, 0.9634, 0.8984], dtype=torch.float64)\n",
      "tensor([0.4492], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4492, 0.9560, 0.8906], dtype=torch.float64)\n",
      "tensor([0.4265], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4265, 0.9319, 0.8828], dtype=torch.float64)\n",
      "tensor([0.3515], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3515, 0.9307, 0.8750], dtype=torch.float64)\n",
      "tensor([0.3299], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3299, 0.9414, 0.8672], dtype=torch.float64)\n",
      "tensor([0.3540], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3540, 0.9524, 0.8594], dtype=torch.float64)\n",
      "tensor([0.3889], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3889, 0.9419, 0.8516], dtype=torch.float64)\n",
      "tensor([0.3651], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3651, 0.9639, 0.8438], dtype=torch.float64)\n",
      "tensor([0.4207], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4207, 0.9503, 0.8359], dtype=torch.float64)\n",
      "tensor([0.3925], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3925, 0.9360, 0.8281], dtype=torch.float64)\n",
      "tensor([0.3440], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3440, 0.9536, 0.8203], dtype=torch.float64)\n",
      "tensor([0.3818], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3818, 0.9277, 0.8125], dtype=torch.float64)\n",
      "tensor([0.3149], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3149, 0.9433, 0.8047], dtype=torch.float64)\n",
      "tensor([0.3429], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3429, 0.9549, 0.7969], dtype=torch.float64)\n",
      "tensor([0.3804], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3804, 0.9485, 0.7891], dtype=torch.float64)\n",
      "tensor([0.3686], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3686, 0.9489, 0.7812], dtype=torch.float64)\n",
      "tensor([0.3655], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3655, 0.9519, 0.7734], dtype=torch.float64)\n",
      "tensor([0.3717], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3717, 0.9535, 0.7656], dtype=torch.float64)\n",
      "tensor([0.3761], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3761, 0.9506, 0.7578], dtype=torch.float64)\n",
      "tensor([0.3672], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3672, 0.9492, 0.7500], dtype=torch.float64)\n",
      "tensor([0.3595], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3595, 0.9513, 0.7422], dtype=torch.float64)\n",
      "tensor([0.3621], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3621, 0.9619, 0.7344], dtype=torch.float64)\n",
      "tensor([0.3918], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3918, 0.9550, 0.7266], dtype=torch.float64)\n",
      "tensor([0.3777], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3777, 0.9735, 0.7188], dtype=torch.float64)\n",
      "tensor([0.4314], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4314, 0.9717, 0.7109], dtype=torch.float64)\n",
      "tensor([0.4389], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4389, 0.9698, 0.7031], dtype=torch.float64)\n",
      "tensor([0.4351], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4351, 0.9835, 0.6953], dtype=torch.float64)\n",
      "tensor([0.4764], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4764, 1.0013, 0.6875], dtype=torch.float64)\n",
      "tensor([0.5411], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5411, 0.9908, 0.6797], dtype=torch.float64)\n",
      "tensor([0.5249], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5249, 0.9929, 0.6719], dtype=torch.float64)\n",
      "tensor([0.5271], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5271, 1.0097, 0.6641], dtype=torch.float64)\n",
      "tensor([0.5789], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5789, 1.0263, 0.6562], dtype=torch.float64)\n",
      "tensor([0.6359], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6359, 1.0265, 0.6484], dtype=torch.float64)\n",
      "tensor([0.6485], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6485, 1.0538, 0.6406], dtype=torch.float64)\n",
      "tensor([0.7082], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7082, 1.0475, 0.6328], dtype=torch.float64)\n",
      "tensor([0.7078], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7078, 1.0261, 0.6250], dtype=torch.float64)\n",
      "tensor([0.6639], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6639, 1.0357, 0.6172], dtype=torch.float64)\n",
      "tensor([0.6751], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6751, 1.0223, 0.6094], dtype=torch.float64)\n",
      "tensor([0.6501], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6501, 1.0191, 0.6016], dtype=torch.float64)\n",
      "tensor([0.6370], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6370, 1.0254, 0.5938], dtype=torch.float64)\n",
      "tensor([0.6497], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6497, 1.0516, 0.5859], dtype=torch.float64)\n",
      "tensor([0.7068], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7068, 1.0492, 0.5781], dtype=torch.float64)\n",
      "tensor([0.7141], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7141, 1.0593, 0.5703], dtype=torch.float64)\n",
      "tensor([0.7370], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7370, 1.0688, 0.5625], dtype=torch.float64)\n",
      "tensor([0.7619], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7619, 1.0569, 0.5547], dtype=torch.float64)\n",
      "tensor([0.7428], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7428, 1.0259, 0.5469], dtype=torch.float64)\n",
      "tensor([0.6750], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6750, 1.0031, 0.5391], dtype=torch.float64)\n",
      "tensor([0.5940], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5940, 1.0083, 0.5312], dtype=torch.float64)\n",
      "tensor([0.5905], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5905, 1.0249, 0.5234], dtype=torch.float64)\n",
      "tensor([0.6400], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6400, 1.0269, 0.5156], dtype=torch.float64)\n",
      "tensor([0.6577], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6577, 1.0376, 0.5078], dtype=torch.float64)\n",
      "tensor([0.6839], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6839, 1.0563, 0.5000], dtype=torch.float64)\n",
      "tensor([0.7286], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7286, 1.0771, 0.4922], dtype=torch.float64)\n",
      "tensor([0.7802], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7802, 1.0839, 0.4844], dtype=torch.float64)\n",
      "tensor([0.8026], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8026, 1.1143, 0.4766], dtype=torch.float64)\n",
      "tensor([0.8546], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8546, 1.1193, 0.4688], dtype=torch.float64)\n",
      "tensor([0.8649], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8649, 1.1197, 0.4609], dtype=torch.float64)\n",
      "tensor([0.8664], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8664, 1.1223, 0.4531], dtype=torch.float64)\n",
      "tensor([0.8693], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8693, 1.0955, 0.4453], dtype=torch.float64)\n",
      "tensor([0.8425], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8425, 1.1027, 0.4375], dtype=torch.float64)\n",
      "tensor([0.8483], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8483, 1.1164, 0.4297], dtype=torch.float64)\n",
      "tensor([0.8624], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8624, 1.1144, 0.4219], dtype=torch.float64)\n",
      "tensor([0.8621], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8621, 1.1401, 0.4141], dtype=torch.float64)\n",
      "tensor([0.8873], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8873, 1.1227, 0.4062], dtype=torch.float64)\n",
      "tensor([0.8729], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8729, 1.1259, 0.3984], dtype=torch.float64)\n",
      "tensor([0.8749], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8749, 1.1242, 0.3906], dtype=torch.float64)\n",
      "tensor([0.8736], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8736, 1.1009, 0.3828], dtype=torch.float64)\n",
      "tensor([0.8511], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8511, 1.1149, 0.3750], dtype=torch.float64)\n",
      "tensor([0.8626], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8626, 1.1074, 0.3672], dtype=torch.float64)\n",
      "tensor([0.8567], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8567, 1.1221, 0.3594], dtype=torch.float64)\n",
      "tensor([0.8706], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8706, 1.1022, 0.3516], dtype=torch.float64)\n",
      "tensor([0.8528], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8528, 1.0955, 0.3438], dtype=torch.float64)\n",
      "tensor([0.8444], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8444, 1.0698, 0.3359], dtype=torch.float64)\n",
      "tensor([0.7958], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7958, 1.0567, 0.3281], dtype=torch.float64)\n",
      "tensor([0.7619], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7619, 1.0632, 0.3203], dtype=torch.float64)\n",
      "tensor([0.7690], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7690, 1.0554, 0.3125], dtype=torch.float64)\n",
      "tensor([0.7546], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7546, 1.0314, 0.3047], dtype=torch.float64)\n",
      "tensor([0.6989], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6989, 1.0170, 0.2969], dtype=torch.float64)\n",
      "tensor([0.6415], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6415, 1.0219, 0.2891], dtype=torch.float64)\n",
      "tensor([0.6425], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6425, 1.0298, 0.2812], dtype=torch.float64)\n",
      "tensor([0.6669], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6669, 1.0176, 0.2734], dtype=torch.float64)\n",
      "tensor([0.6357], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6357, 1.0282, 0.2656], dtype=torch.float64)\n",
      "tensor([0.6603], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6603, 1.0333, 0.2578], dtype=torch.float64)\n",
      "tensor([0.6817], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6817, 1.0244, 0.2500], dtype=torch.float64)\n",
      "tensor([0.6598], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6598, 1.0407, 0.2422], dtype=torch.float64)\n",
      "tensor([0.7039], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7039, 1.0687, 0.2344], dtype=torch.float64)\n",
      "tensor([0.7729], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7729, 1.0671, 0.2266], dtype=torch.float64)\n",
      "tensor([0.7830], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7830, 1.0754, 0.2188], dtype=torch.float64)\n",
      "tensor([0.8006], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8006, 1.0618, 0.2109], dtype=torch.float64)\n",
      "tensor([0.7791], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7791, 1.0756, 0.2031], dtype=torch.float64)\n",
      "tensor([0.8011], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8011, 1.0688, 0.1953], dtype=torch.float64)\n",
      "tensor([0.7929], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7929, 1.0452, 0.1875], dtype=torch.float64)\n",
      "tensor([0.7454], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7454, 1.0507, 0.1797], dtype=torch.float64)\n",
      "tensor([0.7474], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7474, 1.0638, 0.1719], dtype=torch.float64)\n",
      "tensor([0.7750], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7750, 1.0696, 0.1641], dtype=torch.float64)\n",
      "tensor([0.7910], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7910, 1.0812, 0.1562], dtype=torch.float64)\n",
      "tensor([0.8159], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8159, 1.0751, 0.1484], dtype=torch.float64)\n",
      "tensor([0.8097], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8097, 1.0757, 0.1406], dtype=torch.float64)\n",
      "tensor([0.8100], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8100, 1.0809, 0.1328], dtype=torch.float64)\n",
      "tensor([0.8200], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8200, 1.0875, 0.1250], dtype=torch.float64)\n",
      "tensor([0.8341], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8341, 1.0769, 0.1172], dtype=torch.float64)\n",
      "tensor([0.8164], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8164, 1.1070, 0.1094], dtype=torch.float64)\n",
      "tensor([0.8583], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8583, 1.0894, 0.1016], dtype=torch.float64)\n",
      "tensor([0.8429], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8429, 1.0773, 0.0938], dtype=torch.float64)\n",
      "tensor([0.8186], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8186, 1.0789, 0.0859], dtype=torch.float64)\n",
      "tensor([0.8188], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8188, 1.0755, 0.0781], dtype=torch.float64)\n",
      "tensor([0.8126], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8126, 1.0947, 0.0703], dtype=torch.float64)\n",
      "tensor([0.8464], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8464, 1.0941, 0.0625], dtype=torch.float64)\n",
      "tensor([0.8482], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8482, 1.0644, 0.0547], dtype=torch.float64)\n",
      "tensor([0.7963], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7963, 1.0644, 0.0469], dtype=torch.float64)\n",
      "tensor([0.7892], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7892, 1.0590, 0.0391], dtype=torch.float64)\n",
      "tensor([0.7769], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7769, 1.0970, 0.0312], dtype=torch.float64)\n",
      "tensor([0.8468], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8468, 1.0892, 0.0234], dtype=torch.float64)\n",
      "tensor([0.8429], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8429, 1.1082, 0.0156], dtype=torch.float64)\n",
      "tensor([0.8623], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8623, 1.0943, 0.0078], dtype=torch.float64)\n",
      "tensor([0.8501], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.2191772077753457e-06 7.098753293919155e-07\n",
      "loss: [13.47863434]\n",
      "episode:  581\n",
      "loss: [12.21588111]\n",
      "episode:  582\n",
      "loss: [11.98408545]\n",
      "episode:  583\n",
      "loss: [14.51148831]\n",
      "episode:  584\n",
      "loss: [21.52708302]\n",
      "episode:  585\n",
      "loss: [12.42384673]\n",
      "episode:  586\n",
      "loss: [12.03531476]\n",
      "episode:  587\n",
      "loss: [42.82845866]\n",
      "episode:  588\n",
      "loss: [16.48394579]\n",
      "episode:  589\n",
      "loss: [11.43101799]\n",
      "episode:  590\n",
      "loss: [10.5414482]\n",
      "episode:  591\n",
      "loss: [15.05709951]\n",
      "episode:  592\n",
      "loss: [13.74192591]\n",
      "episode:  593\n",
      "loss: [12.43536209]\n",
      "episode:  594\n",
      "loss: [14.58102809]\n",
      "episode:  595\n",
      "loss: [13.32787638]\n",
      "episode:  596\n",
      "loss: [13.38154935]\n",
      "episode:  597\n",
      "loss: [10.62260216]\n",
      "episode:  598\n",
      "loss: [17.40182899]\n",
      "episode:  599\n",
      "loss: [11.08994142]\n",
      "episode:  600\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3352], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3352, 1.0111, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5705], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5705, 0.9994, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5910], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5910, 0.9974, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5902], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5902, 1.0054, 0.9688], dtype=torch.float64)\n",
      "tensor([0.6040], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6040, 0.9847, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5560], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5560, 0.9997, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5851], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5851, 1.0086, 0.9453], dtype=torch.float64)\n",
      "tensor([0.6061], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6061, 1.0231, 0.9375], dtype=torch.float64)\n",
      "tensor([0.6362], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6362, 1.0268, 0.9297], dtype=torch.float64)\n",
      "tensor([0.6477], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6477, 1.0146, 0.9219], dtype=torch.float64)\n",
      "tensor([0.6260], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6260, 0.9996, 0.9141], dtype=torch.float64)\n",
      "tensor([0.5927], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5927, 1.0122, 0.9062], dtype=torch.float64)\n",
      "tensor([0.6096], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6096, 1.0067, 0.8984], dtype=torch.float64)\n",
      "tensor([0.6014], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6014, 1.0209, 0.8906], dtype=torch.float64)\n",
      "tensor([0.6256], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6256, 1.0199, 0.8828], dtype=torch.float64)\n",
      "tensor([0.6272], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6272, 1.0278, 0.8750], dtype=torch.float64)\n",
      "tensor([0.6412], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6412, 1.0166, 0.8672], dtype=torch.float64)\n",
      "tensor([0.6221], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6221, 1.0305, 0.8594], dtype=torch.float64)\n",
      "tensor([0.6434], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6434, 1.0450, 0.8516], dtype=torch.float64)\n",
      "tensor([0.6766], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6766, 1.0537, 0.8438], dtype=torch.float64)\n",
      "tensor([0.7020], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7020, 1.0465, 0.8359], dtype=torch.float64)\n",
      "tensor([0.6926], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6926, 1.0375, 0.8281], dtype=torch.float64)\n",
      "tensor([0.6724], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6724, 1.0524, 0.8203], dtype=torch.float64)\n",
      "tensor([0.6997], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6997, 1.0423, 0.8125], dtype=torch.float64)\n",
      "tensor([0.6847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6847, 1.0523, 0.8047], dtype=torch.float64)\n",
      "tensor([0.7028], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7028, 1.0662, 0.7969], dtype=torch.float64)\n",
      "tensor([0.7358], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7358, 1.0526, 0.7891], dtype=torch.float64)\n",
      "tensor([0.7148], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7148, 1.0575, 0.7812], dtype=torch.float64)\n",
      "tensor([0.7211], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7211, 1.0505, 0.7734], dtype=torch.float64)\n",
      "tensor([0.7084], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7084, 1.0404, 0.7656], dtype=torch.float64)\n",
      "tensor([0.6852], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6852, 1.0266, 0.7578], dtype=torch.float64)\n",
      "tensor([0.6523], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6523, 1.0254, 0.7500], dtype=torch.float64)\n",
      "tensor([0.6434], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6434, 1.0290, 0.7422], dtype=torch.float64)\n",
      "tensor([0.6495], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6495, 1.0125, 0.7344], dtype=torch.float64)\n",
      "tensor([0.6165], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6165, 1.0183, 0.7266], dtype=torch.float64)\n",
      "tensor([0.6226], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6226, 1.0527, 0.7188], dtype=torch.float64)\n",
      "tensor([0.6956], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6956, 1.0632, 0.7109], dtype=torch.float64)\n",
      "tensor([0.7329], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7329, 1.0540, 0.7031], dtype=torch.float64)\n",
      "tensor([0.7220], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7220, 1.0584, 0.6953], dtype=torch.float64)\n",
      "tensor([0.7293], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7293, 1.0481, 0.6875], dtype=torch.float64)\n",
      "tensor([0.7099], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7099, 1.0497, 0.6797], dtype=torch.float64)\n",
      "tensor([0.7097], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7097, 1.0410, 0.6719], dtype=torch.float64)\n",
      "tensor([0.6921], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6921, 1.0405, 0.6641], dtype=torch.float64)\n",
      "tensor([0.6877], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6877, 1.0331, 0.6562], dtype=torch.float64)\n",
      "tensor([0.6719], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6719, 1.0426, 0.6484], dtype=torch.float64)\n",
      "tensor([0.6887], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6887, 1.0297, 0.6406], dtype=torch.float64)\n",
      "tensor([0.6660], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6660, 1.0149, 0.6328], dtype=torch.float64)\n",
      "tensor([0.6275], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6275, 1.0018, 0.6250], dtype=torch.float64)\n",
      "tensor([0.5783], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5783, 1.0130, 0.6172], dtype=torch.float64)\n",
      "tensor([0.6004], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6004, 1.0193, 0.6094], dtype=torch.float64)\n",
      "tensor([0.6249], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6249, 1.0093, 0.6016], dtype=torch.float64)\n",
      "tensor([0.6005], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6005, 1.0175, 0.5938], dtype=torch.float64)\n",
      "tensor([0.6193], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6193, 0.9891, 0.5859], dtype=torch.float64)\n",
      "tensor([0.5377], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5377, 0.9777, 0.5781], dtype=torch.float64)\n",
      "tensor([0.4828], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4828, 0.9805, 0.5703], dtype=torch.float64)\n",
      "tensor([0.4779], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4779, 0.9797, 0.5625], dtype=torch.float64)\n",
      "tensor([0.4743], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4743, 0.9958, 0.5547], dtype=torch.float64)\n",
      "tensor([0.5228], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5228, 1.0060, 0.5469], dtype=torch.float64)\n",
      "tensor([0.5657], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5657, 1.0100, 0.5391], dtype=torch.float64)\n",
      "tensor([0.5882], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5882, 0.9820, 0.5312], dtype=torch.float64)\n",
      "tensor([0.5080], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5080, 0.9743, 0.5234], dtype=torch.float64)\n",
      "tensor([0.4647], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4647, 0.9721, 0.5156], dtype=torch.float64)\n",
      "tensor([0.4474], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4474, 0.9611, 0.5078], dtype=torch.float64)\n",
      "tensor([0.4093], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4093, 0.9692, 0.5000], dtype=torch.float64)\n",
      "tensor([0.4246], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4246, 0.9728, 0.4922], dtype=torch.float64)\n",
      "tensor([0.4396], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4396, 0.9768, 0.4844], dtype=torch.float64)\n",
      "tensor([0.4557], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4557, 0.9694, 0.4766], dtype=torch.float64)\n",
      "tensor([0.4368], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4368, 0.9706, 0.4688], dtype=torch.float64)\n",
      "tensor([0.4347], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4347, 0.9764, 0.4609], dtype=torch.float64)\n",
      "tensor([0.4522], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4522, 0.9939, 0.4531], dtype=torch.float64)\n",
      "tensor([0.5114], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5114, 0.9867, 0.4453], dtype=torch.float64)\n",
      "tensor([0.5038], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5038, 0.9876, 0.4375], dtype=torch.float64)\n",
      "tensor([0.5046], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5046, 0.9735, 0.4297], dtype=torch.float64)\n",
      "tensor([0.4613], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4613, 0.9793, 0.4219], dtype=torch.float64)\n",
      "tensor([0.4661], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4661, 0.9786, 0.4141], dtype=torch.float64)\n",
      "tensor([0.4647], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4647, 0.9883, 0.4062], dtype=torch.float64)\n",
      "tensor([0.4952], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4952, 0.9850, 0.3984], dtype=torch.float64)\n",
      "tensor([0.4929], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4929, 1.0146, 0.3906], dtype=torch.float64)\n",
      "tensor([0.5841], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5841, 1.0169, 0.3828], dtype=torch.float64)\n",
      "tensor([0.6130], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6130, 1.0261, 0.3750], dtype=torch.float64)\n",
      "tensor([0.6481], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6481, 1.0179, 0.3672], dtype=torch.float64)\n",
      "tensor([0.6315], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6315, 1.0095, 0.3594], dtype=torch.float64)\n",
      "tensor([0.6020], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6020, 1.0005, 0.3516], dtype=torch.float64)\n",
      "tensor([0.5676], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5676, 1.0005, 0.3438], dtype=torch.float64)\n",
      "tensor([0.5592], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5592, 0.9978, 0.3359], dtype=torch.float64)\n",
      "tensor([0.5482], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5482, 0.9928, 0.3281], dtype=torch.float64)\n",
      "tensor([0.5280], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5280, 1.0022, 0.3203], dtype=torch.float64)\n",
      "tensor([0.5511], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5511, 0.9906, 0.3125], dtype=torch.float64)\n",
      "tensor([0.5203], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5203, 0.9639, 0.3047], dtype=torch.float64)\n",
      "tensor([0.4230], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4230, 0.9488, 0.2969], dtype=torch.float64)\n",
      "tensor([0.3432], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3432, 0.9616, 0.2891], dtype=torch.float64)\n",
      "tensor([0.3594], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3594, 0.9378, 0.2812], dtype=torch.float64)\n",
      "tensor([0.2860], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2860, 0.9639, 0.2734], dtype=torch.float64)\n",
      "tensor([0.3473], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3473, 0.9620, 0.2656], dtype=torch.float64)\n",
      "tensor([0.3594], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3594, 0.9826, 0.2578], dtype=torch.float64)\n",
      "tensor([0.4297], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4297, 0.9709, 0.2500], dtype=torch.float64)\n",
      "tensor([0.4125], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4125, 0.9628, 0.2422], dtype=torch.float64)\n",
      "tensor([0.3802], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3802, 0.9429, 0.2344], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3046], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3046, 0.9352, 0.2266], dtype=torch.float64)\n",
      "tensor([0.2552], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2552, 0.9336, 0.2188], dtype=torch.float64)\n",
      "tensor([0.2341], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2341, 0.9379, 0.2109], dtype=torch.float64)\n",
      "tensor([0.2408], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2408, 0.9091, 0.2031], dtype=torch.float64)\n",
      "tensor([0.1546], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1546, 0.8957, 0.1953], dtype=torch.float64)\n",
      "tensor([0.1048], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1048, 0.9169, 0.1875], dtype=torch.float64)\n",
      "tensor([0.1385], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1385, 0.9135, 0.1797], dtype=torch.float64)\n",
      "tensor([0.1376], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1376, 0.9201, 0.1719], dtype=torch.float64)\n",
      "tensor([0.1555], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1555, 0.8932, 0.1641], dtype=torch.float64)\n",
      "tensor([0.0989], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0989, 0.8985, 0.1562], dtype=torch.float64)\n",
      "tensor([0.0986], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0986, 0.8951, 0.1484], dtype=torch.float64)\n",
      "tensor([0.0927], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0927, 0.9123, 0.1406], dtype=torch.float64)\n",
      "tensor([0.1230], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1230, 0.9038, 0.1328], dtype=torch.float64)\n",
      "tensor([0.1113], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1113, 0.9077, 0.1250], dtype=torch.float64)\n",
      "tensor([0.1161], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1161, 0.8948, 0.1172], dtype=torch.float64)\n",
      "tensor([0.0928], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0928, 0.8649, 0.1094], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8416, 0.1016], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8577, 0.0938], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8318, 0.0859], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8207, 0.0781], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8183, 0.0703], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8066, 0.0625], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8074, 0.0547], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8043, 0.0469], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8190, 0.0391], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8132, 0.0312], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8217, 0.0234], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7966, 0.0156], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7976, 0.0078], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.2245672316444167e-06 7.553945254190573e-07\n",
      "loss: [12.14366447]\n",
      "episode:  601\n",
      "loss: [11.44486832]\n",
      "episode:  602\n",
      "loss: [13.74477349]\n",
      "episode:  603\n",
      "loss: [18.18445927]\n",
      "episode:  604\n",
      "loss: [13.69196693]\n",
      "episode:  605\n",
      "loss: [12.45985528]\n",
      "episode:  606\n",
      "loss: [13.69056942]\n",
      "episode:  607\n",
      "loss: [13.90846147]\n",
      "episode:  608\n",
      "loss: [22.16303126]\n",
      "episode:  609\n",
      "loss: [15.68003747]\n",
      "episode:  610\n",
      "loss: [12.65877795]\n",
      "episode:  611\n",
      "loss: [12.11952465]\n",
      "episode:  612\n",
      "loss: [14.13099105]\n",
      "episode:  613\n",
      "loss: [11.59190827]\n",
      "episode:  614\n",
      "loss: [16.19203802]\n",
      "episode:  615\n",
      "loss: [13.76208672]\n",
      "episode:  616\n",
      "loss: [12.67086862]\n",
      "episode:  617\n",
      "loss: [14.91118329]\n",
      "episode:  618\n",
      "loss: [12.1276217]\n",
      "episode:  619\n",
      "loss: [14.78291831]\n",
      "episode:  620\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3352], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3352, 0.9985, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5431], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5431, 0.9905, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5640], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5640, 0.9949, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5793], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5793, 0.9903, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5681], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5681, 0.9897, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5622], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5622, 0.9542, 0.9531], dtype=torch.float64)\n",
      "tensor([0.4591], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4591, 0.9423, 0.9453], dtype=torch.float64)\n",
      "tensor([0.4010], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4010, 0.9433, 0.9375], dtype=torch.float64)\n",
      "tensor([0.3896], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3896, 0.9461, 0.9297], dtype=torch.float64)\n",
      "tensor([0.3934], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3934, 0.9426, 0.9219], dtype=torch.float64)\n",
      "tensor([0.3824], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3824, 0.9239, 0.9141], dtype=torch.float64)\n",
      "tensor([0.3253], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3253, 0.9424, 0.9062], dtype=torch.float64)\n",
      "tensor([0.3637], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3637, 0.9541, 0.8984], dtype=torch.float64)\n",
      "tensor([0.4038], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4038, 0.9502, 0.8906], dtype=torch.float64)\n",
      "tensor([0.3996], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3996, 0.9433, 0.8828], dtype=torch.float64)\n",
      "tensor([0.3777], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3777, 0.9207, 0.8750], dtype=torch.float64)\n",
      "tensor([0.3068], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3068, 0.9173, 0.8672], dtype=torch.float64)\n",
      "tensor([0.2801], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2801, 0.9001, 0.8594], dtype=torch.float64)\n",
      "tensor([0.2235], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2235, 0.8973, 0.8516], dtype=torch.float64)\n",
      "tensor([0.2015], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2015, 0.8964, 0.8438], dtype=torch.float64)\n",
      "tensor([0.1925], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1925, 0.8699, 0.8359], dtype=torch.float64)\n",
      "tensor([0.1209], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1209, 0.8701, 0.8281], dtype=torch.float64)\n",
      "tensor([0.1087], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1087, 0.8656, 0.8203], dtype=torch.float64)\n",
      "tensor([0.0978], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0978, 0.8506, 0.8125], dtype=torch.float64)\n",
      "tensor([0.0746], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0746, 0.8454, 0.8047], dtype=torch.float64)\n",
      "tensor([0.0735], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0735, 0.8328, 0.7969], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8269, 0.7891], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8151, 0.7812], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8293, 0.7734], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8478, 0.7656], dtype=torch.float64)\n",
      "tensor([0.0733], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0733, 0.8641, 0.7578], dtype=torch.float64)\n",
      "tensor([0.0792], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0792, 0.8794, 0.7500], dtype=torch.float64)\n",
      "tensor([0.1046], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1046, 0.8789, 0.7422], dtype=torch.float64)\n",
      "tensor([0.1065], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1065, 0.8762, 0.7344], dtype=torch.float64)\n",
      "tensor([0.1008], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1008, 0.8665, 0.7266], dtype=torch.float64)\n",
      "tensor([0.0822], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0822, 0.8749, 0.7188], dtype=torch.float64)\n",
      "tensor([0.0921], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0921, 0.8620, 0.7109], dtype=torch.float64)\n",
      "tensor([0.0749], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0749, 0.8687, 0.7031], dtype=torch.float64)\n",
      "tensor([0.0780], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0780, 0.8724, 0.6953], dtype=torch.float64)\n",
      "tensor([0.0832], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0832, 0.8853, 0.6875], dtype=torch.float64)\n",
      "tensor([0.1047], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1047, 0.8805, 0.6797], dtype=torch.float64)\n",
      "tensor([0.0986], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0986, 0.8810, 0.6719], dtype=torch.float64)\n",
      "tensor([0.0971], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0971, 0.8964, 0.6641], dtype=torch.float64)\n",
      "tensor([0.1217], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1217, 0.8921, 0.6562], dtype=torch.float64)\n",
      "tensor([0.1171], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1171, 0.8722, 0.6484], dtype=torch.float64)\n",
      "tensor([0.0813], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0813, 0.8467, 0.6406], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8387, 0.6328], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8335, 0.6250], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8344, 0.6172], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8321, 0.6094], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8152, 0.6016], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8085, 0.5938], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8177, 0.5859], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8097, 0.5781], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8151, 0.5703], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8275, 0.5625], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8304, 0.5547], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8340, 0.5469], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8426, 0.5391], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8438, 0.5312], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8488, 0.5234], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8534, 0.5156], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8553, 0.5078], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8413, 0.5000], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8142, 0.4922], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8186, 0.4844], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8149, 0.4766], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8242, 0.4688], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8266, 0.4609], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8269, 0.4531], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8240, 0.4453], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8011, 0.4375], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8126, 0.4297], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8257, 0.4219], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8298, 0.4141], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8344, 0.4062], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8507, 0.3984], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8763, 0.3906], dtype=torch.float64)\n",
      "tensor([0.0737], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0737, 0.8557, 0.3828], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8479, 0.3750], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8534, 0.3672], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8652, 0.3594], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8518, 0.3516], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8474, 0.3438], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8346, 0.3359], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8361, 0.3281], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8444, 0.3203], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8525, 0.3125], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8541, 0.3047], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8667, 0.2969], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8538, 0.2891], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8411, 0.2812], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8380, 0.2734], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8513, 0.2656], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8410, 0.2578], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8304, 0.2500], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8351, 0.2422], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8546, 0.2344], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8451, 0.2266], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8631, 0.2188], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8750, 0.2109], dtype=torch.float64)\n",
      "tensor([0.0734], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0734, 0.8628, 0.2031], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8757, 0.1953], dtype=torch.float64)\n",
      "tensor([0.0735], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0735, 0.8699, 0.1875], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8888, 0.1797], dtype=torch.float64)\n",
      "tensor([0.0849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0849, 0.9153, 0.1719], dtype=torch.float64)\n",
      "tensor([0.1295], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1295, 0.9241, 0.1641], dtype=torch.float64)\n",
      "tensor([0.1640], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1640, 0.9435, 0.1562], dtype=torch.float64)\n",
      "tensor([0.2319], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2319, 0.9217, 0.1484], dtype=torch.float64)\n",
      "tensor([0.1838], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1838, 0.9235, 0.1406], dtype=torch.float64)\n",
      "tensor([0.1751], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1751, 0.9261, 0.1328], dtype=torch.float64)\n",
      "tensor([0.1796], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1796, 0.9363, 0.1250], dtype=torch.float64)\n",
      "tensor([0.2103], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2103, 0.9056, 0.1172], dtype=torch.float64)\n",
      "tensor([0.1299], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1299, 0.8935, 0.1094], dtype=torch.float64)\n",
      "tensor([0.0924], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0924, 0.8735, 0.1016], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8649, 0.0938], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8643, 0.0859], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8736, 0.0781], dtype=torch.float64)\n",
      "tensor([0.0735], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0735, 0.8722, 0.0703], dtype=torch.float64)\n",
      "tensor([0.0734], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0734, 0.8819, 0.0625], dtype=torch.float64)\n",
      "tensor([0.0796], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0796, 0.8724, 0.0547], dtype=torch.float64)\n",
      "tensor([0.0734], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0734, 0.8859, 0.0469], dtype=torch.float64)\n",
      "tensor([0.0878], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0878, 0.8927, 0.0391], dtype=torch.float64)\n",
      "tensor([0.0958], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0958, 0.8935, 0.0312], dtype=torch.float64)\n",
      "tensor([0.0954], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0954, 0.8818, 0.0234], dtype=torch.float64)\n",
      "tensor([0.0761], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0761, 0.8882, 0.0156], dtype=torch.float64)\n",
      "tensor([0.0935], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0935, 0.9043, 0.0078], dtype=torch.float64)\n",
      "tensor([0.1167], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.3711056667786908e-06 5.70442568319175e-07\n",
      "loss: [15.2436165]\n",
      "episode:  621\n",
      "loss: [12.22189443]\n",
      "episode:  622\n",
      "loss: [13.05254818]\n",
      "episode:  623\n",
      "loss: [13.99871293]\n",
      "episode:  624\n",
      "loss: [13.5870033]\n",
      "episode:  625\n",
      "loss: [12.44665617]\n",
      "episode:  626\n",
      "loss: [10.71278086]\n",
      "episode:  627\n",
      "loss: [12.13189479]\n",
      "episode:  628\n",
      "loss: [12.5836304]\n",
      "episode:  629\n",
      "loss: [12.97027948]\n",
      "episode:  630\n",
      "loss: [13.97449209]\n",
      "episode:  631\n",
      "loss: [15.11029174]\n",
      "episode:  632\n",
      "loss: [17.49951625]\n",
      "episode:  633\n",
      "loss: [13.9637323]\n",
      "episode:  634\n",
      "loss: [19.32014335]\n",
      "episode:  635\n",
      "loss: [22.72762901]\n",
      "episode:  636\n",
      "loss: [13.71572758]\n",
      "episode:  637\n",
      "loss: [12.54075724]\n",
      "episode:  638\n",
      "loss: [15.07011343]\n",
      "episode:  639\n",
      "loss: [14.16942655]\n",
      "episode:  640\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3353], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3353, 1.0094, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5673], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5673, 1.0057, 0.9844], dtype=torch.float64)\n",
      "tensor([0.6022], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6022, 0.9931, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5825], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5825, 0.9949, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5816], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5816, 1.0162, 0.9609], dtype=torch.float64)\n",
      "tensor([0.6216], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6216, 1.0281, 0.9531], dtype=torch.float64)\n",
      "tensor([0.6502], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6502, 1.0240, 0.9453], dtype=torch.float64)\n",
      "tensor([0.6469], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6469, 1.0055, 0.9375], dtype=torch.float64)\n",
      "tensor([0.6109], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6109, 1.0170, 0.9297], dtype=torch.float64)\n",
      "tensor([0.6248], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6248, 1.0255, 0.9219], dtype=torch.float64)\n",
      "tensor([0.6423], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6423, 1.0277, 0.9141], dtype=torch.float64)\n",
      "tensor([0.6486], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6486, 1.0112, 0.9062], dtype=torch.float64)\n",
      "tensor([0.6180], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6180, 1.0045, 0.8984], dtype=torch.float64)\n",
      "tensor([0.5990], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5990, 0.9869, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5465], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5465, 0.9761, 0.8828], dtype=torch.float64)\n",
      "tensor([0.5030], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5030, 0.9815, 0.8750], dtype=torch.float64)\n",
      "tensor([0.5071], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5071, 0.9831, 0.8672], dtype=torch.float64)\n",
      "tensor([0.5108], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5108, 0.9708, 0.8594], dtype=torch.float64)\n",
      "tensor([0.4752], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4752, 0.9665, 0.8516], dtype=torch.float64)\n",
      "tensor([0.4534], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4534, 0.9808, 0.8438], dtype=torch.float64)\n",
      "tensor([0.4876], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4876, 0.9944, 0.8359], dtype=torch.float64)\n",
      "tensor([0.5319], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5319, 0.9931, 0.8281], dtype=torch.float64)\n",
      "tensor([0.5363], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5363, 0.9888, 0.8203], dtype=torch.float64)\n",
      "tensor([0.5235], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5235, 1.0001, 0.8125], dtype=torch.float64)\n",
      "tensor([0.5508], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5508, 0.9873, 0.8047], dtype=torch.float64)\n",
      "tensor([0.5192], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5192, 0.9894, 0.7969], dtype=torch.float64)\n",
      "tensor([0.5164], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5164, 0.9643, 0.7891], dtype=torch.float64)\n",
      "tensor([0.4428], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4428, 0.9681, 0.7812], dtype=torch.float64)\n",
      "tensor([0.4360], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4360, 0.9636, 0.7734], dtype=torch.float64)\n",
      "tensor([0.4202], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4202, 0.9594, 0.7656], dtype=torch.float64)\n",
      "tensor([0.4028], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4028, 0.9546, 0.7578], dtype=torch.float64)\n",
      "tensor([0.3839], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3839, 0.9513, 0.7500], dtype=torch.float64)\n",
      "tensor([0.3687], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3687, 0.9451, 0.7422], dtype=torch.float64)\n",
      "tensor([0.3461], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3461, 0.9419, 0.7344], dtype=torch.float64)\n",
      "tensor([0.3303], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3303, 0.9559, 0.7266], dtype=torch.float64)\n",
      "tensor([0.3650], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3650, 0.9413, 0.7188], dtype=torch.float64)\n",
      "tensor([0.3295], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3295, 0.9432, 0.7109], dtype=torch.float64)\n",
      "tensor([0.3257], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3257, 0.9110, 0.7031], dtype=torch.float64)\n",
      "tensor([0.2314], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2314, 0.8982, 0.6953], dtype=torch.float64)\n",
      "tensor([0.1673], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1673, 0.9017, 0.6875], dtype=torch.float64)\n",
      "tensor([0.1580], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1580, 0.9123, 0.6797], dtype=torch.float64)\n",
      "tensor([0.1839], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1839, 0.9180, 0.6719], dtype=torch.float64)\n",
      "tensor([0.2062], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2062, 0.9125, 0.6641], dtype=torch.float64)\n",
      "tensor([0.1936], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1936, 0.9097, 0.6562], dtype=torch.float64)\n",
      "tensor([0.1792], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1792, 0.9016, 0.6484], dtype=torch.float64)\n",
      "tensor([0.1506], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1506, 0.8912, 0.6406], dtype=torch.float64)\n",
      "tensor([0.1175], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1175, 0.8937, 0.6328], dtype=torch.float64)\n",
      "tensor([0.1152], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1152, 0.9030, 0.6250], dtype=torch.float64)\n",
      "tensor([0.1348], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1348, 0.8929, 0.6172], dtype=torch.float64)\n",
      "tensor([0.1153], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1153, 0.8829, 0.6094], dtype=torch.float64)\n",
      "tensor([0.0932], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0932, 0.8681, 0.6016], dtype=torch.float64)\n",
      "tensor([0.0738], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0738, 0.8853, 0.5938], dtype=torch.float64)\n",
      "tensor([0.0895], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0895, 0.8745, 0.5859], dtype=torch.float64)\n",
      "tensor([0.0752], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0752, 0.8819, 0.5781], dtype=torch.float64)\n",
      "tensor([0.0829], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0829, 0.8794, 0.5703], dtype=torch.float64)\n",
      "tensor([0.0795], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0795, 0.8804, 0.5625], dtype=torch.float64)\n",
      "tensor([0.0803], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0803, 0.8937, 0.5547], dtype=torch.float64)\n",
      "tensor([0.1045], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1045, 0.8961, 0.5469], dtype=torch.float64)\n",
      "tensor([0.1128], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1128, 0.8980, 0.5391], dtype=torch.float64)\n",
      "tensor([0.1174], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1174, 0.8887, 0.5312], dtype=torch.float64)\n",
      "tensor([0.1006], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1006, 0.8826, 0.5234], dtype=torch.float64)\n",
      "tensor([0.0861], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0861, 0.8791, 0.5156], dtype=torch.float64)\n",
      "tensor([0.0771], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0771, 0.8626, 0.5078], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8424, 0.5000], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8265, 0.4922], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8236, 0.4844], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8377, 0.4766], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8479, 0.4688], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8475, 0.4609], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8504, 0.4531], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8603, 0.4453], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8870, 0.4375], dtype=torch.float64)\n",
      "tensor([0.0855], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0855, 0.8793, 0.4297], dtype=torch.float64)\n",
      "tensor([0.0750], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0750, 0.8804, 0.4219], dtype=torch.float64)\n",
      "tensor([0.0751], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0751, 0.8763, 0.4141], dtype=torch.float64)\n",
      "tensor([0.0739], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0739, 0.8767, 0.4062], dtype=torch.float64)\n",
      "tensor([0.0739], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0739, 0.8700, 0.3984], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8805, 0.3906], dtype=torch.float64)\n",
      "tensor([0.0748], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0748, 0.8709, 0.3828], dtype=torch.float64)\n",
      "tensor([0.0733], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0733, 0.8416, 0.3750], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0732, 0.8379, 0.3672], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8335, 0.3594], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8051, 0.3516], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8066, 0.3438], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8059, 0.3359], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8200, 0.3281], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8282, 0.3203], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8160, 0.3125], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8198, 0.3047], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8054, 0.2969], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8064, 0.2891], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8268, 0.2812], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8225, 0.2734], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8336, 0.2656], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8295, 0.2578], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8362, 0.2500], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8289, 0.2422], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8249, 0.2344], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8267, 0.2266], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8345, 0.2188], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8231, 0.2109], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7959, 0.2031], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7754, 0.1953], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7674, 0.1875], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7742, 0.1797], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7614, 0.1719], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7667, 0.1641], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7532, 0.1562], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7522, 0.1484], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7567, 0.1406], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7546, 0.1328], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7615, 0.1250], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7826, 0.1172], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8023, 0.1094], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8025, 0.1016], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8169, 0.0938], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8050, 0.0859], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8019, 0.0781], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7910, 0.0703], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8016, 0.0625], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8061, 0.0547], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7839, 0.0469], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7905, 0.0391], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7727, 0.0312], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7736, 0.0234], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7744, 0.0156], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7654, 0.0078], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.1820310409578912e-06 6.721850567486784e-07\n",
      "loss: [12.34624085]\n",
      "episode:  641\n",
      "loss: [15.54501128]\n",
      "episode:  642\n",
      "loss: [13.75457932]\n",
      "episode:  643\n",
      "loss: [12.12896083]\n",
      "episode:  644\n",
      "loss: [11.54636818]\n",
      "episode:  645\n",
      "loss: [16.67509986]\n",
      "episode:  646\n",
      "loss: [13.63610002]\n",
      "episode:  647\n",
      "loss: [18.2233047]\n",
      "episode:  648\n",
      "loss: [11.72831626]\n",
      "episode:  649\n",
      "loss: [16.63563086]\n",
      "episode:  650\n",
      "loss: [11.01631977]\n",
      "episode:  651\n",
      "loss: [14.65894694]\n",
      "episode:  652\n",
      "loss: [17.45271445]\n",
      "episode:  653\n",
      "loss: [13.15919537]\n",
      "episode:  654\n",
      "loss: [14.97336219]\n",
      "episode:  655\n",
      "loss: [15.29170122]\n",
      "episode:  656\n",
      "loss: [12.72526842]\n",
      "episode:  657\n",
      "loss: [16.83659129]\n",
      "episode:  658\n",
      "loss: [15.48081061]\n",
      "episode:  659\n",
      "loss: [14.38504846]\n",
      "episode:  660\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3353], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3353, 1.0075, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5638], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5638, 1.0061, 0.9844], dtype=torch.float64)\n",
      "tensor([0.6023], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6023, 1.0022, 0.9766], dtype=torch.float64)\n",
      "tensor([0.6014], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6014, 1.0012, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5982], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5982, 0.9895, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5683], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5683, 1.0038, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5952], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5952, 1.0182, 0.9453], dtype=torch.float64)\n",
      "tensor([0.6261], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6261, 1.0108, 0.9375], dtype=torch.float64)\n",
      "tensor([0.6170], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6170, 0.9822, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5452], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5452, 0.9956, 0.9219], dtype=torch.float64)\n",
      "tensor([0.5656], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5656, 1.0015, 0.9141], dtype=torch.float64)\n",
      "tensor([0.5849], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5849, 0.9954, 0.9062], dtype=torch.float64)\n",
      "tensor([0.5703], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5703, 1.0121, 0.8984], dtype=torch.float64)\n",
      "tensor([0.6044], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6044, 1.0352, 0.8906], dtype=torch.float64)\n",
      "tensor([0.6528], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6528, 1.0142, 0.8828], dtype=torch.float64)\n",
      "tensor([0.6216], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6216, 1.0385, 0.8750], dtype=torch.float64)\n",
      "tensor([0.6603], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6603, 1.0283, 0.8672], dtype=torch.float64)\n",
      "tensor([0.6475], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6475, 1.0165, 0.8594], dtype=torch.float64)\n",
      "tensor([0.6222], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6222, 1.0171, 0.8516], dtype=torch.float64)\n",
      "tensor([0.6176], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6176, 1.0259, 0.8438], dtype=torch.float64)\n",
      "tensor([0.6323], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6323, 1.0395, 0.8359], dtype=torch.float64)\n",
      "tensor([0.6638], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6638, 1.0506, 0.8281], dtype=torch.float64)\n",
      "tensor([0.6937], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6937, 1.0409, 0.8203], dtype=torch.float64)\n",
      "tensor([0.6802], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6802, 1.0459, 0.8125], dtype=torch.float64)\n",
      "tensor([0.6882], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6882, 1.0705, 0.8047], dtype=torch.float64)\n",
      "tensor([0.7413], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7413, 1.0774, 0.7969], dtype=torch.float64)\n",
      "tensor([0.7670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7670, 1.0925, 0.7891], dtype=torch.float64)\n",
      "tensor([0.8004], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8004, 1.0798, 0.7812], dtype=torch.float64)\n",
      "tensor([0.7835], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7835, 1.0880, 0.7734], dtype=torch.float64)\n",
      "tensor([0.7960], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7960, 1.0977, 0.7656], dtype=torch.float64)\n",
      "tensor([0.8166], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8166, 1.0850, 0.7578], dtype=torch.float64)\n",
      "tensor([0.7973], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7973, 1.0879, 0.7500], dtype=torch.float64)\n",
      "tensor([0.7994], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7994, 1.0749, 0.7422], dtype=torch.float64)\n",
      "tensor([0.7763], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7763, 1.0712, 0.7344], dtype=torch.float64)\n",
      "tensor([0.7649], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7649, 1.0708, 0.7266], dtype=torch.float64)\n",
      "tensor([0.7622], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7622, 1.0800, 0.7188], dtype=torch.float64)\n",
      "tensor([0.7800], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7800, 1.0722, 0.7109], dtype=torch.float64)\n",
      "tensor([0.7689], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7689, 1.0628, 0.7031], dtype=torch.float64)\n",
      "tensor([0.7476], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7476, 1.0774, 0.6953], dtype=torch.float64)\n",
      "tensor([0.7736], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7736, 1.0744, 0.6875], dtype=torch.float64)\n",
      "tensor([0.7732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7732, 1.0655, 0.6797], dtype=torch.float64)\n",
      "tensor([0.7554], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7554, 1.0797, 0.6719], dtype=torch.float64)\n",
      "tensor([0.7806], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7806, 1.0840, 0.6641], dtype=torch.float64)\n",
      "tensor([0.7934], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7934, 1.0920, 0.6562], dtype=torch.float64)\n",
      "tensor([0.8110], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8110, 1.0857, 0.6484], dtype=torch.float64)\n",
      "tensor([0.8030], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8030, 1.0978, 0.6406], dtype=torch.float64)\n",
      "tensor([0.8243], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8243, 1.1011, 0.6328], dtype=torch.float64)\n",
      "tensor([0.8346], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8346, 1.0960, 0.6250], dtype=torch.float64)\n",
      "tensor([0.8276], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8276, 1.1249, 0.6172], dtype=torch.float64)\n",
      "tensor([0.8634], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8634, 1.1209, 0.6094], dtype=torch.float64)\n",
      "tensor([0.8631], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8631, 1.1380, 0.6016], dtype=torch.float64)\n",
      "tensor([0.8799], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8799, 1.1751, 0.5938], dtype=torch.float64)\n",
      "tensor([0.9179], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9179, 1.1652, 0.5859], dtype=torch.float64)\n",
      "tensor([0.9122], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9122, 1.1713, 0.5781], dtype=torch.float64)\n",
      "tensor([0.9177], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9177, 1.1708, 0.5703], dtype=torch.float64)\n",
      "tensor([0.9180], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9180, 1.1581, 0.5625], dtype=torch.float64)\n",
      "tensor([0.9059], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9059, 1.1338, 0.5547], dtype=torch.float64)\n",
      "tensor([0.8813], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8813, 1.1316, 0.5469], dtype=torch.float64)\n",
      "tensor([0.8770], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8770, 1.1144, 0.5391], dtype=torch.float64)\n",
      "tensor([0.8600], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8600, 1.0954, 0.5312], dtype=torch.float64)\n",
      "tensor([0.8358], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8358, 1.1123, 0.5234], dtype=torch.float64)\n",
      "tensor([0.8544], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8544, 1.1300, 0.5156], dtype=torch.float64)\n",
      "tensor([0.8736], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8736, 1.1074, 0.5078], dtype=torch.float64)\n",
      "tensor([0.8537], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8537, 1.1168, 0.5000], dtype=torch.float64)\n",
      "tensor([0.8611], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8611, 1.1015, 0.4922], dtype=torch.float64)\n",
      "tensor([0.8471], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8471, 1.0866, 0.4844], dtype=torch.float64)\n",
      "tensor([0.8195], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8195, 1.0918, 0.4766], dtype=torch.float64)\n",
      "tensor([0.8244], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8244, 1.0853, 0.4688], dtype=torch.float64)\n",
      "tensor([0.8137], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8137, 1.0832, 0.4609], dtype=torch.float64)\n",
      "tensor([0.8083], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8083, 1.0867, 0.4531], dtype=torch.float64)\n",
      "tensor([0.8140], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8140, 1.0986, 0.4453], dtype=torch.float64)\n",
      "tensor([0.8374], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8374, 1.1074, 0.4375], dtype=torch.float64)\n",
      "tensor([0.8520], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8520, 1.1106, 0.4297], dtype=torch.float64)\n",
      "tensor([0.8568], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8568, 1.1084, 0.4219], dtype=torch.float64)\n",
      "tensor([0.8553], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8553, 1.1389, 0.4141], dtype=torch.float64)\n",
      "tensor([0.8850], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8850, 1.1234, 0.4062], dtype=torch.float64)\n",
      "tensor([0.8731], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8731, 1.1199, 0.3984], dtype=torch.float64)\n",
      "tensor([0.8687], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8687, 1.1287, 0.3906], dtype=torch.float64)\n",
      "tensor([0.8770], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8770, 1.0991, 0.3828], dtype=torch.float64)\n",
      "tensor([0.8493], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8493, 1.0999, 0.3750], dtype=torch.float64)\n",
      "tensor([0.8475], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8475, 1.1050, 0.3672], dtype=torch.float64)\n",
      "tensor([0.8525], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8525, 1.1165, 0.3594], dtype=torch.float64)\n",
      "tensor([0.8644], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8644, 1.1134, 0.3516], dtype=torch.float64)\n",
      "tensor([0.8627], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8627, 1.1101, 0.3438], dtype=torch.float64)\n",
      "tensor([0.8596], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8596, 1.0777, 0.3359], dtype=torch.float64)\n",
      "tensor([0.8127], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8127, 1.0959, 0.3281], dtype=torch.float64)\n",
      "tensor([0.8381], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8381, 1.1138, 0.3203], dtype=torch.float64)\n",
      "tensor([0.8614], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8614, 1.1085, 0.3125], dtype=torch.float64)\n",
      "tensor([0.8587], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8587, 1.1078, 0.3047], dtype=torch.float64)\n",
      "tensor([0.8580], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8580, 1.1081, 0.2969], dtype=torch.float64)\n",
      "tensor([0.8584], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8584, 1.0829, 0.2891], dtype=torch.float64)\n",
      "tensor([0.8245], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8245, 1.0530, 0.2812], dtype=torch.float64)\n",
      "tensor([0.7623], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7623, 1.0462, 0.2734], dtype=torch.float64)\n",
      "tensor([0.7359], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7359, 1.0277, 0.2656], dtype=torch.float64)\n",
      "tensor([0.6825], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6825, 1.0355, 0.2578], dtype=torch.float64)\n",
      "tensor([0.6932], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6932, 1.0130, 0.2500], dtype=torch.float64)\n",
      "tensor([0.6274], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6274, 0.9789, 0.2422], dtype=torch.float64)\n",
      "tensor([0.4992], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4992, 0.9816, 0.2344], dtype=torch.float64)\n",
      "tensor([0.4676], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4676, 0.9658, 0.2266], dtype=torch.float64)\n",
      "tensor([0.4055], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4055, 0.9584, 0.2188], dtype=torch.float64)\n",
      "tensor([0.3614], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3614, 0.9515, 0.2109], dtype=torch.float64)\n",
      "tensor([0.3246], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3246, 0.9464, 0.2031], dtype=torch.float64)\n",
      "tensor([0.2958], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2958, 0.9739, 0.1953], dtype=torch.float64)\n",
      "tensor([0.3755], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3755, 0.9636, 0.1875], dtype=torch.float64)\n",
      "tensor([0.3662], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3662, 0.9683, 0.1797], dtype=torch.float64)\n",
      "tensor([0.3777], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3777, 0.9565, 0.1719], dtype=torch.float64)\n",
      "tensor([0.3420], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3420, 0.9679, 0.1641], dtype=torch.float64)\n",
      "tensor([0.3675], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3675, 0.9809, 0.1562], dtype=torch.float64)\n",
      "tensor([0.4169], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4169, 0.9583, 0.1484], dtype=torch.float64)\n",
      "tensor([0.3579], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3579, 0.9506, 0.1406], dtype=torch.float64)\n",
      "tensor([0.3138], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3138, 0.9655, 0.1328], dtype=torch.float64)\n",
      "tensor([0.3480], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3480, 0.9709, 0.1250], dtype=torch.float64)\n",
      "tensor([0.3752], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3752, 0.9884, 0.1172], dtype=torch.float64)\n",
      "tensor([0.4398], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4398, 0.9931, 0.1094], dtype=torch.float64)\n",
      "tensor([0.4747], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4747, 0.9996, 0.1016], dtype=torch.float64)\n",
      "tensor([0.5057], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5057, 0.9829, 0.0938], dtype=torch.float64)\n",
      "tensor([0.4603], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4603, 0.9903, 0.0859], dtype=torch.float64)\n",
      "tensor([0.4694], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4694, 0.9801, 0.0781], dtype=torch.float64)\n",
      "tensor([0.4385], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4385, 0.9628, 0.0703], dtype=torch.float64)\n",
      "tensor([0.3718], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3718, 0.9606, 0.0625], dtype=torch.float64)\n",
      "tensor([0.3431], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3431, 0.9666, 0.0547], dtype=torch.float64)\n",
      "tensor([0.3532], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3532, 0.9648, 0.0469], dtype=torch.float64)\n",
      "tensor([0.3496], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3496, 0.9755, 0.0391], dtype=torch.float64)\n",
      "tensor([0.3826], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3826, 0.9887, 0.0312], dtype=torch.float64)\n",
      "tensor([0.4350], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4350, 1.0021, 0.0234], dtype=torch.float64)\n",
      "tensor([0.4942], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4942, 1.0041, 0.0156], dtype=torch.float64)\n",
      "tensor([0.5181], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5181, 1.0098, 0.0078], dtype=torch.float64)\n",
      "tensor([0.5431], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.0954486773600324e-06 5.141550292273862e-07\n",
      "loss: [32.08916709]\n",
      "episode:  661\n",
      "loss: [12.55104309]\n",
      "episode:  662\n",
      "loss: [12.990014]\n",
      "episode:  663\n",
      "loss: [14.90736477]\n",
      "episode:  664\n",
      "loss: [14.99678062]\n",
      "episode:  665\n",
      "loss: [12.53937879]\n",
      "episode:  666\n",
      "loss: [13.23157956]\n",
      "episode:  667\n",
      "loss: [14.60641267]\n",
      "episode:  668\n",
      "loss: [13.82307721]\n",
      "episode:  669\n",
      "loss: [14.29411649]\n",
      "episode:  670\n",
      "loss: [15.84653405]\n",
      "episode:  671\n",
      "loss: [18.86306158]\n",
      "episode:  672\n",
      "loss: [16.30870397]\n",
      "episode:  673\n",
      "loss: [12.64030643]\n",
      "episode:  674\n",
      "loss: [13.89072019]\n",
      "episode:  675\n",
      "loss: [15.03060214]\n",
      "episode:  676\n",
      "loss: [12.34847931]\n",
      "episode:  677\n",
      "loss: [11.55399654]\n",
      "episode:  678\n",
      "loss: [13.3655633]\n",
      "episode:  679\n",
      "loss: [22.48174656]\n",
      "episode:  680\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3353], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3353, 0.9893, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5174], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5174, 0.9951, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5715], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5715, 0.9937, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5775], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5775, 0.9818, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5439], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5439, 0.9809, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5324], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5324, 0.9900, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5538], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5538, 0.9803, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5298], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5298, 0.9660, 0.9375], dtype=torch.float64)\n",
      "tensor([0.4823], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4823, 0.9610, 0.9297], dtype=torch.float64)\n",
      "tensor([0.4559], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4559, 0.9599, 0.9219], dtype=torch.float64)\n",
      "tensor([0.4453], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4453, 0.9665, 0.9141], dtype=torch.float64)\n",
      "tensor([0.4603], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4603, 0.9706, 0.9062], dtype=torch.float64)\n",
      "tensor([0.4734], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4734, 0.9549, 0.8984], dtype=torch.float64)\n",
      "tensor([0.4300], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4300, 0.9485, 0.8906], dtype=torch.float64)\n",
      "tensor([0.4008], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4008, 0.9545, 0.8828], dtype=torch.float64)\n",
      "tensor([0.4098], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4098, 0.9515, 0.8750], dtype=torch.float64)\n",
      "tensor([0.4016], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4016, 0.9547, 0.8672], dtype=torch.float64)\n",
      "tensor([0.4071], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4071, 0.9521, 0.8594], dtype=torch.float64)\n",
      "tensor([0.3993], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3993, 0.9507, 0.8516], dtype=torch.float64)\n",
      "tensor([0.3921], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3921, 0.9667, 0.8438], dtype=torch.float64)\n",
      "tensor([0.4342], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4342, 0.9578, 0.8359], dtype=torch.float64)\n",
      "tensor([0.4166], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4166, 0.9591, 0.8281], dtype=torch.float64)\n",
      "tensor([0.4147], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4147, 0.9587, 0.8203], dtype=torch.float64)\n",
      "tensor([0.4113], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4113, 0.9561, 0.8125], dtype=torch.float64)\n",
      "tensor([0.4017], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4017, 0.9494, 0.8047], dtype=torch.float64)\n",
      "tensor([0.3787], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3787, 0.9581, 0.7969], dtype=torch.float64)\n",
      "tensor([0.3969], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3969, 0.9358, 0.7891], dtype=torch.float64)\n",
      "tensor([0.3359], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3359, 0.9501, 0.7812], dtype=torch.float64)\n",
      "tensor([0.3614], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3614, 0.9539, 0.7734], dtype=torch.float64)\n",
      "tensor([0.3762], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3762, 0.9816, 0.7656], dtype=torch.float64)\n",
      "tensor([0.4566], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4566, 0.9729, 0.7578], dtype=torch.float64)\n",
      "tensor([0.4487], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4487, 0.9892, 0.7500], dtype=torch.float64)\n",
      "tensor([0.4971], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4971, 0.9727, 0.7422], dtype=torch.float64)\n",
      "tensor([0.4577], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4577, 0.9647, 0.7344], dtype=torch.float64)\n",
      "tensor([0.4236], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4236, 0.9648, 0.7266], dtype=torch.float64)\n",
      "tensor([0.4154], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4154, 0.9500, 0.7188], dtype=torch.float64)\n",
      "tensor([0.3678], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3678, 0.9561, 0.7109], dtype=torch.float64)\n",
      "tensor([0.3750], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3750, 0.9360, 0.7031], dtype=torch.float64)\n",
      "tensor([0.3144], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3144, 0.9427, 0.6953], dtype=torch.float64)\n",
      "tensor([0.3204], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3204, 0.9566, 0.6875], dtype=torch.float64)\n",
      "tensor([0.3648], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3648, 0.9656, 0.6797], dtype=torch.float64)\n",
      "tensor([0.4033], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4033, 0.9498, 0.6719], dtype=torch.float64)\n",
      "tensor([0.3639], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3639, 0.9500, 0.6641], dtype=torch.float64)\n",
      "tensor([0.3549], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3549, 0.9667, 0.6562], dtype=torch.float64)\n",
      "tensor([0.4043], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4043, 0.9554, 0.6484], dtype=torch.float64)\n",
      "tensor([0.3816], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3816, 0.9602, 0.6406], dtype=torch.float64)\n",
      "tensor([0.3909], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3909, 0.9496, 0.6328], dtype=torch.float64)\n",
      "tensor([0.3604], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3604, 0.9314, 0.6250], dtype=torch.float64)\n",
      "tensor([0.2965], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2965, 0.9155, 0.6172], dtype=torch.float64)\n",
      "tensor([0.2263], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2263, 0.9051, 0.6094], dtype=torch.float64)\n",
      "tensor([0.1699], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1699, 0.9025, 0.6016], dtype=torch.float64)\n",
      "tensor([0.1463], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1463, 0.9140, 0.5938], dtype=torch.float64)\n",
      "tensor([0.1726], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1726, 0.9242, 0.5859], dtype=torch.float64)\n",
      "tensor([0.2133], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2133, 0.9140, 0.5781], dtype=torch.float64)\n",
      "tensor([0.1919], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1919, 0.9023, 0.5703], dtype=torch.float64)\n",
      "tensor([0.1494], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1494, 0.8755, 0.5625], dtype=torch.float64)\n",
      "tensor([0.0834], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0834, 0.8685, 0.5547], dtype=torch.float64)\n",
      "tensor([0.0736], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0736, 0.8669, 0.5469], dtype=torch.float64)\n",
      "tensor([0.0734], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0734, 0.8664, 0.5391], dtype=torch.float64)\n",
      "tensor([0.0733], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0733, 0.8623, 0.5312], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8781, 0.5234], dtype=torch.float64)\n",
      "tensor([0.0753], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0753, 0.8719, 0.5156], dtype=torch.float64)\n",
      "tensor([0.0737], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0737, 0.8930, 0.5078], dtype=torch.float64)\n",
      "tensor([0.1000], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1000, 0.9071, 0.5000], dtype=torch.float64)\n",
      "tensor([0.1330], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1330, 0.8942, 0.4922], dtype=torch.float64)\n",
      "tensor([0.1118], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1118, 0.8983, 0.4844], dtype=torch.float64)\n",
      "tensor([0.1153], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1153, 0.9115, 0.4766], dtype=torch.float64)\n",
      "tensor([0.1479], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1479, 0.9070, 0.4688], dtype=torch.float64)\n",
      "tensor([0.1433], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1433, 0.9008, 0.4609], dtype=torch.float64)\n",
      "tensor([0.1244], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1244, 0.9037, 0.4531], dtype=torch.float64)\n",
      "tensor([0.1264], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1264, 0.9119, 0.4453], dtype=torch.float64)\n",
      "tensor([0.1494], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1494, 0.9013, 0.4375], dtype=torch.float64)\n",
      "tensor([0.1253], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1253, 0.9087, 0.4297], dtype=torch.float64)\n",
      "tensor([0.1389], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1389, 0.8994, 0.4219], dtype=torch.float64)\n",
      "tensor([0.1192], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1192, 0.9026, 0.4141], dtype=torch.float64)\n",
      "tensor([0.1213], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1213, 0.8986, 0.4062], dtype=torch.float64)\n",
      "tensor([0.1138], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1138, 0.8919, 0.3984], dtype=torch.float64)\n",
      "tensor([0.0999], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0999, 0.8951, 0.3906], dtype=torch.float64)\n",
      "tensor([0.1030], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1030, 0.8875, 0.3828], dtype=torch.float64)\n",
      "tensor([0.0892], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0892, 0.9007, 0.3750], dtype=torch.float64)\n",
      "tensor([0.1107], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1107, 0.8872, 0.3672], dtype=torch.float64)\n",
      "tensor([0.0892], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0892, 0.8943, 0.3594], dtype=torch.float64)\n",
      "tensor([0.0982], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0982, 0.8841, 0.3516], dtype=torch.float64)\n",
      "tensor([0.0808], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0808, 0.8734, 0.3438], dtype=torch.float64)\n",
      "tensor([0.0734], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0734, 0.8681, 0.3359], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8623, 0.3281], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8620, 0.3203], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8531, 0.3125], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8506, 0.3047], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8440, 0.2969], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8578, 0.2891], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8498, 0.2812], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8418, 0.2734], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8436, 0.2656], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8463, 0.2578], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8395, 0.2500], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8332, 0.2422], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8252, 0.2344], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8179, 0.2266], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8157, 0.2188], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8178, 0.2109], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8065, 0.2031], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8111, 0.1953], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7977, 0.1875], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7883, 0.1797], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7964, 0.1719], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7864, 0.1641], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7936, 0.1562], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7946, 0.1484], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7944, 0.1406], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7891, 0.1328], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7904, 0.1250], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7718, 0.1172], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7805, 0.1094], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7742, 0.1016], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7789, 0.0938], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7778, 0.0859], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7654, 0.0781], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7578, 0.0703], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7589, 0.0625], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7406, 0.0547], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7584, 0.0469], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7641, 0.0391], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7554, 0.0312], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7531, 0.0234], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7411, 0.0156], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7456, 0.0078], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.15028532115061e-06 5.929072725247e-07\n",
      "loss: [14.25088375]\n",
      "episode:  681\n",
      "loss: [15.05729927]\n",
      "episode:  682\n",
      "loss: [12.15554928]\n",
      "episode:  683\n",
      "loss: [9.10280393]\n",
      "episode:  684\n",
      "loss: [15.17849649]\n",
      "episode:  685\n",
      "loss: [11.25706597]\n",
      "episode:  686\n",
      "loss: [13.15971675]\n",
      "episode:  687\n",
      "loss: [16.29485295]\n",
      "episode:  688\n",
      "loss: [17.6146563]\n",
      "episode:  689\n",
      "loss: [12.47654547]\n",
      "episode:  690\n",
      "loss: [13.68836105]\n",
      "episode:  691\n",
      "loss: [11.36251499]\n",
      "episode:  692\n",
      "loss: [13.98013662]\n",
      "episode:  693\n",
      "loss: [14.03700183]\n",
      "episode:  694\n",
      "loss: [21.49292297]\n",
      "episode:  695\n",
      "loss: [14.28659194]\n",
      "episode:  696\n",
      "loss: [11.8967252]\n",
      "episode:  697\n",
      "loss: [15.03251907]\n",
      "episode:  698\n",
      "loss: [15.47058589]\n",
      "episode:  699\n",
      "loss: [14.06118772]\n",
      "episode:  700\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3353], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3353, 1.0202, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5876], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5876, 0.9889, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5693], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5693, 1.0006, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5922], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5922, 1.0070, 0.9688], dtype=torch.float64)\n",
      "tensor([0.6076], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6076, 0.9813, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5473], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5473, 0.9827, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5368], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5368, 0.9863, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5429], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5429, 0.9961, 0.9375], dtype=torch.float64)\n",
      "tensor([0.5701], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5701, 0.9930, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5656], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5656, 1.0129, 0.9219], dtype=torch.float64)\n",
      "tensor([0.6078], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6078, 1.0212, 0.9141], dtype=torch.float64)\n",
      "tensor([0.6302], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6302, 1.0195, 0.9062], dtype=torch.float64)\n",
      "tensor([0.6303], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6303, 1.0089, 0.8984], dtype=torch.float64)\n",
      "tensor([0.6095], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6095, 1.0024, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5923], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5923, 1.0037, 0.8828], dtype=torch.float64)\n",
      "tensor([0.5903], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5903, 1.0036, 0.8750], dtype=torch.float64)\n",
      "tensor([0.5882], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5882, 1.0105, 0.8672], dtype=torch.float64)\n",
      "tensor([0.6011], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6011, 1.0036, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5871], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5871, 1.0131, 0.8516], dtype=torch.float64)\n",
      "tensor([0.6037], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6037, 1.0234, 0.8438], dtype=torch.float64)\n",
      "tensor([0.6252], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6252, 1.0115, 0.8359], dtype=torch.float64)\n",
      "tensor([0.6060], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6060, 1.0369, 0.8281], dtype=torch.float64)\n",
      "tensor([0.6536], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6536, 1.0334, 0.8203], dtype=torch.float64)\n",
      "tensor([0.6565], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6565, 1.0362, 0.8125], dtype=torch.float64)\n",
      "tensor([0.6634], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6634, 1.0384, 0.8047], dtype=torch.float64)\n",
      "tensor([0.6698], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6698, 1.0262, 0.7969], dtype=torch.float64)\n",
      "tensor([0.6463], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6463, 1.0270, 0.7891], dtype=torch.float64)\n",
      "tensor([0.6436], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6436, 1.0077, 0.7812], dtype=torch.float64)\n",
      "tensor([0.6006], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6006, 1.0181, 0.7734], dtype=torch.float64)\n",
      "tensor([0.6166], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6166, 1.0224, 0.7656], dtype=torch.float64)\n",
      "tensor([0.6291], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6291, 1.0260, 0.7578], dtype=torch.float64)\n",
      "tensor([0.6398], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6398, 1.0240, 0.7500], dtype=torch.float64)\n",
      "tensor([0.6382], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6382, 1.0219, 0.7422], dtype=torch.float64)\n",
      "tensor([0.6338], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6338, 1.0340, 0.7344], dtype=torch.float64)\n",
      "tensor([0.6585], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6585, 1.0218, 0.7266], dtype=torch.float64)\n",
      "tensor([0.6387], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6387, 1.0222, 0.7188], dtype=torch.float64)\n",
      "tensor([0.6359], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6359, 1.0215, 0.7109], dtype=torch.float64)\n",
      "tensor([0.6343], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6343, 0.9998, 0.7031], dtype=torch.float64)\n",
      "tensor([0.5743], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5743, 0.9759, 0.6953], dtype=torch.float64)\n",
      "tensor([0.4864], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4864, 0.9805, 0.6875], dtype=torch.float64)\n",
      "tensor([0.4793], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4793, 0.9690, 0.6797], dtype=torch.float64)\n",
      "tensor([0.4418], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4418, 0.9567, 0.6719], dtype=torch.float64)\n",
      "tensor([0.3947], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3947, 0.9465, 0.6641], dtype=torch.float64)\n",
      "tensor([0.3517], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3517, 0.9453, 0.6562], dtype=torch.float64)\n",
      "tensor([0.3374], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3374, 0.9471, 0.6484], dtype=torch.float64)\n",
      "tensor([0.3395], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3395, 0.9619, 0.6406], dtype=torch.float64)\n",
      "tensor([0.3858], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3858, 0.9757, 0.6328], dtype=torch.float64)\n",
      "tensor([0.4395], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4395, 0.9699, 0.6250], dtype=torch.float64)\n",
      "tensor([0.4348], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4348, 0.9481, 0.6172], dtype=torch.float64)\n",
      "tensor([0.3665], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3665, 0.9213, 0.6094], dtype=torch.float64)\n",
      "tensor([0.2660], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2660, 0.9176, 0.6016], dtype=torch.float64)\n",
      "tensor([0.2221], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2221, 0.9395, 0.5938], dtype=torch.float64)\n",
      "tensor([0.2791], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2791, 0.9345, 0.5859], dtype=torch.float64)\n",
      "tensor([0.2797], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2797, 0.9337, 0.5781], dtype=torch.float64)\n",
      "tensor([0.2765], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2765, 0.9245, 0.5703], dtype=torch.float64)\n",
      "tensor([0.2450], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2450, 0.9398, 0.5625], dtype=torch.float64)\n",
      "tensor([0.2840], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2840, 0.9215, 0.5547], dtype=torch.float64)\n",
      "tensor([0.2360], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2360, 0.9396, 0.5469], dtype=torch.float64)\n",
      "tensor([0.2794], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2794, 0.9632, 0.5391], dtype=torch.float64)\n",
      "tensor([0.3688], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3688, 0.9511, 0.5312], dtype=torch.float64)\n",
      "tensor([0.3563], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3563, 0.9499, 0.5234], dtype=torch.float64)\n",
      "tensor([0.3477], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3477, 0.9816, 0.5156], dtype=torch.float64)\n",
      "tensor([0.4474], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4474, 0.9758, 0.5078], dtype=torch.float64)\n",
      "tensor([0.4546], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4546, 0.9845, 0.5000], dtype=torch.float64)\n",
      "tensor([0.4834], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4834, 0.9693, 0.4922], dtype=torch.float64)\n",
      "tensor([0.4435], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4435, 0.9821, 0.4844], dtype=torch.float64)\n",
      "tensor([0.4731], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4731, 0.9806, 0.4766], dtype=torch.float64)\n",
      "tensor([0.4759], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4759, 0.9864, 0.4688], dtype=torch.float64)\n",
      "tensor([0.4944], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4944, 0.9827, 0.4609], dtype=torch.float64)\n",
      "tensor([0.4874], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4874, 0.9915, 0.4531], dtype=torch.float64)\n",
      "tensor([0.5128], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5128, 0.9681, 0.4453], dtype=torch.float64)\n",
      "tensor([0.4468], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4468, 0.9446, 0.4375], dtype=torch.float64)\n",
      "tensor([0.3505], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3505, 0.9346, 0.4297], dtype=torch.float64)\n",
      "tensor([0.2873], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2873, 0.9448, 0.4219], dtype=torch.float64)\n",
      "tensor([0.2999], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2999, 0.9458, 0.4141], dtype=torch.float64)\n",
      "tensor([0.3064], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3064, 0.9414, 0.4062], dtype=torch.float64)\n",
      "tensor([0.2933], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2933, 0.9410, 0.3984], dtype=torch.float64)\n",
      "tensor([0.2872], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2872, 0.9303, 0.3906], dtype=torch.float64)\n",
      "tensor([0.2499], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2499, 0.9475, 0.3828], dtype=torch.float64)\n",
      "tensor([0.2936], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2936, 0.9434, 0.3750], dtype=torch.float64)\n",
      "tensor([0.2929], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2929, 0.9412, 0.3672], dtype=torch.float64)\n",
      "tensor([0.2847], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2847, 0.9371, 0.3594], dtype=torch.float64)\n",
      "tensor([0.2681], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2681, 0.9145, 0.3516], dtype=torch.float64)\n",
      "tensor([0.1888], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1888, 0.9191, 0.3438], dtype=torch.float64)\n",
      "tensor([0.1794], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1794, 0.9232, 0.3359], dtype=torch.float64)\n",
      "tensor([0.1881], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1881, 0.9335, 0.3281], dtype=torch.float64)\n",
      "tensor([0.2235], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2235, 0.9469, 0.3203], dtype=torch.float64)\n",
      "tensor([0.2772], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2772, 0.9297, 0.3125], dtype=torch.float64)\n",
      "tensor([0.2374], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2374, 0.9164, 0.3047], dtype=torch.float64)\n",
      "tensor([0.1818], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1818, 0.9291, 0.2969], dtype=torch.float64)\n",
      "tensor([0.2041], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2041, 0.9306, 0.2891], dtype=torch.float64)\n",
      "tensor([0.2154], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2154, 0.9332, 0.2812], dtype=torch.float64)\n",
      "tensor([0.2264], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2264, 0.9332, 0.2734], dtype=torch.float64)\n",
      "tensor([0.2291], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2291, 0.9224, 0.2656], dtype=torch.float64)\n",
      "tensor([0.1940], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1940, 0.9387, 0.2578], dtype=torch.float64)\n",
      "tensor([0.2356], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2356, 0.9815, 0.2500], dtype=torch.float64)\n",
      "tensor([0.3869], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3869, 0.9903, 0.2422], dtype=torch.float64)\n",
      "tensor([0.4619], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4619, 0.9760, 0.2344], dtype=torch.float64)\n",
      "tensor([0.4378], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4378, 0.9655, 0.2266], dtype=torch.float64)\n",
      "tensor([0.3955], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3955, 0.9801, 0.2188], dtype=torch.float64)\n",
      "tensor([0.4289], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4289, 0.9774, 0.2109], dtype=torch.float64)\n",
      "tensor([0.4298], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4298, 0.9689, 0.2031], dtype=torch.float64)\n",
      "tensor([0.4019], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4019, 0.9717, 0.1953], dtype=torch.float64)\n",
      "tensor([0.4016], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4016, 0.9678, 0.1875], dtype=torch.float64)\n",
      "tensor([0.3881], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3881, 0.9546, 0.1797], dtype=torch.float64)\n",
      "tensor([0.3399], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3399, 0.9675, 0.1719], dtype=torch.float64)\n",
      "tensor([0.3663], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3663, 0.9747, 0.1641], dtype=torch.float64)\n",
      "tensor([0.3971], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3971, 0.9678, 0.1562], dtype=torch.float64)\n",
      "tensor([0.3837], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3837, 0.9747, 0.1484], dtype=torch.float64)\n",
      "tensor([0.4012], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4012, 0.9702, 0.1406], dtype=torch.float64)\n",
      "tensor([0.3911], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3911, 0.9726, 0.1328], dtype=torch.float64)\n",
      "tensor([0.3951], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3951, 0.9664, 0.1250], dtype=torch.float64)\n",
      "tensor([0.3755], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3755, 0.9811, 0.1172], dtype=torch.float64)\n",
      "tensor([0.4162], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4162, 0.9855, 0.1094], dtype=torch.float64)\n",
      "tensor([0.4426], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4426, 0.9684, 0.1016], dtype=torch.float64)\n",
      "tensor([0.3945], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3945, 0.9724, 0.0938], dtype=torch.float64)\n",
      "tensor([0.3916], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3916, 0.9674, 0.0859], dtype=torch.float64)\n",
      "tensor([0.3739], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3739, 0.9556, 0.0781], dtype=torch.float64)\n",
      "tensor([0.3290], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3290, 0.9527, 0.0703], dtype=torch.float64)\n",
      "tensor([0.3049], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3049, 0.9510, 0.0625], dtype=torch.float64)\n",
      "tensor([0.2912], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2912, 0.9450, 0.0547], dtype=torch.float64)\n",
      "tensor([0.2667], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2667, 0.9665, 0.0469], dtype=torch.float64)\n",
      "tensor([0.3285], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3285, 0.9515, 0.0391], dtype=torch.float64)\n",
      "tensor([0.2980], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2980, 0.9507, 0.0312], dtype=torch.float64)\n",
      "tensor([0.2852], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2852, 0.9580, 0.0234], dtype=torch.float64)\n",
      "tensor([0.3042], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3042, 0.9420, 0.0156], dtype=torch.float64)\n",
      "tensor([0.2574], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2574, 0.9377, 0.0078], dtype=torch.float64)\n",
      "tensor([0.2281], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.1820244832312547e-06 6.39489776255882e-07\n",
      "loss: [14.49222931]\n",
      "episode:  701\n",
      "loss: [15.59062992]\n",
      "episode:  702\n",
      "loss: [18.31986233]\n",
      "episode:  703\n",
      "loss: [16.78674513]\n",
      "episode:  704\n",
      "loss: [12.7502558]\n",
      "episode:  705\n",
      "loss: [13.10625738]\n",
      "episode:  706\n",
      "loss: [10.69179614]\n",
      "episode:  707\n",
      "loss: [142.41989404]\n",
      "episode:  708\n",
      "loss: [17.90306262]\n",
      "episode:  709\n",
      "loss: [15.50309286]\n",
      "episode:  710\n",
      "loss: [11.7825166]\n",
      "episode:  711\n",
      "loss: [14.22521375]\n",
      "episode:  712\n",
      "loss: [13.68158858]\n",
      "episode:  713\n",
      "loss: [15.56015797]\n",
      "episode:  714\n",
      "loss: [13.98918812]\n",
      "episode:  715\n",
      "loss: [11.10577448]\n",
      "episode:  716\n",
      "loss: [9.58364556]\n",
      "episode:  717\n",
      "loss: [13.28168846]\n",
      "episode:  718\n",
      "loss: [12.4988483]\n",
      "episode:  719\n",
      "loss: [14.93183315]\n",
      "episode:  720\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3353], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3353, 1.0218, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5905], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5905, 1.0068, 0.9844], dtype=torch.float64)\n",
      "tensor([0.6086], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6086, 0.9857, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5631], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5631, 0.9879, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5580], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5580, 1.0083, 0.9609], dtype=torch.float64)\n",
      "tensor([0.6026], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6026, 1.0320, 0.9531], dtype=torch.float64)\n",
      "tensor([0.6541], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6541, 1.0393, 0.9453], dtype=torch.float64)\n",
      "tensor([0.6764], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6764, 1.0369, 0.9375], dtype=torch.float64)\n",
      "tensor([0.6750], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6750, 1.0466, 0.9297], dtype=torch.float64)\n",
      "tensor([0.6919], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6919, 1.0354, 0.9219], dtype=torch.float64)\n",
      "tensor([0.6732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6732, 1.0542, 0.9141], dtype=torch.float64)\n",
      "tensor([0.7038], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7038, 1.0457, 0.9062], dtype=torch.float64)\n",
      "tensor([0.6927], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6927, 1.0464, 0.8984], dtype=torch.float64)\n",
      "tensor([0.6911], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6911, 1.0584, 0.8906], dtype=torch.float64)\n",
      "tensor([0.7121], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7121, 1.0568, 0.8828], dtype=torch.float64)\n",
      "tensor([0.7137], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7137, 1.0628, 0.8750], dtype=torch.float64)\n",
      "tensor([0.7269], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7269, 1.0794, 0.8672], dtype=torch.float64)\n",
      "tensor([0.7644], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7644, 1.0777, 0.8594], dtype=torch.float64)\n",
      "tensor([0.7689], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7689, 1.0820, 0.8516], dtype=torch.float64)\n",
      "tensor([0.7784], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7784, 1.0804, 0.8438], dtype=torch.float64)\n",
      "tensor([0.7775], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7775, 1.0841, 0.8359], dtype=torch.float64)\n",
      "tensor([0.7846], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7846, 1.0936, 0.8281], dtype=torch.float64)\n",
      "tensor([0.8038], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8038, 1.0826, 0.8203], dtype=torch.float64)\n",
      "tensor([0.7875], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7875, 1.0630, 0.8125], dtype=torch.float64)\n",
      "tensor([0.7459], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7459, 1.0524, 0.8047], dtype=torch.float64)\n",
      "tensor([0.7157], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7157, 1.0229, 0.7969], dtype=torch.float64)\n",
      "tensor([0.6488], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6488, 1.0206, 0.7891], dtype=torch.float64)\n",
      "tensor([0.6309], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6309, 1.0165, 0.7812], dtype=torch.float64)\n",
      "tensor([0.6190], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6190, 1.0185, 0.7734], dtype=torch.float64)\n",
      "tensor([0.6211], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6211, 1.0195, 0.7656], dtype=torch.float64)\n",
      "tensor([0.6241], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6241, 0.9957, 0.7578], dtype=torch.float64)\n",
      "tensor([0.5593], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5593, 0.9856, 0.7500], dtype=torch.float64)\n",
      "tensor([0.5128], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5128, 0.9889, 0.7422], dtype=torch.float64)\n",
      "tensor([0.5117], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5117, 0.9919, 0.7344], dtype=torch.float64)\n",
      "tensor([0.5205], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5205, 0.9889, 0.7266], dtype=torch.float64)\n",
      "tensor([0.5137], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5137, 0.9819, 0.7188], dtype=torch.float64)\n",
      "tensor([0.4904], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4904, 0.9794, 0.7109], dtype=torch.float64)\n",
      "tensor([0.4767], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4767, 0.9823, 0.7031], dtype=torch.float64)\n",
      "tensor([0.4824], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4824, 0.9798, 0.6953], dtype=torch.float64)\n",
      "tensor([0.4760], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4760, 0.9592, 0.6875], dtype=torch.float64)\n",
      "tensor([0.4109], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4109, 0.9591, 0.6797], dtype=torch.float64)\n",
      "tensor([0.3945], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3945, 0.9731, 0.6719], dtype=torch.float64)\n",
      "tensor([0.4339], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4339, 0.9891, 0.6641], dtype=torch.float64)\n",
      "tensor([0.4928], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4928, 0.9838, 0.6562], dtype=torch.float64)\n",
      "tensor([0.4908], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4908, 0.9893, 0.6484], dtype=torch.float64)\n",
      "tensor([0.5074], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5074, 0.9969, 0.6406], dtype=torch.float64)\n",
      "tensor([0.5347], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5347, 0.9664, 0.6328], dtype=torch.float64)\n",
      "tensor([0.4472], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4472, 0.9817, 0.6250], dtype=torch.float64)\n",
      "tensor([0.4733], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4733, 0.9788, 0.6172], dtype=torch.float64)\n",
      "tensor([0.4705], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4705, 0.9920, 0.6094], dtype=torch.float64)\n",
      "tensor([0.5105], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5105, 0.9733, 0.6016], dtype=torch.float64)\n",
      "tensor([0.4626], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4626, 0.9608, 0.5938], dtype=torch.float64)\n",
      "tensor([0.4124], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4124, 0.9560, 0.5859], dtype=torch.float64)\n",
      "tensor([0.3853], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3853, 0.9711, 0.5781], dtype=torch.float64)\n",
      "tensor([0.4250], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4250, 0.9922, 0.5703], dtype=torch.float64)\n",
      "tensor([0.4999], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4999, 1.0158, 0.5625], dtype=torch.float64)\n",
      "tensor([0.5901], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5901, 0.9978, 0.5547], dtype=torch.float64)\n",
      "tensor([0.5572], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5572, 0.9976, 0.5469], dtype=torch.float64)\n",
      "tensor([0.5485], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5485, 1.0071, 0.5391], dtype=torch.float64)\n",
      "tensor([0.5752], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5752, 1.0145, 0.5312], dtype=torch.float64)\n",
      "tensor([0.6041], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6041, 1.0243, 0.5234], dtype=torch.float64)\n",
      "tensor([0.6409], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6409, 1.0114, 0.5156], dtype=torch.float64)\n",
      "tensor([0.6105], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6105, 1.0095, 0.5078], dtype=torch.float64)\n",
      "tensor([0.5975], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5975, 1.0338, 0.5000], dtype=torch.float64)\n",
      "tensor([0.6637], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6637, 1.0096, 0.4922], dtype=torch.float64)\n",
      "tensor([0.6106], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6106, 1.0034, 0.4844], dtype=torch.float64)\n",
      "tensor([0.5789], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5789, 0.9978, 0.4766], dtype=torch.float64)\n",
      "tensor([0.5541], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5541, 0.9919, 0.4688], dtype=torch.float64)\n",
      "tensor([0.5303], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5303, 1.0206, 0.4609], dtype=torch.float64)\n",
      "tensor([0.6117], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6117, 1.0194, 0.4531], dtype=torch.float64)\n",
      "tensor([0.6278], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6278, 1.0436, 0.4453], dtype=torch.float64)\n",
      "tensor([0.6933], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6933, 1.0539, 0.4375], dtype=torch.float64)\n",
      "tensor([0.7284], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7284, 1.0601, 0.4297], dtype=torch.float64)\n",
      "tensor([0.7490], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7490, 1.0694, 0.4219], dtype=torch.float64)\n",
      "tensor([0.7728], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7728, 1.0637, 0.4141], dtype=torch.float64)\n",
      "tensor([0.7666], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7666, 1.0846, 0.4062], dtype=torch.float64)\n",
      "tensor([0.8049], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8049, 1.0928, 0.3984], dtype=torch.float64)\n",
      "tensor([0.8274], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8274, 1.0870, 0.3906], dtype=torch.float64)\n",
      "tensor([0.8214], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8214, 1.0719, 0.3828], dtype=torch.float64)\n",
      "tensor([0.7926], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7926, 1.0736, 0.3750], dtype=torch.float64)\n",
      "tensor([0.7910], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7910, 1.0658, 0.3672], dtype=torch.float64)\n",
      "tensor([0.7765], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7765, 1.0824, 0.3594], dtype=torch.float64)\n",
      "tensor([0.8050], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8050, 1.1084, 0.3516], dtype=torch.float64)\n",
      "tensor([0.8521], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8521, 1.1119, 0.3438], dtype=torch.float64)\n",
      "tensor([0.8604], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8604, 1.1172, 0.3359], dtype=torch.float64)\n",
      "tensor([0.8665], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8665, 1.0953, 0.3281], dtype=torch.float64)\n",
      "tensor([0.8460], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8460, 1.0917, 0.3203], dtype=torch.float64)\n",
      "tensor([0.8370], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8370, 1.0925, 0.3125], dtype=torch.float64)\n",
      "tensor([0.8371], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8371, 1.0889, 0.3047], dtype=torch.float64)\n",
      "tensor([0.8308], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8308, 1.0739, 0.2969], dtype=torch.float64)\n",
      "tensor([0.8024], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8024, 1.0861, 0.2891], dtype=torch.float64)\n",
      "tensor([0.8202], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8202, 1.0726, 0.2812], dtype=torch.float64)\n",
      "tensor([0.7988], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7988, 1.0744, 0.2734], dtype=torch.float64)\n",
      "tensor([0.7985], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7985, 1.0547, 0.2656], dtype=torch.float64)\n",
      "tensor([0.7615], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7615, 1.0598, 0.2578], dtype=torch.float64)\n",
      "tensor([0.7649], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7649, 1.0516, 0.2500], dtype=torch.float64)\n",
      "tensor([0.7490], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7490, 1.0548, 0.2422], dtype=torch.float64)\n",
      "tensor([0.7528], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7528, 1.0434, 0.2344], dtype=torch.float64)\n",
      "tensor([0.7304], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7304, 1.0443, 0.2266], dtype=torch.float64)\n",
      "tensor([0.7281], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7281, 1.0465, 0.2188], dtype=torch.float64)\n",
      "tensor([0.7326], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7326, 1.0627, 0.2109], dtype=torch.float64)\n",
      "tensor([0.7675], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7675, 1.0711, 0.2031], dtype=torch.float64)\n",
      "tensor([0.7902], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7902, 1.0570, 0.1953], dtype=torch.float64)\n",
      "tensor([0.7685], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7685, 1.0570, 0.1875], dtype=torch.float64)\n",
      "tensor([0.7644], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7644, 1.0455, 0.1797], dtype=torch.float64)\n",
      "tensor([0.7402], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7402, 1.0164, 0.1719], dtype=torch.float64)\n",
      "tensor([0.6477], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6477, 1.0095, 0.1641], dtype=torch.float64)\n",
      "tensor([0.5966], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5966, 1.0080, 0.1562], dtype=torch.float64)\n",
      "tensor([0.5755], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5755, 1.0022, 0.1484], dtype=torch.float64)\n",
      "tensor([0.5498], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5498, 1.0167, 0.1406], dtype=torch.float64)\n",
      "tensor([0.5876], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5876, 1.0139, 0.1328], dtype=torch.float64)\n",
      "tensor([0.5893], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5893, 0.9853, 0.1250], dtype=torch.float64)\n",
      "tensor([0.4970], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4970, 0.9764, 0.1172], dtype=torch.float64)\n",
      "tensor([0.4386], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4386, 0.9689, 0.1094], dtype=torch.float64)\n",
      "tensor([0.3956], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3956, 0.9867, 0.1016], dtype=torch.float64)\n",
      "tensor([0.4394], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4394, 0.9919, 0.0938], dtype=torch.float64)\n",
      "tensor([0.4691], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4691, 0.9651, 0.0859], dtype=torch.float64)\n",
      "tensor([0.3903], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3903, 0.9623, 0.0781], dtype=torch.float64)\n",
      "tensor([0.3560], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3560, 0.9512, 0.0703], dtype=torch.float64)\n",
      "tensor([0.3085], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3085, 0.9457, 0.0625], dtype=torch.float64)\n",
      "tensor([0.2752], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2752, 0.9585, 0.0547], dtype=torch.float64)\n",
      "tensor([0.3058], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3058, 0.9507, 0.0469], dtype=torch.float64)\n",
      "tensor([0.2890], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2890, 0.9608, 0.0391], dtype=torch.float64)\n",
      "tensor([0.3161], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3161, 0.9664, 0.0312], dtype=torch.float64)\n",
      "tensor([0.3419], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3419, 0.9580, 0.0234], dtype=torch.float64)\n",
      "tensor([0.3218], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3218, 0.9626, 0.0156], dtype=torch.float64)\n",
      "tensor([0.3296], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3296, 0.9667, 0.0078], dtype=torch.float64)\n",
      "tensor([0.3448], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  9.840226238711097e-07 5.453251890727525e-07\n",
      "loss: [11.49194485]\n",
      "episode:  721\n",
      "loss: [16.27305091]\n",
      "episode:  722\n",
      "loss: [12.79684558]\n",
      "episode:  723\n",
      "loss: [74.79333278]\n",
      "episode:  724\n",
      "loss: [13.99997558]\n",
      "episode:  725\n",
      "loss: [13.06520484]\n",
      "episode:  726\n",
      "loss: [13.99134012]\n",
      "episode:  727\n",
      "loss: [11.08907695]\n",
      "episode:  728\n",
      "loss: [11.13928869]\n",
      "episode:  729\n",
      "loss: [13.97649855]\n",
      "episode:  730\n",
      "loss: [16.60704971]\n",
      "episode:  731\n",
      "loss: [17.40062085]\n",
      "episode:  732\n",
      "loss: [12.43305688]\n",
      "episode:  733\n",
      "loss: [14.17477818]\n",
      "episode:  734\n",
      "loss: [15.5871635]\n",
      "episode:  735\n",
      "loss: [16.25387294]\n",
      "episode:  736\n",
      "loss: [18.27551399]\n",
      "episode:  737\n",
      "loss: [14.88281885]\n",
      "episode:  738\n",
      "loss: [12.25739576]\n",
      "episode:  739\n",
      "loss: [13.13028092]\n",
      "episode:  740\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3353], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3353, 0.9939, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5305], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5305, 0.9782, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5269], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5269, 0.9789, 0.9766], dtype=torch.float64)\n",
      "tensor([0.5265], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5265, 0.9963, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5736], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5736, 1.0036, 0.9609], dtype=torch.float64)\n",
      "tensor([0.5968], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5968, 1.0065, 0.9531], dtype=torch.float64)\n",
      "tensor([0.6056], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6056, 0.9964, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5859], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5859, 0.9816, 0.9375], dtype=torch.float64)\n",
      "tensor([0.5385], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5385, 0.9781, 0.9297], dtype=torch.float64)\n",
      "tensor([0.5168], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5168, 0.9636, 0.9219], dtype=torch.float64)\n",
      "tensor([0.4692], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4692, 0.9477, 0.9141], dtype=torch.float64)\n",
      "tensor([0.4120], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4120, 0.9407, 0.9062], dtype=torch.float64)\n",
      "tensor([0.3780], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3780, 0.9444, 0.8984], dtype=torch.float64)\n",
      "tensor([0.3794], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3794, 0.9369, 0.8906], dtype=torch.float64)\n",
      "tensor([0.3567], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3567, 0.9429, 0.8828], dtype=torch.float64)\n",
      "tensor([0.3670], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3670, 0.9656, 0.8750], dtype=torch.float64)\n",
      "tensor([0.4321], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4321, 0.9640, 0.8672], dtype=torch.float64)\n",
      "tensor([0.4402], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4402, 0.9874, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5069], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5069, 0.9968, 0.8516], dtype=torch.float64)\n",
      "tensor([0.5463], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5463, 1.0101, 0.8438], dtype=torch.float64)\n",
      "tensor([0.5897], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5897, 1.0235, 0.8359], dtype=torch.float64)\n",
      "tensor([0.6219], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6219, 1.0154, 0.8281], dtype=torch.float64)\n",
      "tensor([0.6123], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6123, 0.9962, 0.8203], dtype=torch.float64)\n",
      "tensor([0.5608], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5608, 1.0002, 0.8125], dtype=torch.float64)\n",
      "tensor([0.5591], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5591, 1.0300, 0.8047], dtype=torch.float64)\n",
      "tensor([0.6308], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6308, 1.0116, 0.7969], dtype=torch.float64)\n",
      "tensor([0.6080], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6080, 0.9968, 0.7891], dtype=torch.float64)\n",
      "tensor([0.5590], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5590, 1.0192, 0.7812], dtype=torch.float64)\n",
      "tensor([0.6098], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6098, 1.0148, 0.7734], dtype=torch.float64)\n",
      "tensor([0.6116], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6116, 1.0092, 0.7656], dtype=torch.float64)\n",
      "tensor([0.5975], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5975, 1.0085, 0.7578], dtype=torch.float64)\n",
      "tensor([0.5918], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5918, 1.0180, 0.7500], dtype=torch.float64)\n",
      "tensor([0.6158], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6158, 1.0038, 0.7422], dtype=torch.float64)\n",
      "tensor([0.5819], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5819, 1.0089, 0.7344], dtype=torch.float64)\n",
      "tensor([0.5893], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5893, 0.9954, 0.7266], dtype=torch.float64)\n",
      "tensor([0.5501], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5501, 0.9985, 0.7188], dtype=torch.float64)\n",
      "tensor([0.5499], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5499, 0.9925, 0.7109], dtype=torch.float64)\n",
      "tensor([0.5315], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5315, 0.9923, 0.7031], dtype=torch.float64)\n",
      "tensor([0.5267], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5267, 1.0190, 0.6953], dtype=torch.float64)\n",
      "tensor([0.6065], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6065, 1.0166, 0.6875], dtype=torch.float64)\n",
      "tensor([0.6185], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6185, 0.9888, 0.6797], dtype=torch.float64)\n",
      "tensor([0.5369], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5369, 0.9970, 0.6719], dtype=torch.float64)\n",
      "tensor([0.5421], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5421, 1.0080, 0.6641], dtype=torch.float64)\n",
      "tensor([0.5768], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5768, 1.0028, 0.6562], dtype=torch.float64)\n",
      "tensor([0.5694], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5694, 1.0197, 0.6484], dtype=torch.float64)\n",
      "tensor([0.6190], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6190, 1.0432, 0.6406], dtype=torch.float64)\n",
      "tensor([0.6796], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6796, 1.0241, 0.6328], dtype=torch.float64)\n",
      "tensor([0.6531], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6531, 1.0176, 0.6250], dtype=torch.float64)\n",
      "tensor([0.6325], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6325, 1.0085, 0.6172], dtype=torch.float64)\n",
      "tensor([0.6001], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6001, 0.9977, 0.6094], dtype=torch.float64)\n",
      "tensor([0.5593], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5593, 0.9884, 0.6016], dtype=torch.float64)\n",
      "tensor([0.5212], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5212, 0.9771, 0.5938], dtype=torch.float64)\n",
      "tensor([0.4771], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4771, 0.9758, 0.5859], dtype=torch.float64)\n",
      "tensor([0.4622], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4622, 0.9573, 0.5781], dtype=torch.float64)\n",
      "tensor([0.4015], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4015, 0.9397, 0.5703], dtype=torch.float64)\n",
      "tensor([0.3322], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3322, 0.9442, 0.5625], dtype=torch.float64)\n",
      "tensor([0.3253], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3253, 0.9303, 0.5547], dtype=torch.float64)\n",
      "tensor([0.2773], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2773, 0.9252, 0.5469], dtype=torch.float64)\n",
      "tensor([0.2452], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2452, 0.9094, 0.5391], dtype=torch.float64)\n",
      "tensor([0.1829], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1829, 0.8987, 0.5312], dtype=torch.float64)\n",
      "tensor([0.1339], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1339, 0.8940, 0.5234], dtype=torch.float64)\n",
      "tensor([0.1130], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1130, 0.8856, 0.5156], dtype=torch.float64)\n",
      "tensor([0.0934], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0934, 0.9103, 0.5078], dtype=torch.float64)\n",
      "tensor([0.1408], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1408, 0.9102, 0.5000], dtype=torch.float64)\n",
      "tensor([0.1526], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1526, 0.9059, 0.4922], dtype=torch.float64)\n",
      "tensor([0.1432], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1432, 0.8920, 0.4844], dtype=torch.float64)\n",
      "tensor([0.1091], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1091, 0.9159, 0.4766], dtype=torch.float64)\n",
      "tensor([0.1584], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1584, 0.9050, 0.4688], dtype=torch.float64)\n",
      "tensor([0.1406], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1406, 0.9189, 0.4609], dtype=torch.float64)\n",
      "tensor([0.1745], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1745, 0.9361, 0.4531], dtype=torch.float64)\n",
      "tensor([0.2397], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2397, 0.9276, 0.4453], dtype=torch.float64)\n",
      "tensor([0.2316], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2316, 0.9204, 0.4375], dtype=torch.float64)\n",
      "tensor([0.2049], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2049, 0.9159, 0.4297], dtype=torch.float64)\n",
      "tensor([0.1813], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1813, 0.9021, 0.4219], dtype=torch.float64)\n",
      "tensor([0.1352], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1352, 0.9066, 0.4141], dtype=torch.float64)\n",
      "tensor([0.1347], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1347, 0.9153, 0.4062], dtype=torch.float64)\n",
      "tensor([0.1585], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1585, 0.9171, 0.3984], dtype=torch.float64)\n",
      "tensor([0.1696], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1696, 0.9230, 0.3906], dtype=torch.float64)\n",
      "tensor([0.1896], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1896, 0.9369, 0.3828], dtype=torch.float64)\n",
      "tensor([0.2402], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2402, 0.9185, 0.3750], dtype=torch.float64)\n",
      "tensor([0.1954], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1954, 0.9170, 0.3672], dtype=torch.float64)\n",
      "tensor([0.1769], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1769, 0.9066, 0.3594], dtype=torch.float64)\n",
      "tensor([0.1418], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1418, 0.9225, 0.3516], dtype=torch.float64)\n",
      "tensor([0.1768], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1768, 0.9310, 0.3438], dtype=torch.float64)\n",
      "tensor([0.2135], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2135, 0.9346, 0.3359], dtype=torch.float64)\n",
      "tensor([0.2357], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2357, 0.9315, 0.3281], dtype=torch.float64)\n",
      "tensor([0.2319], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2319, 0.9405, 0.3203], dtype=torch.float64)\n",
      "tensor([0.2591], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2591, 0.9561, 0.3125], dtype=torch.float64)\n",
      "tensor([0.3177], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3177, 0.9710, 0.3047], dtype=torch.float64)\n",
      "tensor([0.3836], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3836, 0.9550, 0.2969], dtype=torch.float64)\n",
      "tensor([0.3512], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3512, 0.9697, 0.2891], dtype=torch.float64)\n",
      "tensor([0.3881], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3881, 0.9737, 0.2812], dtype=torch.float64)\n",
      "tensor([0.4119], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4119, 0.9917, 0.2734], dtype=torch.float64)\n",
      "tensor([0.4772], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4772, 0.9897, 0.2656], dtype=torch.float64)\n",
      "tensor([0.4903], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4903, 1.0127, 0.2578], dtype=torch.float64)\n",
      "tensor([0.5676], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5676, 1.0055, 0.2500], dtype=torch.float64)\n",
      "tensor([0.5675], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5675, 1.0051, 0.2422], dtype=torch.float64)\n",
      "tensor([0.5652], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5652, 1.0072, 0.2344], dtype=torch.float64)\n",
      "tensor([0.5708], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5708, 0.9949, 0.2266], dtype=torch.float64)\n",
      "tensor([0.5322], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5322, 1.0071, 0.2188], dtype=torch.float64)\n",
      "tensor([0.5586], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5586, 1.0184, 0.2109], dtype=torch.float64)\n",
      "tensor([0.6023], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6023, 1.0042, 0.2031], dtype=torch.float64)\n",
      "tensor([0.5694], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5694, 1.0162, 0.1953], dtype=torch.float64)\n",
      "tensor([0.5970], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5970, 1.0202, 0.1875], dtype=torch.float64)\n",
      "tensor([0.6177], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6177, 1.0168, 0.1797], dtype=torch.float64)\n",
      "tensor([0.6122], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6122, 1.0289, 0.1719], dtype=torch.float64)\n",
      "tensor([0.6487], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6487, 1.0376, 0.1641], dtype=torch.float64)\n",
      "tensor([0.6870], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6870, 1.0569, 0.1562], dtype=torch.float64)\n",
      "tensor([0.7491], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7491, 1.0481, 0.1484], dtype=torch.float64)\n",
      "tensor([0.7440], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7440, 1.0436, 0.1406], dtype=torch.float64)\n",
      "tensor([0.7324], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7324, 1.0393, 0.1328], dtype=torch.float64)\n",
      "tensor([0.7150], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7150, 1.0335, 0.1250], dtype=torch.float64)\n",
      "tensor([0.6902], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6902, 1.0501, 0.1172], dtype=torch.float64)\n",
      "tensor([0.7347], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7347, 1.0454, 0.1094], dtype=torch.float64)\n",
      "tensor([0.7330], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7330, 1.0210, 0.1016], dtype=torch.float64)\n",
      "tensor([0.6536], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6536, 1.0290, 0.0938], dtype=torch.float64)\n",
      "tensor([0.6541], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6541, 1.0421, 0.0859], dtype=torch.float64)\n",
      "tensor([0.6957], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6957, 1.0294, 0.0781], dtype=torch.float64)\n",
      "tensor([0.6669], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6669, 1.0107, 0.0703], dtype=torch.float64)\n",
      "tensor([0.5973], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5973, 1.0025, 0.0625], dtype=torch.float64)\n",
      "tensor([0.5492], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5492, 1.0091, 0.0547], dtype=torch.float64)\n",
      "tensor([0.5550], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5550, 1.0100, 0.0469], dtype=torch.float64)\n",
      "tensor([0.5588], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5588, 1.0104, 0.0391], dtype=torch.float64)\n",
      "tensor([0.5604], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5604, 1.0053, 0.0312], dtype=torch.float64)\n",
      "tensor([0.5440], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5440, 1.0375, 0.0234], dtype=torch.float64)\n",
      "tensor([0.6414], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6414, 1.0405, 0.0156], dtype=torch.float64)\n",
      "tensor([0.6800], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6800, 1.0405, 0.0078], dtype=torch.float64)\n",
      "tensor([0.6912], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.0098884957610105e-06 5.37537747291864e-07\n",
      "loss: [20.25550562]\n",
      "episode:  741\n",
      "loss: [16.3031675]\n",
      "episode:  742\n",
      "loss: [14.57791775]\n",
      "episode:  743\n",
      "loss: [14.99651183]\n",
      "episode:  744\n",
      "loss: [16.49141455]\n",
      "episode:  745\n",
      "loss: [16.45997]\n",
      "episode:  746\n",
      "loss: [16.19524828]\n",
      "episode:  747\n",
      "loss: [13.89293624]\n",
      "episode:  748\n",
      "loss: [19.55037222]\n",
      "episode:  749\n",
      "loss: [15.96508495]\n",
      "episode:  750\n",
      "loss: [13.04393322]\n",
      "episode:  751\n",
      "loss: [10.41089306]\n",
      "episode:  752\n",
      "loss: [17.95125019]\n",
      "episode:  753\n",
      "loss: [13.39039888]\n",
      "episode:  754\n",
      "loss: [11.45305637]\n",
      "episode:  755\n",
      "loss: [15.43669116]\n",
      "episode:  756\n",
      "loss: [11.06020232]\n",
      "episode:  757\n",
      "loss: [15.33101121]\n",
      "episode:  758\n",
      "loss: [11.91529459]\n",
      "episode:  759\n",
      "loss: [15.44820896]\n",
      "episode:  760\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3353], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3353, 0.9898, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5191], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5191, 1.0062, 0.9844], dtype=torch.float64)\n",
      "tensor([0.5944], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5944, 1.0078, 0.9766], dtype=torch.float64)\n",
      "tensor([0.6103], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6103, 0.9926, 0.9688], dtype=torch.float64)\n",
      "tensor([0.5812], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5812, 1.0123, 0.9609], dtype=torch.float64)\n",
      "tensor([0.6144], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6144, 0.9994, 0.9531], dtype=torch.float64)\n",
      "tensor([0.5955], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5955, 0.9949, 0.9453], dtype=torch.float64)\n",
      "tensor([0.5796], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5796, 1.0284, 0.9375], dtype=torch.float64)\n",
      "tensor([0.6413], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6413, 1.0401, 0.9297], dtype=torch.float64)\n",
      "tensor([0.6736], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6736, 1.0350, 0.9219], dtype=torch.float64)\n",
      "tensor([0.6690], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6690, 1.0258, 0.9141], dtype=torch.float64)\n",
      "tensor([0.6500], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6500, 1.0282, 0.9062], dtype=torch.float64)\n",
      "tensor([0.6502], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6502, 1.0213, 0.8984], dtype=torch.float64)\n",
      "tensor([0.6364], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6364, 0.9986, 0.8906], dtype=torch.float64)\n",
      "tensor([0.5873], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5873, 0.9988, 0.8828], dtype=torch.float64)\n",
      "tensor([0.5756], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5756, 0.9771, 0.8750], dtype=torch.float64)\n",
      "tensor([0.5105], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5105, 0.9879, 0.8672], dtype=torch.float64)\n",
      "tensor([0.5253], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5253, 0.9963, 0.8594], dtype=torch.float64)\n",
      "tensor([0.5503], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5503, 0.9864, 0.8516], dtype=torch.float64)\n",
      "tensor([0.5265], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5265, 0.9706, 0.8438], dtype=torch.float64)\n",
      "tensor([0.4746], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4746, 0.9738, 0.8359], dtype=torch.float64)\n",
      "tensor([0.4708], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4708, 0.9860, 0.8281], dtype=torch.float64)\n",
      "tensor([0.5031], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5031, 0.9792, 0.8203], dtype=torch.float64)\n",
      "tensor([0.4890], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4890, 0.9866, 0.8125], dtype=torch.float64)\n",
      "tensor([0.5054], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5054, 0.9953, 0.8047], dtype=torch.float64)\n",
      "tensor([0.5318], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5318, 0.9834, 0.7969], dtype=torch.float64)\n",
      "tensor([0.5023], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5023, 0.9634, 0.7891], dtype=torch.float64)\n",
      "tensor([0.4372], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4372, 0.9561, 0.7812], dtype=torch.float64)\n",
      "tensor([0.4006], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.4006, 0.9404, 0.7734], dtype=torch.float64)\n",
      "tensor([0.3464], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3464, 0.9435, 0.7656], dtype=torch.float64)\n",
      "tensor([0.3416], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3416, 0.9404, 0.7578], dtype=torch.float64)\n",
      "tensor([0.3302], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3302, 0.9314, 0.7500], dtype=torch.float64)\n",
      "tensor([0.3004], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3004, 0.9202, 0.7422], dtype=torch.float64)\n",
      "tensor([0.2604], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2604, 0.9072, 0.7344], dtype=torch.float64)\n",
      "tensor([0.2129], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2129, 0.9107, 0.7266], dtype=torch.float64)\n",
      "tensor([0.2092], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2092, 0.9203, 0.7188], dtype=torch.float64)\n",
      "tensor([0.2346], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2346, 0.9194, 0.7109], dtype=torch.float64)\n",
      "tensor([0.2369], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2369, 0.9309, 0.7031], dtype=torch.float64)\n",
      "tensor([0.2685], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2685, 0.9433, 0.6953], dtype=torch.float64)\n",
      "tensor([0.3112], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3112, 0.9490, 0.6875], dtype=torch.float64)\n",
      "tensor([0.3390], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3390, 0.9502, 0.6797], dtype=torch.float64)\n",
      "tensor([0.3496], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3496, 0.9399, 0.6719], dtype=torch.float64)\n",
      "tensor([0.3204], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3204, 0.9356, 0.6641], dtype=torch.float64)\n",
      "tensor([0.3000], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3000, 0.9478, 0.6562], dtype=torch.float64)\n",
      "tensor([0.3326], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3326, 0.9407, 0.6484], dtype=torch.float64)\n",
      "tensor([0.3185], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3185, 0.9225, 0.6406], dtype=torch.float64)\n",
      "tensor([0.2582], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2582, 0.9170, 0.6328], dtype=torch.float64)\n",
      "tensor([0.2209], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.2209, 0.8997, 0.6250], dtype=torch.float64)\n",
      "tensor([0.1541], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1541, 0.8863, 0.6172], dtype=torch.float64)\n",
      "tensor([0.1066], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1066, 0.8884, 0.6094], dtype=torch.float64)\n",
      "tensor([0.1018], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1018, 0.8977, 0.6016], dtype=torch.float64)\n",
      "tensor([0.1178], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1178, 0.8897, 0.5938], dtype=torch.float64)\n",
      "tensor([0.1054], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1054, 0.8803, 0.5859], dtype=torch.float64)\n",
      "tensor([0.0856], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0856, 0.8877, 0.5781], dtype=torch.float64)\n",
      "tensor([0.0954], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0954, 0.8810, 0.5703], dtype=torch.float64)\n",
      "tensor([0.0845], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0845, 0.8789, 0.5625], dtype=torch.float64)\n",
      "tensor([0.0785], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0785, 0.9095, 0.5547], dtype=torch.float64)\n",
      "tensor([0.1379], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1379, 0.9044, 0.5469], dtype=torch.float64)\n",
      "tensor([0.1389], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.1389, 0.8768, 0.5391], dtype=torch.float64)\n",
      "tensor([0.0830], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0830, 0.8685, 0.5312], dtype=torch.float64)\n",
      "tensor([0.0735], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0735, 0.8511, 0.5234], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8475, 0.5156], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8343, 0.5078], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8367, 0.5000], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8161, 0.4922], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8235, 0.4844], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8069, 0.4766], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8100, 0.4688], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8156, 0.4609], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0732, 0.8139, 0.4531], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8357, 0.4453], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8252, 0.4375], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8313, 0.4297], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8357, 0.4219], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8418, 0.4141], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8417, 0.4062], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8300, 0.3984], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8286, 0.3906], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8379, 0.3828], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8307, 0.3750], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8270, 0.3672], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8240, 0.3594], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8407, 0.3516], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8408, 0.3438], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8605, 0.3359], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8634, 0.3281], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8532, 0.3203], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8512, 0.3125], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8526, 0.3047], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8527, 0.2969], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8592, 0.2891], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8262, 0.2812], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8077, 0.2734], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8087, 0.2656], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8115, 0.2578], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8172, 0.2500], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8176, 0.2422], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8172, 0.2344], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8078, 0.2266], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8113, 0.2188], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.8089, 0.2109], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7908, 0.2031], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7805, 0.1953], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7898, 0.1875], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7742, 0.1797], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7659, 0.1719], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7714, 0.1641], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7770, 0.1562], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7787, 0.1484], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7924, 0.1406], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7828, 0.1328], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7908, 0.1250], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7944, 0.1172], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7918, 0.1094], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7936, 0.1016], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7974, 0.0938], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7962, 0.0859], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7709, 0.0781], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7792, 0.0703], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7792, 0.0625], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7837, 0.0547], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7708, 0.0469], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7530, 0.0391], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7633, 0.0312], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7683, 0.0234], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7616, 0.0156], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.0732, 0.7699, 0.0078], dtype=torch.float64)\n",
      "tensor([0.0732], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  1.3342597873681313e-06 5.558851658531152e-07\n",
      "loss: [16.85148917]\n",
      "episode:  761\n",
      "loss: [22.40379115]\n",
      "episode:  762\n",
      "loss: [14.70757543]\n",
      "episode:  763\n",
      "loss: [11.82104092]\n",
      "episode:  764\n",
      "loss: [18.09053166]\n",
      "episode:  765\n",
      "loss: [13.98646442]\n",
      "episode:  766\n",
      "loss: [13.92314672]\n",
      "episode:  767\n",
      "loss: [18.24921172]\n",
      "episode:  768\n",
      "loss: [15.88488094]\n",
      "episode:  769\n",
      "loss: [15.62851747]\n",
      "episode:  770\n",
      "loss: [13.63421857]\n",
      "episode:  771\n",
      "loss: [16.15835363]\n",
      "episode:  772\n",
      "loss: [14.99188866]\n",
      "episode:  773\n",
      "loss: [15.56339262]\n",
      "episode:  774\n",
      "loss: [12.45283923]\n",
      "episode:  775\n",
      "loss: [15.28016801]\n",
      "episode:  776\n",
      "loss: [16.19012475]\n",
      "episode:  777\n",
      "loss: [14.8115954]\n",
      "episode:  778\n",
      "loss: [10.84513511]\n",
      "episode:  779\n",
      "loss: [13.39715169]\n",
      "episode:  780\n",
      "tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.3353], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.3353, 0.9995, 0.9922], dtype=torch.float64)\n",
      "tensor([0.5463], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.5463, 1.0208, 0.9844], dtype=torch.float64)\n",
      "tensor([0.6266], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6266, 1.0322, 0.9766], dtype=torch.float64)\n",
      "tensor([0.6618], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6618, 1.0393, 0.9688], dtype=torch.float64)\n",
      "tensor([0.6805], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6805, 1.0500, 0.9609], dtype=torch.float64)\n",
      "tensor([0.7030], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7030, 1.0687, 0.9531], dtype=torch.float64)\n",
      "tensor([0.7417], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7417, 1.0616, 0.9453], dtype=torch.float64)\n",
      "tensor([0.7346], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7346, 1.0493, 0.9375], dtype=torch.float64)\n",
      "tensor([0.7089], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7089, 1.0733, 0.9297], dtype=torch.float64)\n",
      "tensor([0.7482], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7482, 1.0740, 0.9219], dtype=torch.float64)\n",
      "tensor([0.7560], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7560, 1.0669, 0.9141], dtype=torch.float64)\n",
      "tensor([0.7429], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7429, 1.0741, 0.9062], dtype=torch.float64)\n",
      "tensor([0.7544], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7544, 1.0564, 0.8984], dtype=torch.float64)\n",
      "tensor([0.7210], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7210, 1.0750, 0.8906], dtype=torch.float64)\n",
      "tensor([0.7526], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7526, 1.0842, 0.8828], dtype=torch.float64)\n",
      "tensor([0.7779], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7779, 1.1019, 0.8750], dtype=torch.float64)\n",
      "tensor([0.8157], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8157, 1.0872, 0.8672], dtype=torch.float64)\n",
      "tensor([0.7958], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7958, 1.0712, 0.8594], dtype=torch.float64)\n",
      "tensor([0.7619], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7619, 1.0847, 0.8516], dtype=torch.float64)\n",
      "tensor([0.7820], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7820, 1.0874, 0.8438], dtype=torch.float64)\n",
      "tensor([0.7910], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7910, 1.0878, 0.8359], dtype=torch.float64)\n",
      "tensor([0.7939], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7939, 1.0748, 0.8281], dtype=torch.float64)\n",
      "tensor([0.7707], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7707, 1.0722, 0.8203], dtype=torch.float64)\n",
      "tensor([0.7611], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7611, 1.0995, 0.8125], dtype=torch.float64)\n",
      "tensor([0.8111], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8111, 1.0849, 0.8047], dtype=torch.float64)\n",
      "tensor([0.7938], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7938, 1.0919, 0.7969], dtype=torch.float64)\n",
      "tensor([0.8040], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8040, 1.1045, 0.7891], dtype=torch.float64)\n",
      "tensor([0.8295], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8295, 1.1171, 0.7812], dtype=torch.float64)\n",
      "tensor([0.8517], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8517, 1.1081, 0.7734], dtype=torch.float64)\n",
      "tensor([0.8453], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8453, 1.1075, 0.7656], dtype=torch.float64)\n",
      "tensor([0.8438], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8438, 1.0996, 0.7578], dtype=torch.float64)\n",
      "tensor([0.8294], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8294, 1.0917, 0.7500], dtype=torch.float64)\n",
      "tensor([0.8124], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8124, 1.0810, 0.7422], dtype=torch.float64)\n",
      "tensor([0.7899], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7899, 1.0874, 0.7344], dtype=torch.float64)\n",
      "tensor([0.7980], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7980, 1.0901, 0.7266], dtype=torch.float64)\n",
      "tensor([0.8050], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8050, 1.0940, 0.7188], dtype=torch.float64)\n",
      "tensor([0.8137], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8137, 1.0858, 0.7109], dtype=torch.float64)\n",
      "tensor([0.8006], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8006, 1.0878, 0.7031], dtype=torch.float64)\n",
      "tensor([0.8024], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8024, 1.0825, 0.6953], dtype=torch.float64)\n",
      "tensor([0.7932], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7932, 1.0738, 0.6875], dtype=torch.float64)\n",
      "tensor([0.7758], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7758, 1.0627, 0.6797], dtype=torch.float64)\n",
      "tensor([0.7502], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7502, 1.0381, 0.6719], dtype=torch.float64)\n",
      "tensor([0.6944], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6944, 1.0618, 0.6641], dtype=torch.float64)\n",
      "tensor([0.7325], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7325, 1.0723, 0.6562], dtype=torch.float64)\n",
      "tensor([0.7624], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7624, 1.0895, 0.6484], dtype=torch.float64)\n",
      "tensor([0.8012], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8012, 1.0869, 0.6406], dtype=torch.float64)\n",
      "tensor([0.8039], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8039, 1.0939, 0.6328], dtype=torch.float64)\n",
      "tensor([0.8176], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8176, 1.0845, 0.6250], dtype=torch.float64)\n",
      "tensor([0.8031], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8031, 1.0781, 0.6172], dtype=torch.float64)\n",
      "tensor([0.7891], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7891, 1.0786, 0.6094], dtype=torch.float64)\n",
      "tensor([0.7878], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7878, 1.0880, 0.6016], dtype=torch.float64)\n",
      "tensor([0.8053], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8053, 1.0742, 0.5938], dtype=torch.float64)\n",
      "tensor([0.7835], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7835, 1.0571, 0.5859], dtype=torch.float64)\n",
      "tensor([0.7454], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7454, 1.0520, 0.5781], dtype=torch.float64)\n",
      "tensor([0.7275], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7275, 1.0597, 0.5703], dtype=torch.float64)\n",
      "tensor([0.7401], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7401, 1.0512, 0.5625], dtype=torch.float64)\n",
      "tensor([0.7257], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7257, 1.0709, 0.5547], dtype=torch.float64)\n",
      "tensor([0.7639], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7639, 1.0745, 0.5469], dtype=torch.float64)\n",
      "tensor([0.7787], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7787, 1.0677, 0.5391], dtype=torch.float64)\n",
      "tensor([0.7690], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7690, 1.0660, 0.5312], dtype=torch.float64)\n",
      "tensor([0.7640], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7640, 1.0594, 0.5234], dtype=torch.float64)\n",
      "tensor([0.7496], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7496, 1.0423, 0.5156], dtype=torch.float64)\n",
      "tensor([0.7116], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7116, 1.0453, 0.5078], dtype=torch.float64)\n",
      "tensor([0.7105], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7105, 1.0377, 0.5000], dtype=torch.float64)\n",
      "tensor([0.6950], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6950, 1.0362, 0.4922], dtype=torch.float64)\n",
      "tensor([0.6890], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6890, 1.0283, 0.4844], dtype=torch.float64)\n",
      "tensor([0.6719], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6719, 1.0419, 0.4766], dtype=torch.float64)\n",
      "tensor([0.6971], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6971, 1.0276, 0.4688], dtype=torch.float64)\n",
      "tensor([0.6731], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.6731, 1.0520, 0.4609], dtype=torch.float64)\n",
      "tensor([0.7190], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7190, 1.0551, 0.4531], dtype=torch.float64)\n",
      "tensor([0.7354], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7354, 1.0545, 0.4453], dtype=torch.float64)\n",
      "tensor([0.7380], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7380, 1.0540, 0.4375], dtype=torch.float64)\n",
      "tensor([0.7378], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7378, 1.0598, 0.4297], dtype=torch.float64)\n",
      "tensor([0.7504], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7504, 1.0781, 0.4219], dtype=torch.float64)\n",
      "tensor([0.7891], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.7891, 1.0901, 0.4141], dtype=torch.float64)\n",
      "tensor([0.8187], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8187, 1.0939, 0.4062], dtype=torch.float64)\n",
      "tensor([0.8317], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8317, 1.1207, 0.3984], dtype=torch.float64)\n",
      "tensor([0.8655], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8655, 1.1290, 0.3906], dtype=torch.float64)\n",
      "tensor([0.8770], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8770, 1.1420, 0.3828], dtype=torch.float64)\n",
      "tensor([0.8910], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8910, 1.1458, 0.3750], dtype=torch.float64)\n",
      "tensor([0.8963], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.8963, 1.1706, 0.3672], dtype=torch.float64)\n",
      "tensor([0.9211], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9211, 1.1692, 0.3594], dtype=torch.float64)\n",
      "tensor([0.9223], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9223, 1.1920, 0.3516], dtype=torch.float64)\n",
      "tensor([0.9449], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9449, 1.1699, 0.3438], dtype=torch.float64)\n",
      "tensor([0.9251], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9251, 1.1586, 0.3359], dtype=torch.float64)\n",
      "tensor([0.9128], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9128, 1.1483, 0.3281], dtype=torch.float64)\n",
      "tensor([0.9020], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9020, 1.1563, 0.3203], dtype=torch.float64)\n",
      "tensor([0.9089], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9089, 1.1553, 0.3125], dtype=torch.float64)\n",
      "tensor([0.9088], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9088, 1.1687, 0.3047], dtype=torch.float64)\n",
      "tensor([0.9219], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9219, 1.1634, 0.2969], dtype=torch.float64)\n",
      "tensor([0.9178], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9178, 1.1752, 0.2891], dtype=torch.float64)\n",
      "tensor([0.9291], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9291, 1.1764, 0.2812], dtype=torch.float64)\n",
      "tensor([0.9311], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9311, 1.1799, 0.2734], dtype=torch.float64)\n",
      "tensor([0.9347], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9347, 1.1865, 0.2656], dtype=torch.float64)\n",
      "tensor([0.9414], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9414, 1.2184, 0.2578], dtype=torch.float64)\n",
      "tensor([0.9731], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9731, 1.2196, 0.2500], dtype=torch.float64)\n",
      "tensor([0.9765], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9765, 1.2212, 0.2422], dtype=torch.float64)\n",
      "tensor([0.9784], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9784, 1.2379, 0.2344], dtype=torch.float64)\n",
      "tensor([0.9948], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9948, 1.2360, 0.2266], dtype=torch.float64)\n",
      "tensor([0.9942], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9942, 1.2304, 0.2188], dtype=torch.float64)\n",
      "tensor([0.9887], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9887, 1.2467, 0.2109], dtype=torch.float64)\n",
      "tensor([1.0044], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0044, 1.2560, 0.2031], dtype=torch.float64)\n",
      "tensor([1.0147], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0147, 1.2540, 0.1953], dtype=torch.float64)\n",
      "tensor([1.0135], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0135, 1.2499, 0.1875], dtype=torch.float64)\n",
      "tensor([1.0095], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0095, 1.2362, 0.1797], dtype=torch.float64)\n",
      "tensor([0.9959], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9959, 1.2600, 0.1719], dtype=torch.float64)\n",
      "tensor([1.0183], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0183, 1.2348, 0.1641], dtype=torch.float64)\n",
      "tensor([0.9953], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9953, 1.2537, 0.1562], dtype=torch.float64)\n",
      "tensor([1.0123], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0123, 1.2699, 0.1484], dtype=torch.float64)\n",
      "tensor([1.0293], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0293, 1.2708, 0.1406], dtype=torch.float64)\n",
      "tensor([1.0314], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0314, 1.2460, 0.1328], dtype=torch.float64)\n",
      "tensor([1.0075], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0075, 1.2425, 0.1250], dtype=torch.float64)\n",
      "tensor([1.0026], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0026, 1.2287, 0.1172], dtype=torch.float64)\n",
      "tensor([0.9889], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9889, 1.2320, 0.1094], dtype=torch.float64)\n",
      "tensor([0.9913], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9913, 1.2167, 0.1016], dtype=torch.float64)\n",
      "tensor([0.9766], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9766, 1.2344, 0.0938], dtype=torch.float64)\n",
      "tensor([0.9930], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9930, 1.2412, 0.0859], dtype=torch.float64)\n",
      "tensor([1.0008], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0008, 1.2538, 0.0781], dtype=torch.float64)\n",
      "tensor([1.0137], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([1.0137, 1.2131, 0.0703], dtype=torch.float64)\n",
      "tensor([0.9749], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9749, 1.2191, 0.0625], dtype=torch.float64)\n",
      "tensor([0.9783], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9783, 1.2136, 0.0547], dtype=torch.float64)\n",
      "tensor([0.9733], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9733, 1.2010, 0.0469], dtype=torch.float64)\n",
      "tensor([0.9608], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9608, 1.1779, 0.0391], dtype=torch.float64)\n",
      "tensor([0.9374], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9374, 1.1949, 0.0312], dtype=torch.float64)\n",
      "tensor([0.9526], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9526, 1.2021, 0.0234], dtype=torch.float64)\n",
      "tensor([0.9607], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9607, 1.1967, 0.0156], dtype=torch.float64)\n",
      "tensor([0.9561], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([0.9561, 1.2044, 0.0078], dtype=torch.float64)\n",
      "tensor([0.9633], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "test_result:  9.019351401227029e-07 5.387816397449912e-07\n",
      "loss: [13.34299458]\n",
      "episode:  781\n",
      "loss: [12.01641347]\n",
      "episode:  782\n",
      "loss: [15.70685525]\n",
      "episode:  783\n",
      "loss: [12.13961397]\n",
      "episode:  784\n",
      "loss: [12.59267742]\n",
      "episode:  785\n",
      "loss: [12.33879612]\n",
      "episode:  786\n",
      "loss: [16.07848009]\n",
      "episode:  787\n",
      "loss: [13.60434302]\n",
      "episode:  788\n",
      "loss: [20.10725987]\n",
      "episode:  789\n",
      "loss: [15.37176828]\n",
      "episode:  790\n",
      "loss: [13.43462248]\n",
      "episode:  791\n",
      "loss: [14.3744148]\n",
      "episode:  792\n",
      "loss: [14.38694665]\n",
      "episode:  793\n",
      "loss: [12.15967865]\n",
      "episode:  794\n",
      "loss: [13.61592103]\n",
      "episode:  795\n",
      "loss: [15.65881203]\n",
      "episode:  796\n",
      "loss: [12.50601009]\n",
      "episode:  797\n",
      "loss: [16.52941211]\n",
      "episode:  798\n",
      "loss: [14.65132717]\n",
      "episode:  799\n",
      "loss: [15.79906819]\n",
      "[7.040437300994911e-05, 1.5439828849438335e-05, 1.6317819685764417e-05, 1.517092814483762e-05, 1.7057597142239386e-05, 1.6538777006136722e-05, 1.2317469984923952e-05, 1.0676666278434035e-05, 5.1738718282718396e-06, 3.73697359201294e-06, 2.233289686098265e-06, 7.757069185611964e-06, 1.4607591117994787e-06, 1.839622916122992e-06, 2.8995275063102885e-06, 1.3945548903381885e-06, 1.1636412784827283e-06, 1.1498938026172045e-06, 1.3110746643984342e-06, 9.907781776985475e-07, 9.389030241218405e-07, 1.2327110548078983e-06, 1.1687066804841264e-06, 1.143851201128583e-06, 1.1345340944343285e-06, 1.1672801331643367e-06, 1.0379932397602214e-06, 1.2825378366395432e-06, 1.2056793133030061e-06, 1.2191772077753457e-06, 1.2245672316444167e-06, 1.3711056667786908e-06, 1.1820310409578912e-06, 1.0954486773600324e-06, 1.15028532115061e-06, 1.1820244832312547e-06, 9.840226238711097e-07, 1.0098884957610105e-06, 1.3342597873681313e-06, 9.019351401227029e-07]\n",
      "[5.745904537411398e-07, 6.07355110863272e-07, 6.081765725673269e-07, 6.9031849806856e-07, 5.376053092067889e-07, 5.198345062331744e-07, 6.422673036252319e-07, 6.047248011322186e-07, 5.629428410339467e-07, 5.673332194910931e-07, 6.395521138966553e-07, 7.337186046363486e-07, 5.703076108150347e-07, 7.208335590683857e-07, 6.900545603696568e-07, 6.72090610065544e-07, 5.641684648420856e-07, 5.608332272190458e-07, 6.113492714845925e-07, 5.778185205200171e-07, 5.797743288431022e-07, 6.937878762530069e-07, 6.6811100966529e-07, 5.965282020474578e-07, 7.501388483933532e-07, 6.672548834503553e-07, 7.11961365604363e-07, 7.999931626317338e-07, 6.849048009239764e-07, 7.098753293919155e-07, 7.553945254190573e-07, 5.70442568319175e-07, 6.721850567486784e-07, 5.141550292273862e-07, 5.929072725247e-07, 6.39489776255882e-07, 5.453251890727525e-07, 5.37537747291864e-07, 5.558851658531152e-07, 5.387816397449912e-07]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(num_eps):\n",
    "    print(\"episode: \", i)\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        losses, delta_losses, test_result, delta_test_results = test(model, apm, opm, num_steps=num_steps, dt=dt, n=20)\n",
    "        print(\"test_result: \", test_result, delta_test_results)\n",
    "        test_res.append(test_result)\n",
    "        test_res_delta.append(delta_test_results)\n",
    "\n",
    "    if i == 300:\n",
    "        for q in optimizer.param_groups:\n",
    "            q[\"lr\"] = 0.001\n",
    "\n",
    "    if i == 400:\n",
    "        for q in optimizer.param_groups:\n",
    "            q[\"lr\"] = 0.0001\n",
    "\n",
    "    if i == 500:\n",
    "        for q in optimizer.param_groups:\n",
    "            q[\"lr\"] = 0.00001\n",
    "\n",
    "    if i == 600:\n",
    "        for q in optimizer.param_groups:\n",
    "            q[\"lr\"] = 0.000001\n",
    "\n",
    "    if i == 700:\n",
    "        for q in optimizer.param_groups:\n",
    "            q[\"lr\"] = 0.0000001\n",
    "\n",
    "    loss = 0\n",
    "    for _ in range(10):\n",
    "        D = generate_data(apm, opm, num_steps, dt, n=1)\n",
    "        old_out = 0\n",
    "        for tupel in D:\n",
    "            inp = torch.tensor(np.array([old_out, tupel[\"p\"], tupel[\"ttm\"]]), dtype=torch.float64)\n",
    "            out = model(inp)\n",
    "            trading_costs = (T / num_steps) * (abs(old_out - out) + 0.01 * (old_out - out) ** 2)\n",
    "            pl = (-tupel[\"nop\"] + tupel[\"op\"]) + out * (tupel[\"np\"] - tupel[\"p\"]) - trading_costs\n",
    "            loss += norm_factor * (torch.pow(pl, 2) - 1 / 1000 * pl)\n",
    "            old_out = out.detach().numpy()[0]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"loss:\", loss.detach().numpy())\n",
    "\n",
    "print(test_res)\n",
    "print(test_res_delta)\n",
    "plt.plot(test_res, label=\"NN\")\n",
    "plt.plot(test_res_delta, label=\"delta\")\n",
    "# plt.xlabel(\"\")\n",
    "plt.xlabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"test_deep_classic_new_no_delta_losses_final_new_2_cost_1000.png\")\n",
    "\n",
    "torch.save(model, \"model_classic_final_new_2_cost_1000.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00fd6ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.040437300994911e-05, 1.5439828849438335e-05, 1.6317819685764417e-05, 1.517092814483762e-05, 1.7057597142239386e-05, 1.6538777006136722e-05, 1.2317469984923952e-05, 1.0676666278434035e-05, 5.1738718282718396e-06, 3.73697359201294e-06, 2.233289686098265e-06, 7.757069185611964e-06, 1.4607591117994787e-06, 1.839622916122992e-06, 2.8995275063102885e-06, 1.3945548903381885e-06, 1.1636412784827283e-06, 1.1498938026172045e-06, 1.3110746643984342e-06, 9.907781776985475e-07, 9.389030241218405e-07, 1.2327110548078983e-06, 1.1687066804841264e-06, 1.143851201128583e-06, 1.1345340944343285e-06, 1.1672801331643367e-06, 1.0379932397602214e-06, 1.2825378366395432e-06, 1.2056793133030061e-06, 1.2191772077753457e-06, 1.2245672316444167e-06, 1.3711056667786908e-06, 1.1820310409578912e-06, 1.0954486773600324e-06, 1.15028532115061e-06, 1.1820244832312547e-06, 9.840226238711097e-07, 1.0098884957610105e-06, 1.3342597873681313e-06, 9.019351401227029e-07]\n",
      "[5.745904537411398e-07, 6.07355110863272e-07, 6.081765725673269e-07, 6.9031849806856e-07, 5.376053092067889e-07, 5.198345062331744e-07, 6.422673036252319e-07, 6.047248011322186e-07, 5.629428410339467e-07, 5.673332194910931e-07, 6.395521138966553e-07, 7.337186046363486e-07, 5.703076108150347e-07, 7.208335590683857e-07, 6.900545603696568e-07, 6.72090610065544e-07, 5.641684648420856e-07, 5.608332272190458e-07, 6.113492714845925e-07, 5.778185205200171e-07, 5.797743288431022e-07, 6.937878762530069e-07, 6.6811100966529e-07, 5.965282020474578e-07, 7.501388483933532e-07, 6.672548834503553e-07, 7.11961365604363e-07, 7.999931626317338e-07, 6.849048009239764e-07, 7.098753293919155e-07, 7.553945254190573e-07, 5.70442568319175e-07, 6.721850567486784e-07, 5.141550292273862e-07, 5.929072725247e-07, 6.39489776255882e-07, 5.453251890727525e-07, 5.37537747291864e-07, 5.558851658531152e-07, 5.387816397449912e-07]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(test_res)\n",
    "print(test_res_delta)\n",
    "plt.plot(np.log(test_res), label=\"NN\")\n",
    "plt.plot(np.log(test_res_delta), label=\"delta\")\n",
    "# plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(\"Number of epochs (*20)\")\n",
    "plt.legend()\n",
    "plt.savefig(\"test_deep_classic_new_no_delta_losses_final_new_2_cost_1000.png\")\n",
    "\n",
    "torch.save(model, \"model_classic_final_new_2_cost_1000.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "622f7a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABGqElEQVR4nO3deXxU1dnA8d/JZCOQhSwEkpAEEsIadlBBFItsbkhx3+pKrdq3tr5VW6svWrW12ta6Va24tFpBcQNRUXYRFwKGNQSSsGUBEhKykD1z3j/OJGTPkMxkMsnz/Xzyycy95955LsuTk+eee47SWiOEEML9ebg6ACGEEI4hCV0IIboJSehCCNFNSEIXQohuQhK6EEJ0E56u+uDQ0FAdGxvrqo8XQgi3tHXr1jytdVhz+1yW0GNjY0lKSnLVxwshhFtSSh1qaZ+UXIQQopuQhC6EEN2EJHQhhOgmXFZDF0KIM1FVVUVmZibl5eWuDqVT+Pr6EhUVhZeXl93HSEIXQriFzMxM/P39iY2NRSnl6nCcSmvNiRMnyMzMZNCgQXYfJyUXIYRbKC8vJyQkpNsncwClFCEhIWf824gkdCGE2+gJybxWe65VSi5dXO6pXP6z4z/EB8czrv84BgYOdHVIQoguyq4eulJqjlIqVSmVppR6sJn9v1VKJdu+dimlapRSwY4Pt+dZ8e4iYm67j+vfmkf0s9G8nPSyq0MSosdSSnHffffVvX/mmWdYtGgRAIsWLcLPz4/jx4/X7e/Tp0+nxtdmQldKWYAXgbnACOBapdSI+m201k9rrcdqrccCvwM2aK3znRCv20s+msybyW9i78IifT9bx4IU2HfsKs6JOofHNz5OVU2Vk6MUQjTHx8eHDz/8kLy8vGb3h4aG8te//rWTozrNnh76ZCBNa52hta4ElgDzWml/LfCuI4LrTvbm7WX+0vmMe2Uct3xyC7uO77LruD4ZmQAMeHMZT/e9mqziLN7f874zQxVCtMDT05OFCxfy97//vdn9t956K0uXLiU/3zX9WXtq6JHAkXrvM4GzmmuolPID5gD3tLB/IbAQIDo6+owCdWdaay5fcjlHS45yz6R7eGHLC3yb+S2J4YmtHldRXcHArGLSx8cSl1PBlCf+zcifDeWv3/6Va0dd26NuEAlR371f3Evy0WSHnnNs/7E8O+fZNtvdfffdjB49mvvvv7/Jvj59+nDrrbfyj3/8g0cffdSh8dnDnh56c1mjpXrBpcA3LZVbtNavaq0naq0nhoU1O1lYt5SSl8LQb1L56sA0ntNzGKKD+Tbz2zaP25u9g/gTUD5hHDz3HGrbNv55dCLbcrbx9eGvOyFyIURjAQEB3HTTTTz33HPN7v+f//kf3nrrLYqKijo5Mvt66JlA/aEVUUB2C22vQcotTXyc8hGvrID+pz6FVz9lh4+F8x/d1HrhCjiUtIYxGoLGnQ0LFsDYsUz9MoWQq0J4ZvMznBdzXudcgBBdjD09aWe69957GT9+PLfcckuTfUFBQVx33XW89NJLnR6XPT30LcAQpdQgpZQ3Jmkvb9xIKRUInA984tgQ3V/yhqX0PwU8+yz885/4VtTQZ08a+WWt19lO/rgZgPDJF4BS8LOf4bF1G4/3v44V+1awJmON84MXQjQRHBzMVVddxeLFi5vd/5vf/IZXXnmF6urqTo2rzYSuta7G1MRXASnAe1rr3UqpO5VSd9ZrOh/4Umt9yjmhupHKSjh5EoDs4mxCvt9ptl9yCcyfD0DiMfg+8/vWz5OSglWB5/CR5v2114LFwm27fBjcdzB3f3Y3FdUVTroIIURr7rvvvlZHu8yfP5+Kik7+/6m1dsnXhAkTdLf14INaR0ZqXV6u/7nln3rJSHRlRLjWVqvWWuuasFC9eBz64bUPt3qaj8b66uP9+jTceNFFWkdF6c/2fqpZhH5i4xPOugohupQ9e/a4OoRO19w1A0m6hbwqj/47wb5vVkBWFhue+18+TvmIGYcteM6YZcomgEfiaCbn92r1xuiJ0hPE5pRTHBfVcMdNN0FmJnOzejF/2Hwe3/g4mUWZzrwcIYSbkITuBDU5WQCUv/ICh7/7ktDiGtT06acbJCYy5GgVWw5/R421ptlz7MxJZlgeeIwY2XDHZZdBQAC89RZPzniSsuoyVqSuaHL8qcpT3LXyLn7M+dFBVyWE6OokoTuBf34JVgWzMhT37+9nNtZP6KNH41NRTdixElLyUpo9x6HkDfjWQNC4cxru6NULrrsO3n2XoTlVRAVEsf7Q+gZNqmqquOL9K/hn0j9ZsmuJ4y5MCNGlSUJvw7acbZyz+BzGvzLervY1NdUEF1eza9pQFHDz2hMwcCDUn9M40TxQlHgMXtv2GuXVTafILEo2N0wDGyd0gMceg8BA1O23c8HA81h/cH3dVAJWbeXW5bfyRdoX9PLsxb78fWd2wUIItyUJvRUPrXmISf+axHeZ3/Hj0R8pqSxp85isrL34VUHp+ESYNQtqauCCC+rq5wCMHIlWigU1Cfzj+38Q91wcb+94u8F5rLvN1ABqRINpc4ywMHj+efjhB+7+porjp46zN28vAK8kvcLbO97m8Qse58Y+U8k4ntr+PwAhhFuRhN6CnOIcntz0JAuGL+Dli80MhwdPHmzzuMz9WwHwjx4Cd9xhNtYvtwD4+aHi4riORNbetJZI/0hu/eRWjpYcBSAlN4WAA9mUhPhDUFDzH3T11XDZZUx6eTkDT1LXS39hywtMCZvA75cd45V7VzP9y/0t1umFEN2LJPQWrDlgHtr53bm/Y9yAcQBkFGS0eVxehhlzHjp4lBlzvnQpXH9904aJiaidO7lg0AW8/dO3qbJW8a+t/wJg8Y+LGZUL3iNHt/xBSsFTT+FRXsHVWUFsOLSBzUc2U5C+h+XPHUc9/zxWD0X88WqOFB1p+TxCiHZZtGgRzzzzjF3733zzTbKzW3rA3nEkobdgdcZqQnyDGRMwhMEVvelVaV9CLz60H4DQQSPBwwOuugq8vZs2TEyEtDQoKyMhJIFZcbN4ZesrlFaV8sEPbzHuqMJ7yrTWP2zoUAgMZE5hKOsPrueVra/w623eBKdlw/LllA2OJrII9p/Y354/AiGEg0hCdyGtNWvTviL5+Wo8+vgTGjeK7a8qDhQcaPPYiqxDAFgiIltvmJgIVit8+y08+ij/+CGErOIsFq5YyNC9eXjWaFN7b41SMHEio49UcezUMd7Z+Q7zToSixo2DSy/Fc2A0kcWw74TcGBXCEZ544gmGDh3KhRdeSGqquT+Vnp7OnDlzmDBhAtOmTWPv3r0Njlm2bBlJSUlcf/31jB07lrKyMh577DEmTZrEqFGjWLhwod3rI7RFlqBrRuqJVCL3ZhOVBdx2Gxw9StxnKzmUl1bX5r87/4unhydXjbyqwbH6aA7VFoVncBsLNo22lVNmzgSrlWHA5P+L5J2d7/BCdh+0VwVq6tS2g504kdC/bcS7GqzKSlxaPtxxBQDe0YOJ2rqJd/Olhy66mXvvheRkx55z7Fgz31ILtm7dypIlS/jxxx+prq5m/PjxTJgwgYULF/Lyyy8zZMgQvv/+e+666y7Wrl1bd9wVV1zBCy+8wDPPPMPEiRMBuOeee3jkkUcAuPHGG/n000+59NJLO3wJktCbsTpjNRftB+3hgXr6aVi6FI+VKyk8cjqh/27N7zhacpSRYSMZ2c88/KO1xjs3n5IgP4I82vjlJy4Oxo2DyEi48Ua4+moePzmOWX2zmJftj5o8Bnr3bjvYiRNRVVVceKofob3DsJTthilTAFCRkYSXaNJyZaSLEB319ddfM3/+fPz8/AC47LLLKC8vZ/PmzVx55ZV17eyZv2XdunX85S9/obS0lPz8fEaOHCkJvb0yizJJzUtlxuAZze5fnbGaxzO8UVMmQ9++0L8/AOVZh9Bak1uai0/6YSIscPMnN/Ptbd/i6eFJbmkuIYXVVIQNaDsIiwW2bTv9/o9/5IItefzynpuI3P82XHmbfRdj+4n/Svgd9MITOJ3QiYzE0woFh/a2fLwQ7qiVnrQzNV5Uxmq1EhQURPIZ/LZQXl7OXXfdRVJSEgMHDmTRokWUlzd9FqU9emQN/aaPbuLSdy/Fqq1N9lVbq9mzYw2jMivhoovMRltCDyqs4NipY2zN3sq/P4J1n/UjKTuJp795GoD0/HT6l5xuf0auvBLPb7/nueKpKKu17fp5rZgYCAkhav9RQpJTISrKPMgEpvcP1Bw5LOuQCtFB5513Hh999BFlZWUUFxezYsUK/Pz8GDRoEO+/b5aF1Fqzffv2Jsf6+/tTXFwMUJe8Q0NDKSkpYdmyZQ6Lsccl9K8Pfc26g+soqy4jqyiryf6k7CTO3W17gOjii8338HAA+pfAgYIDbMnewqCTELv/OHf0m8uiDYtIz08nvcAkdJ/Idiyvd+WVoDX84Q9mVMw5zTwh2hzbjVGSkmDz5tO9c6hL6P0LrRw42fYNXSFEy8aPH8/VV1/N2LFjWbBgAdOmmVFo77zzDosXL2bMmDGMHDmSTz5puiTEzTffzJ133snYsWPx8fHhjjvuIDExkcsvv5xJkyY5LsiWpmF09perps+98N8XarVIaRah1x1Y12T/898/r98fjq6O6F833a0+dUpr0A/OQL+9/W09/62LtDbpV5/865Pa7wk/fc2ya/Si1Q/rKoWu+t0D7Qtu5Ehz3vPPP7PjHnpIa6XMsc8+e3p7drbWoO+6CL0idUX7YhKii5Dpcw1k+lxj85HNrM5YzV2T7gJMiaSxA8f3MSsDPC665PTj+n5+6IAAwkvMWPSsvVvq2gd+sY5fn/1rluxawvqtH+CpwTMiqsl57XKVbcSMveWWWhMnmh8vAPVHxvTrh7ZYZCy6ED1Ej0roj254lFC/UJ6c8SSeHp6kFzRN6D7fJxFQAeqSSxpsV+HhDKrw5Zsj3+B1NNdsHDMG1q/n/lE/J9QvlBMH9pjt7amhgxntEhdn1g89E7Ybo/TqZWKqZbGgBgxgcKm3jEUXogfoMQl9dcZqvkz/kvun3E+ATwCxQbHNJvTeqbZa89lnN9zRvz/RZd6szlhNVO1i3nffDVVVBKz9hofPe9jcELW1bZdBg8zTo6NGndlxkZGmzj9pEnh5NdkXV+Yrsy6KbkE76AEcd9Cea+0RCb3GWsNvVv2G2KBYfnnWL+HwYa7IDGhScrFqK97Hcqn29DAzGtbXvz/hJVCjaxhYbCvFLFhg2n38MXdOvJPZfUbXte1USsGbb8Jf/9p0X2QkEUX2TVsgRFfm6+vLiRMnekRS11pz4sQJfH19z+g4u8ahK6XmAP8ALMBrWus/N9NmOvAs4AXkaa3PP6NInGjxj4vZeXwn71/5Pr6evnDXXTy6egcvL2r44E5OcQ79CmsoC+2Lf+MHg/r3p29hJQCjq4OhT4UZo37ZZfD++3hXa+4bfANwf+cndIA5c5rfHhlJSEE52cXZaK2bjKMVwl1ERUWRmZlJbm6uq0PpFL6+vkRFndn9uDYTulLKArwIzAQygS1KqeVa6z312gQBLwFztNaHlVL9zigKJyqqKOLhdQ8zLXoaC4YvgMOH4fPP8bZa8SgoJL8sn+Be5jH9jIIMIoqhpn940xOFh9OrpByfKhhW7g+RoaZnfM01sHgxvPMOHD1qnu7s06eTr7IVkZH4llbiXQoF5QV11yqEu/Hy8mJQ/YViRBP2lFwmA2la6wytdSWwBJjXqM11wIda68MAWuvjjg2z/f6z/T8cP3WcZ2Y9Y3qnr79uJsUCIoobjnRJL0gnsgg8o5oZR27rdYefgoFFyjzAAzBjhnmE/6mnICvLNb3z1tjGokcWQXax82d7E0K4jj0JPRKoP6F2pm1bfQlAX6XUeqXUVqXUTc2dSCm1UCmVpJRK6qxfm5bvW05CSAKTIyeb1YMWL4Z+5heIyCIa3BhNz08nogR6xcY3PZEtUf8+/hbCTlaeTuhKwYMPwr59sHx53UNIXUZtQi+WhC5Ed2dPQm+u6Nr4roQnMAG4GJgNPKyUSmhykNavaq0naq0nhjW+6egExRXFrD+4nksTbJPefPEFZGbC/fcDTXvomTmpBJWDJXJg05PZEvrPIy7FknP0dEIHc3M0Ph7Kyrp0Dz2nOMfFwQghnMmehJ4J1M9wUUDjrl4m8IXW+pTWOg/YCIzBxb7K+IrKmsrTCf1f/zI96J//HIChlX0a9NCLDtlmJYyIaHqy2p739u2mpx9Z75cUi6Xuh0SXTejSQxei27MnoW8BhiilBimlvIFrgOWN2nwCTFNKeSql/ICzgBTHhnrmVuxbQZBvEFMGTjGJePlyM795nz4QEkJCecOEXnX4oHnRXEK3lWnYatYMpfHd55tugmnTmq4f6mq9e0NgIINOeZNTIj10IbqzNke5aK2rlVL3AKswwxZf11rvVkrdadv/stY6RSn1BbADsGKGNu5yZuBtqbHWsHLfSubGz8XLwxN+/WsIDob//V/TICKCmFMFdSWXwvJCeufZnhiKbHyLADNhVkiImQQLmiZ0Hx/YuNFJV9NBkZEMKjvMl9JDF6Jbs2scutb6M+CzRttebvT+aeBpx4XWMT9k/UBuaa4ptyxfDuvWwQsvmLHjAJGRhB84QVZxNmVVZXVDFoHme+hgyi579tQd7zYiI4lKz5QeuhDdXLd9UnTFvhVYlIU50T8xvfLhw+tq5wBERNA3vxSAAycPkF6Qbsag9/KFgIDmT1pbH/f2htBQJ1+BA0VF0b+gSmroQnRz3XbFonUH13F21Nn0/ehzMz/KypXgWe9yIyLodaIISw0sT12OQhFTZLbT0tOUtQk9MhLaWmKuK4mOJvBkGXkF8rSoEN1Zt0zoWmtSclO4LvE6eGEpxMbC3LkNG0VGoqxWru43nd+t+R1hfmF8UuqJJbqZIYu16id0dxIdjYeG0JOV8rSoEN2YG3Uz7Xe05CiFFYWM9YqG1avNakCNe6W2Ovm/J/2J3075LbmluQwssbRcP4fTQxfPcH4Fl4s2T77GnJSx6EJ0Z90yoe/NM4siT/0xD6qrTUJvzNbLthw9xl9m/oVV139BRG3JpSW1PXR3S+gxMQBEF8pYdCG6M/dL6OnpZsXvU6dabFKb0Aev2WbKLbULQNRXm7izTYKbFTIZj4oK+xK6u5VcbD+AoguRkS5CdGPul9B37DBjyvfubbFJSl4KUTW98V2/Ca64ovmbnP36mSc8s2wLRdsSe6vJevBgc65hwzpwAS7Qqxe6X1iDHvrS+Ql8/NAZrowkhOjS3C+h1ybTVhL63ry93H4kDFVV1Xy5BUwy79//dCKv/d5aDz0+Hvbvh9mz2xG4a6noGAYXW8gpzuFQ/gEuXrmfiE/WujosIYQDuV9Cj4szybiNhH7JnmpTO540qeVzRUScTuS1PfXWEnrt57vjsL/oaGKLLWSXZLN+3Rv0qYLg48VtHyeEcBvul9C9vU1SbSGhl1SWcKToCPGHSmDq1NaTb0RE05LLgAEODriLiIkhMr+a7KIs9m/4EIDIghpOVZS0caAQwl24X0IHU3ZpIaGn5qXSuwICj500T4e2JjKyYcklKAj8/BwaapcRHU2vSivHD+/FY5eZvqBXNRza94OLAxNCOIr7JvR9+8w0to2k5KUw9ITtzYgRrZ8nIgLy86G83CR0dxu9ciZsY9H7HCtg1LHT09kf2y0JXYjuwj0T+tChUFkJBw822bU3by8j82xllrZ66LX18pdegs8+a/sHgDuzJfToQhiX50nNYLM2Y9G+na6MSgjhQO6Z0FsZ6bI3by9TSvqaeVvim1lKrr7aHvl998HYsfDPfzo2zq7EltATTsDgvBosl5hFP6oz0lwZlRDCgdwzoQ8dar6npjbZlZKXwpgCH5PMvbxaP0/tCuKzZ8OaNWa+8+4qLAyrrw+z08Fi1XDuuRT5WfDMzHJ1ZEIIB3HPyblCQiAsrEkPvaqmiv0n9hN31B8mtlFuARgyxCxYMXp028nf3SmFR3QMMw4cAKogMZH88AD65Jxo81AhhHtwzx46NBjpklOcwy8+/QUxz8agq6oIzSlsu35ea8KE7p/Ma0VHm4etfMxvMOUR/QjPLaeiusLVkQkhHKBbJPTffvVbXk9+nSkDp/Dh+KfwqK6xP6H3JLY6OiNGgKcnOjaWmEI4UJDh2riEEA7h3gk9N5fK4zl8uu9Tbki8gWVXLeNSq+1GqCT0pmoTemIiAL3ihuJfCYcOJLsuJiGEw9iV0JVSc5RSqUqpNKXUg83sn66UKlRKJdu+HnF8qI3YRrr8uH4JhRWFzB8+32xPSWmwX9TTKKEHDxsHQF5KkqsiEkI4UJs3RZVSFuBFYCaQCWxRSi3XWu9p1PRrrfUlToixebaEnfrNJ/Tp34cLB19otqekmMTVu3enheI2EhLM9wkTAPBPMIn91P4UV0UkhHAge3rok4E0rXWG1roSWALMc25YdoiJQffqxZgPvmGh9xR8PX3N9pQUKbe0ZMoU+P57mD4dABUbC4A+eMB1MQkhHMaehB4JHKn3PtO2rbFzlFLblVKfK6VGNncipdRCpVSSUiopNze3HeHWY7Gw7+kHicyv5ukHVsPtt8P775sbpZLQm6cUTJ58esKy4GDKfD3xyZRFL4ToDuxJ6M1NV6gbvd8GxGitxwDPAx83dyKt9ata64la64lhYWFnFGhz/jWkiFG/8qTq1pvhv/+Fq66C0lIY2ezPE9GYUhT1DyLoWCFFFUWujkYI0UH2JPRMYGC991FAg4UptdZFWusS2+vPAC+lVKjDomzB8tTljBt1IT6vLobCQvjhB3jjDbj2Wmd/dLfhM3goMSfhrpV3uToUIUQH2ZPQtwBDlFKDlFLewDXA8voNlFL9lTK/xyulJtvO69RHELXWHDx5kLHhY80GLy+zmMXNN8sN0TMQNGwMIwq9ueSRdygPDoClS10dkhCindpM6FrrauAeYBWQAryntd6tlLpTKXWnrdkVwC6l1HbgOeAarXXjsoxDVdRUUGWtIsAnwJkf0/0NH45PWSWzjnhRVFlC+Z/+6OqIhBDtZNdcLrYyymeNtr1c7/ULwAuODa11tTVfSegdtHAhnH8+pyL9+dvNCfx9xW7YurVuaKMQwn247ZOitQk90DfQxZG4OW9vSExkYHAspVf9lFIvqH75JVdHJYRoB7dN6IXlhYD00B3p6qkLWTIS9H//C8WygLQQ7sZtE7qUXBxveux0lp8fjldpObz7rqvDEUKcIUnooo6H8iDx0tvZHg4Vi191dTid58gR8yWEm3PbhF5YYUougT5SQ3ekn427mbWDQO3cCc4dqNR13Hor3Hijq6MQosPcNqFLD9054oPjscYNxrusEnJ6yJQA6emwp/Fcc0K4H0noook+I820utZ9Tdds7Xa0huxsyM2VG8HC7bl1Qvex+ODj6ePqULodv5FjASjcscW1gXSGEyegwrYEX3q6a2MRooPcNqEXlhdK79xJwodPpNwCJbt/dHUozpeVdfq1JHTh5tw2oRdVFklCd5K4kCGkB4N1fw8ouUhCF92I+yb0iiJ5StRJogOjSQsB34weMJQv20wcWqOgJGWHi4MRomPcNqFLycV5vCxeHIsIom92PtTUuDocp7Jmmh9a28OhaM82F0cjRMe4bUIvqpCSizOVxkbgXWXt9g/clB9K51hv2BsKngcOuzocITrErRO6PFTkPDo+3rzYv9+1gThZ5eGDZPlDTrgfIXmnqKkod3VIQrSb2yb0wgopuTiT34gxAJzanezaQJxMZWeRFQCRY6dh0bBry0pXhyREu7llQtdaS8nFyfonjOeUFxTv6t51Ze+jeWT5w+TzzLKFu779xMURCdF+bpnQy6vLqbZWS0J3oriQePYHQ82+va4OxXkqKuh1soSjgR7ETpgBwNEdm10clBDt55YJvW5xC6mhO83gvoPZHwK+B7rxTVHbXDWn+vXFIyKSSh9PPNIz6ubaF8LduGVCr51pUXrozuPn5cfRAX0Iys6HqipXh+MctoeKrP3DQSkqoyMZlK/Zkt0DpjwQ3ZJdCV0pNUcplaqUSlNKPdhKu0lKqRql1BWOC7EpmZircxTHRGCxajh40NWhOIctoauoKAC8E4YTVwBbs7e6Mioh2q3NhK6UsgAvAnOBEcC1SqkRLbR7Cljl6CAbk/VEO4c1wTZ0cW/3rKPrzEwAfGPiAFtCP6nYmp3kyrCEaDd7euiTgTStdYbWuhJYAsxrpt0vgQ+A4w6Mr1mynmjn8Bk1FoDKnckujcNZyg9nUOYJIRG2H1xxcfhVao6k/uDawIRoJ3sSeiRQ/85Ypm1bHaVUJDAfeLm1EymlFiqlkpRSSbm5uWcaax0puXSOgTGJZPnDqeTuWVOuOJROlj9EBpqSC6NGARCUepiCsgIXRiZE+9iT0FUz2xqvTfYs8IDWutWJP7TWr2qtJ2qtJ4aFhdkZYlMyyqVzxPWNY3cYqN27XR2KU1izMskKgKgAW0IfPx6tFJOzYFtO9x5/L7onexJ6JjCw3vsoILtRm4nAEqXUQeAK4CWl1OWOCLA5tQnd38ffWR8hgLjgOHb3g97pR8BqNRuPHIE33nBtYAAPPACfftqhU1hyjpHtD5H+tl84/f2pGTaUSVmwNUdujAr3Y09C3wIMUUoNUkp5A9cAy+s30FoP0lrHaq1jgWXAXVrrjx0dbK3CikJ8PX3xtng76yMEENwrmIMRfnhVVJ0e6fL002ZRZVfOHV5RAc88Ay+3WuFrndb4HS8g2x8G+A+o2+x51tmcneMhN0aFW2ozoWutq4F7MKNXUoD3tNa7lVJ3KqXudHaAzZHH/jtPyZBo86K27LJ2rfn+1VeuCQjMDxOrFbZ1oCxSUIBXZTVFYf4NOwaTJxNaYiVn9/cdj1OITmbXOHSt9Wda6wStdZzW+gnbtpe11k26SFrrm7XWyxwdaH0y02Ln0cOHmxd79sCxY6cT+5dfui6oVNtKSjk5dU97njFb2ej44P4Nt0+aBED4nsOcLD/ZzgCFcA23fVJUeuidY0DUcDIDwLprJ6xfbzaOHQtr1kB1tWuCSq23NN7WdtS6MzLg4YdZl+hPzuThDfeNHo3Vy0tujAq35JYJXUounSc+OJ7dYVC1M9mUWwIC4P77oagIfnDReO3UVAgOBqXOPKFrDXfeCRYLv7xYERU4sOF+b29qxiSaG6PyxKhwM26b0OUp0c4RFxzHnjDwTE0zvfLzz4fZs00ydVUdPTUVRo+GYcPOPKEvXQpffUXFHxex27fo9AiXerzOnsKkHMX3h2XmReFe3DKhy3qinad2LLqlvMLcjLzgAtM7njTJdXX01FQYOhQmTDjzhP7uuxAby64F0wCICYpp2mbSJHpXao4lbUDrxo9cCNF1uWVCL6ooIsBbEnpnGOA/gLQB9UaB/OQn5vvMmfD991DYyVPN5uVBfv7phJ6dDUeP2ndsTQ1s3AgzZrAs9SMsysKsuFlN202eDEB8egGpJ1Kb7heii3K7hF67WpGUXDqHh/KgPGGweRMSAomJ5vWsWSZBrlvXuQHZboge6e9H8cghZpu9vfQdO+DkSfT06SzZvYQLB19IqF9o03YJCdQEBTL9IHx96GvHxC1EJ3C7hF5WXUaNrpGSSycKj0zgSKgXzJzJqeoykrKT6nqx7NjRucHYEvqC7b/nF9mvntmNUdsoneShgRw8eZCrR17dfDsPDzwum8e8fYpv0jv5B5YQHeB2CV1mWux8cX3jmHWTB/qFF7jvy/s467WzOFKRCxERnT9Xemoq2suLbd75LDm0kqr4wfYn9HXrID6et/PX4+XhxeXDLm+xqbriCoLKNNa1axwTtxCdwO0SukzM1fnig+PZG1BBUnkGbyS/gVVbWbp7KcTGuiShl8VGUmOBGl1DSrQfJCWZ4YitsdXP9fTpvLfnPebEz6Fvr74tt585k0o/H87dcpwjhd14GT7RrbhtQpceeueJ62sWgLjn83uosdYQ1zeOd3e9C4MGwYEDnRtMaip5USEADAkewuuhh82N0bZq+du3Q2Eh+xIjyCzKbLncUsvXl1MzpzN/L2zKWG9+ICxfDmVljrkOIZzA7RK6rCfa+eKCTUL/IesHrh99PXdNuottOds4Ee5vZl/srCdGq6shPZ3D/X1RKJ6c8SQvxxdSHhwAf/9768fa6ufvhR3H2+LNZUMva/PjAq6/jbBSOL7yPbj9dpg3D5Y5dVYLITrE7RK69NA7X0xgDBZlQaF4cOqDXD3yahSKzR7ZpudqW5vT6Q4cgKoq9gTXEB0Yzfxh8wkPjWbptGAzle6+fS0fu24dJCSwqmwnEwZMsGvqZctFF1Hu7cFNf/oM3nzTbNyzxzHXIoQTuF1CTwhJ4OHzHiYyoOkTfsI5vCxejA4fzTWjrmF42HAiAyI5L+Y8Pir/0TTorDr6pk0AbAkoJj44HouHhYXjF3L/kINob2949tnmj6uuho0bqTn/PLblbOOsyLPs+7zevSm44Bz6llp5+AJIDVMUJH/nmGsRwgncLqGP6jeKxy54rPnxw8Jpvr7la968/M2699eOupaNynazsDMSem6uWdRi/HhWBBytq+vfMPoGjveBHTNGwVtvmYeOGvv+eygq4vDkBMqqy5gcOdnujx3w2hKqP/qQqa9+zp4QTU2K9NBF1+V2CV24Rm/v3g3mDZ8/fD5HAkEr1TkJ/X/+B06epOiV5zhWcYL4YLOwc0xQDOdGn8sjYwugtPR0aaS+L74Ai4UN8V4AZ5TQiYrC8/L5zImfQ06EP0FZea6bZVKINkhCF+3Sr3c/fHsHUBjs5/yE/tFHsGQJPPww+yN8AeoSOsANiTew3OcAp8aOhP/8p+nxq1bBWWfxdeFOQnqFMLjv4HaFUREXi2e1tfOHagphJ0noot1iAmPIDvVx/tDFRx+FkSPhwQdJy08DGib0K0deiZeHF19O6QfJybBz5+lj8/LMOPXZs/kh+wcmR05GqebWPW+b1wgz7UHNnu65aLZwf5LQRbtFB0ZzIFA7t8e6f78ZQ3777eDlVZfQ6/eyg3sFM3fIXB4J34P29GzYS//qK9CaUz+Zxu7ju8+s3NJIyLgpAJxIlml1RddkV0JXSs1RSqUqpdKUUg82s3+eUmqHUipZKZWklDrX8aGKriY6MJqUPmWQmem8uvL775vvV1wBQFpBGhH+EfT27t2g2Q2JN7BLHyNv+mR4+20znBJMuSU4mKQBGo3uUEJPGHI2x/3g1A5Z+EJ0TW0mdKWUBXgRmAuMAK5VSo1o1GwNMEZrPRa4FXjNwXGKLigmMIaUPuUmeWZmOudD3nsPzjkHoqIASMtPa1BuqXVJwiX08uzFismBZp3RNWvMdACrVsHMmXx/NAmASRGT2h3K8LDhpIaCx/797T6HEM5kTw99MpCmtc7QWlcCS4B59RtorUv06ZUAegOyKkAPEB0YzcEg2xtnlF1qyy1XXVW3KS0/rW7IYn29vHrxk0E/4engFHRQEPziF3DZZWau9Dlz+CHrBwb3HUxY77B2h+Pn5UdORACBh461+xxCOJM9CT0SqD87UaZtWwNKqflKqb3ASkwvXXRzMUExzk3ojcotJZUlHC052mwPHUwvfW/JQY499CsIDzcrLI0ahb7oIr7L/K5DvfNapXEDCSqsgIKCDp9LCEezJ6E3NySgSQ9ca/2R1noYcDnwx2ZPpNRCW409KTc394wCFV1PdGA0RwKcOBa9UbklPT8doMWEftGQiwB4e0of2LzZPKa/cyc79VGyirOYMWhGh0PyHDYSgMrdO9toKUTnsyehZwL1l0aPArJbaqy13gjEKaWaPMqptX5Vaz1Raz0xLKz9v/qKrmFAnwFob08KQ/s4NqGnpsKll5pyy7XX1m1OPpoMmOkfmhMdGM3o8NGs3L+ywfaPUj5CoeyakKstfcedA8CxrRs6fC4hHM2ehL4FGKKUGqSU8gauAZbXb6CUile2wb1KqfGAN3DC0cGKrsXiYSEqIIpjob6OG4u+fDmMGgUbNsBf/mJq4Tb/3fVfYgJjGB0+usXDLx5yMZsOb+Jk+cm6bR+nfszU6KmE9wnvcHix4y6g0gOKdm7p8LmEcLQ2E7rWuhq4B1gFpADvaa13K6XuVErdaWu2ANillErGjIi5Wsty6T1CdGA0qWEW8zCP1drxE771lql/p6XBb38Lnp4A5BTnsDpjNTeMvgEP1fI/24uHXEy1tZov078E4ODJgyQfTebyoZd3PDZgSPhwMoKBvbJ4tOh67BqHrrX+TGudoLWO01o/Ydv2stb6Zdvrp7TWI7XWY7XW52itNzkzaNF1xATGsDGyytwk3Lu3YyfT2syo+JOfQL9+DXa9u+tdrNrKjaNvbPUUZ0edTXCvYJbtMfOWf7L3EwDmDZvX2mF287Z4kz3An96HcxxyPiEcSZ4UFR0SHRjNylDbiI9vvunYyfbvh+PHYdq0Jrve3vE2kyImMTR0aKunsHhYuH3c7by/530eXf8oH6d+zKh+o1q8kdoeJVFhhB8raXvZOyE6mSR00SHRgdHsDbZSExrS8YRum++ccxs+aLz7+G5+PPojN4y+wa7TPDnjSX425mcs2rCI9QfXO6zcUqs8JpJelRqOyXh00bVIQhcdEhMYAwpOjh9hhgp2xNdfQ2goDBvWYPN/dvwHi7Jwzahr7DqNxcPC4ssW87MxP8NDeXDFiCs6FlcjarB5sKk0dZdDzytER0lCFx0SHRgNwOGRUadLJu21aZPpnTeaDXF56nJ+Mugn9Ovdr4UDm7J4WHh93uscuvcQY/qPaX9MzfAdasaiF+ySOV1E1yIJXXRIbULfNSTQbKjtpR84ACdP2n+io0fNyJZG5Zac4hxS8lK4cPCFZxybh/IgKiDqjI9rS99h47ACZftkGl3RtUhCFx3S27s3Ib1C2NK/Bry9TR19zx5ITIS77rL/RF9/bb43uiG69sBaAIc85ekoUf3iyAwAnZ7m6lCEaEASuuiw6MBo0suyYOJEM//4T38Kp07Bp59CZaV9J9m0Cfz8YNy4BpvXHlhLX9++jO0/1vGBt1OkfyQZfcHnkJNmmBSinSShiw6LCYrh4MmDMHWqeVw/LQ3uuw+Ki2HjRvtOsmEDnH02eHnVbdJas+bAGqbHTsfiYXFO8O3gZfEiJ7wXAVl5rg5FiAYkoYsOi+8bT0ZBBtbzzzMbnnoKHnsMfH1hxYq2T/DVV+YHwfz5DTZnFGRwqPBQlyq31CqICCGooMz8JiJEFyEJXXRYfHA85dXlZE0dbRLzb35jyicXXmjmZmntARyrFR54AGJj4Y47Guyqq58P7noJvSI6wrxw9nqqQpwBSeiiw4aEDAFgf0EajB59etjhpZeaWRh3tzIaZOlS+PFH+OMfwcenwa41B9YwoM8Ahoa0/nSoK+jBZk1TnSY3RkXXIQlddFjtY/W1CzjXueQS872lsktlJfzhDzBmDFx3XYNdWmvWHljLjMEzUKq5Kfldy3eoWYWxfN8eF0cixGmS0EWHRQVE4evpy/4TjdbajIgwI1+WL2/+wP/7P8jIgD/9CTwa/lPcdHgTuaW5zBo8y0lRd0xoVAInfaB0ryx0IboOSeiiwzyUB3F949if38ziyfPnw3ffwbZtDbevWAF//rOpm8+d2+SwF7a8QJBvEAtGLHBS1B0zMDCajL5QI2PRRRciCV04RHxwfNOSC5iHi0JDzTDG2pujGRlw001mzPlzzzU5JKsoiw/2fMBt427Dz8vPyZG3z8DAgaQHg/fBI203FqKTSEIXDjEkeAjpBelYdaNFLoKC4NFHYf16+OQT2LcPZtnKKMuWmaGNjbyy9RWs2spdk87gSdNONqDPAA72VfTJyYOaGleHIwQgCV04SO3QxcyiZp6eXLgQhg+HX/3KPDxUWAiffQa2kSL1VVRX8MrWV7g44WIG9226v6uweFg4HhmEZ1UNPPEEVFe7OiQhJKELx6gduths2cXTE/76Vzh82KxE9N13cM45zZ7nw5QPOX7qOPdMuseZ4TrE1ukJrJvcz9zcPe88yMpydUiih5OELhxiSLBtLHrjkS615s6FtWtNMo+La/E8aw6sIaRXCDPjZjojTIfqFxbLwuv94b//ha1b4ZlnXB2S6OHsSuhKqTlKqVSlVJpS6sFm9l+vlNph+9qslHLsBNSiy4sMiMTX07f5HnqtCy4wNfVWJB9NZtyAca0uBN1VDAwYSGZxFvqaa2DkyI6vqSpEB7X5v0YpZQFeBOYCI4BrlVIjGjU7AJyvtR4N/BF41dGBiq6t1aGLdqq2VrPr+C7Gho91XGBONDBwIOXV5eSW5kJCgrnhK4QL2dMNmgykaa0ztNaVwBKgwRLqWuvNWmvbSsF8Bzh+VQHR5cUHx3cooafmpVJRU+HwFYacZUy4iXPT4U0moR88CBUVrg1K9Gj2JPRIoP5g20zbtpbcBnze3A6l1EKlVJJSKik3N9f+KIVbGBI8hPT8ZoYu2in5aDJAl5r7vDVTo6fS17cvn6R+AkOHmonG0tNdHZbowexJ6M1NpNHs9HlKqQswCf2B5vZrrV/VWk/UWk8MCwuzP0rhFuKD46moqSCjIKNdxycfTcbH4tMlJ+NqjqeHJxcnXMyn+z6lOt42xFLKLsKF7EnomcDAeu+jgOzGjZRSo4HXgHla6xOOCU+4k6nRU7EoC9PfnM7n+5v9Ja1V249tZ1S/UXhZvNpu3EXMGzqP/LJ8vuuVbzZIQhcuZE9C3wIMUUoNUkp5A9cADWZbUkpFAx8CN2qt5V90DzWq3yi+u/07An0Duei/F/H898/bfazWmuSjyXV1aXcxO2423hZvPsxZA+HhktCFS7WZ0LXW1cA9wCogBXhPa71bKXWnUupOW7NHgBDgJaVUslIqyWkRiy5tYsREti3cxlmRZ/Haj6/ZfVxOSQ65pbluUz+v5e/jz4xBM/gk9RN0QgKkpro6JNGD2TXYV2v9mdY6QWsdp7V+wrbtZa31y7bXt2ut+2qtx9q+JjozaNG1+Xj6MH/YfHYc20FOcY5dx2w/uh1wnxui9c0bOo+MggxODgyTHrpwqa7/9IZwS7PizARcX2V8ZVf72hEuo8NHOyskp7ls6GUAJAeUwvHjcPKkawMSPZYkdOEUY/qPoV/vfnyZ/qVd7ZOPJTMoaBCBvoFOjszxBvgPICYwhh8DS82G/e0fiy9ER0hCF07hoTyYOXgmX2V81ea4dK01W7O3umW5pVZCSALf+tkGd0nZRbiIJHThNLPiZnH81PG6+nhLNh/ZTHpBOrPjZndSZI6XEJLABo8jaA8PuTEqXEYSunCamYPNjIltlV1e3PIiAT4BXD/6+s4IyykSQhLIrSnCGjNQeujCZSShC6cZ4D+A0eGj+TKj5YR+rOQYy/Ys45axt9DHu08nRudYtdMHF8YMkIQuXEYSunCqWYNnsenwJgrKCprd/69t/6LKWtWll5uzR0JIAgBZAwNhzx4oKnJxRKInkoQunOraxGuprKlkya4lTfZVW6t5ZesrzBw8sy4huquYoBi8PLzYNLGfmXHx449dHZLogSShC6ca138cY8LH8Hry6032vZX8FplFmdw96W4XROZYnh6exAXHsTq8BGJj4Z13XB2S6IEkoQunUkpx67hbScpOYsexHXXbs4qyuO/L+zgv5jwuHXqpCyN0nISQBPbl74frroPVq+HYMVeHJHoYSejC6a5LvA4vDy/e+PENwIw7//mnP6eyppLFly12i+Xm7DEkeAhp+WlYr73GzI3+3nuuDkn0MN3jf5Lo0kL9Qpk3bB5v73yb1LxU/m/9/7Fy/0qenPEk8cHxrg7PYRJCEiivLidzYCCMGWMWjxaiE0lCF53i1rG3kleax7AXh/HHjX9kdtxsfjn5l64Oy6Fqb+zuO7HPlF2++w5uvhmuvRaWLnVtcKJH8HR1AKJnmBU3i4fPe5h+vfvVjWpRqrnFsNxX/YR+4Q03wKuvwpo1UFIC69bBFVeAxeLiKEV3JglddAqLh4XHLnjM1WE41YA+A+jt1Zv9J/bDpAhISzM7PvjAJPO1a2HmTNcGKbo1KbkI4SBKKYaEDGFffqMnRS++GAICmq+pHz0K27Z1ToCi25OELoQDJYQkkJrXaHIuX19YsMD01MvKGu779a9h2jQoLu68IEW3JQldCAeaOGAi6QXpHDp5qOGO6683SXvlytPbampg1SooLZUnS4VDSEIXwoHmD58PwIcpHzbcMX06DBjQ8AnSpCQosM1xI0+WCgewK6ErpeYopVKVUmlKqQeb2T9MKfWtUqpCKfW/jg9TCPcQHxzP6PDRfJDyQcMdFgtccw189hnk5Zltq1aBUrBwIXz1lamnA+zYITM2inZpM6ErpSzAi8BcYARwrVJqRKNm+cD/AM84PEIh3MyC4QvYfGRz0wWyb78dKivhxRfN+1WrYMIE+NWvzJOlS5easetnnw3jx5v9QpwBe3rok4E0rXWG1roSWALMq99Aa31ca70FqHJCjEK4lQXDF6DRfLT3o4Y7RoyASy6B55+H7Gz4/nuYPdtsHzcOXnoJLr0UIiIgPt60fftt11xEF1ZVU9XmsoY9lT0JPRI4Uu99pm3bGVNKLVRKJSmlknJzc9tzCiG6vBFhIxgaMrRp2QXggQfgxAm48UZzU3S2bdm9G244XWb54gvYsMGMfvnZz+DIkabn6YFKq0p58usnCflLCPd/db+rw+mS7EnozT3Op9vzYVrrV7XWE7XWE8PCwtpzCiG6PKUUC4YvYMPBDRwuPNxw57nnwpQp5iEjf39TXgG46SYztHHlStM7DwyEZ581pZi1azv9GrqavXl7SXg+gYfWPoS3xZs3kt+gsqbS1WF1OfYk9ExgYL33UUC2c8IRonu4ZtQ1KKUY/I/BzPrPLNYfXH965wMPmO8zZoCXl3kdGgrLlsHkyafbjRoFISFm2gA3t/bAWrKKstp1rNaaOz+9k7LqMjbevJG3Ln+L/LL8Nteq7YnsSehbgCFKqUFKKW/gGmC5c8MSwr0lhieybeE27p96Pyl5KcxfOp/jp46bnZdcAr/4hbkZ2hoPDzPccd060O36pbhL+DL9S2b8ewax/4jlpo9uIiU35YyOX7p7KRsObeDJnzzJtJhpzIqbRUivEP67U2azbKzNhK61rgbuAVYBKcB7WuvdSqk7lVJ3Aiil+iulMoHfAH9QSmUqpQKcGbgQXV1ieCJPzniSL2/4klOVp3hwtW3Er4eHuQE6fTqpeams3Ley5ZNMnw6HD8PBg50Rsl0qqit4bMNjbMna0mbbqpoqfvXFr4jrG8fdk+7mw5QPmfL6FNLy0+z6rOKKYu778j7GDxjP7eNvB8DL4sWVI67kk9RPKKks6dC1dDtaa5d8TZgwQQvRUzzw1QOaRehNhzbVbdt8eLMO/FOgZhH6d6t/p61Wa9MDd+3SGrRevLgTo21ZeVW5vvidizWL0F6PeekXf3ix+bht/rb5b5pF6BWpK7TWWqfnp+u+f+6rE19K1CUVJa1+1uGTh/WCpQs0i9CbD29usG/jwY2aReh3drzT8YtyM0CSbiGvKu2iX+UmTpyok5KSXPLZQnS2ksoShr84HH9vf+49+158LD7c/dndDPAfwJSBU/j39n9z7ahrSeyXyL78fVTWVBLmF0Y/vzB+Pf8vlJx/Djn//IvT4qu2VrM3by97926i7Hg2FYOjCe4VTExgDHHBcQT4BGDVVhatX8SKfSt4ZuYzrDu4jpX7VzJ14FRGho0kJiiGmMAYYoJi8LZ4k1+Wz9XLrmbqwKmsvG5l3XTJq9JWMfeduVwx4grumnQXXh5eeHp44mXxorSqlJTcFL7L/I63d76N1pqHz3uYh89/uEG8Vm0l9tlYYoNi+d8p5llGPy8/AnwC8LH4dPmpmcP8whjgP6BdxyqltmqtJza7TxK6EJ3j8/2fc/WyqymuNBNxjeo3ii9v+JL+ffrz+MbHeWT9I4CZhreXVy/ySvMoqihiyfsw5QhE/5rmx5w50LL3PZiXYuXROb48Makc3cznvTT3RX6RE4l18iSeyXib93a/x6HCQ+SV5jVp6+vpS/LPkxkaOrTB9j99/Sd+v/b3Lcbh5+XHDYk38PtpvycmKKbZNn9Y+wee+PqJM7vALuKBqQ/w5wv/3K5jJaEL0UVYtZUjhUc4cPIAEwZMwN/Hv25fTnEO/j7+9PHuU7ettKqUwr//mQEP/JFVn7/AqRg7e3VWq5lWwM6eqkIRFzSIxFEzUNXVUFSE9fJ5HHjxCdLLsiitKsVDeRAVEMX4bA0TJ5pRON98Y6YGBk5VnuJw4WEOFR6ixlqDv48/cX3jiAxo+tiK1prtx7ZTUFZAtbWaKmsV1dZqvDy8GBY6jJigmDbXmq2qqWLX8V1o2yjqU5WnKK4spry63L4/IxdKCElgVL9R7Tq2tYQuNXQhurqUFFNHf/XVttvW1Gj9/PNaBwRoPXiw1vffb+rw9tixw3zOG29o/fTT5vWLLzZtd/fdWnt7a22xaD13rtZVVWd0OaJjaKWGLrMtCtHVDR1qpgP4spVx11arWe5u2jT45S9h0iQYMgT+9jczL4w9a5pu3Gi+n38+3Hef6YW/+GLDIZPl5Wahjp/+1Oz7/HMz/LK13/RPnjRz1jzyiHk6VjhPS5ne2V/SQxfiDNx6q9aBgc33hl9/XeuYGNOjDg3V+t//1rp25MmxY1qfe67Z9/TTp7c358ortR448HSbN94wx61de7rN0qVm26pV5v1vf2ve33671tXVzZ/3b38zbUDr2bO1zss7w4sX9dFKD10SuhDu4P33zX/XTZsabt++3ZQ+Jk/W+t13tS4ra3psWZnWV11ljl+5svnzW61ah4drff31p7eVlmodEqL1T396etucOVpHRZ1O3lar1g89ZM591VVNf+BUV5vSz7nnmpKRt7eJtbUfLKJVktCFcHcFBSZxP/TQ6W1Wq0mUISFt93orK7Xu31/rSy9tfn9qavN1+vvvN597+LDWBw5o7eHRMIZaf/6zOf7NNxtuX77cbH//ffP+hRfM+y1bWo9XtKi1hC41dCHcQVAQnHOOmYmx1r//DZs2wVNPmTlfWuPlBbfcYib/ysxsun/DBvP9vPMabv/FL0x9PjERBg0y226+uenx998Pw4ebqYHr19Ofew6iouDyy837G24APz/4179aj1e0iyR0IdzFnDmwdSscO2a+fvtbk+RvucW+42+7zSTnN95oum/jRggPh4SEhttjY+Hxx838M48/Dl9/bWaDbEwpczN261b49luzbfduWL0a7r4bPD3NtsBAuOoqc2O15Awe29faLPjxm9+YG6yvvGL/sT1JS113Z39JyUWIM5SUZMoVTz2l9bBhWvv5aZ2cfGbnmDHD3ECtqTm9raLC1MWvvLJj8RUXmxu3V19tSkRjx2rdu7fWubkN223aZK7jtdfsO6/VqvU995hjfHy0Dg7WOi6ux9bhkZKLEN3AuHHQr5+ZfvfIETNkcMyYMzvHHXfAoUOnSzcpKWZO9sxMMxSxI/r0Mb8FLFsGs2aZHvoHH5ipgeubMsWUZ157zb7zPvwwvPAC3HuvWVT7qacgPR1+/LFj8XZHLWV6Z39JD12Idvj5z7Xu00frr79u3/Hl5VqHhZnebt++pscbGqr1xx87Jr70dK2VMjdP33uv5Xa1Qxl/+KHpvpoarV9+2fTKL73UtLvtttM98rw8rT09tX7gAcfE7GaQybmE6CbKy6GsDPr2bf85UlJgxQrTUwfTA+7f3zHxgelNR0S03uMvLDQPTMXEmJq7R71iwT/+YXrjgYHmN5LZs83qTRbL6TZz50Jqqumpd/GJuBxN5nIRQnQ9b79t1lZ99VVTCgJTppkwAWbOhOXLW07Wb7wBt94KP/xgnortQVpL6FJDF0K4xvXXm2GSDz4IWVmQn2+GNQYEmPp6az3vyy83QzHfe6/TwnUHktCFEK6hlJkPprDQjFUPCYHkZJPMw8NbP7ZvX3Pj9T//ga++cusl+hzJ09UBCCF6sFGjzKRjW7aYHvewYXDRRfYd+9BDcOWVJrFPmmSW9ZvY/KyyPYXU0IUQ7quiwjwx+9hjcPy4eVL1jju69Y3SDtfQlVJzlFKpSqk0pdSDzexXSqnnbPt3KKXGdzRoIYRok4+PSeDJyXDBBfDzn8Po0TB2rJk2+Be/MHX2wkJXR9op2kzoSikL8CIwFxgBXKuUGtGo2VxgiO1rIfBPB8cphBAtCwkx89Q89RQMHGjmnQkLg3fegauvNsMob78d1q6F/ftNgi8rM8NArVZXR+8w9tTQJwNpWusMAKXUEmAesKdem3nAv22D3r9TSgUppQZorXMcHrEQQjTHYjGThN1//+lt1dVmaOObb5rkvnhx0+OUMiNrAgPNePjapfva+uqI224z89I4mD0JPRI4Uu99JnCWHW0igQYJXSm1ENODJzo6+kxjFUKIM+PpaaYamDIFnn4avvvOTGyWmwtVVWZ0THm5WVWpqMj01rVtOY76rxt/dVRbo3jayZ6E3tyPosZXZE8btNavAq+CuSlqx2cLIYRjBAaap067MXtuimYCA+u9jwKy29FGCCGEE9mT0LcAQ5RSg5RS3sA1wPJGbZYDN9lGu5wNFEr9XAghOlebJRetdbVS6h5gFWABXtda71ZK3Wnb/zLwGXARkAaUAnbOuC+EEMJR7HpSVGv9GSZp19/2cr3XGrjbsaEJIYQ4EzKXixBCdBOS0IUQopuQhC6EEN2EJHQhhOgmXDbbolIqFzjUzsNDgTwHhuMueuJ1yzX3HD3xuttzzTFa67DmdrgsoXeEUiqppekju7OeeN1yzT1HT7xuR1+zlFyEEKKbkIQuhBDdhLsm9FddHYCL9MTrlmvuOXridTv0mt2yhi6EEKIpd+2hCyGEaEQSuhBCdBNdOqH3xMWp7bjm623XukMptVkpNcYVcTpaW9ddr90kpVSNUuqKzozPGey5ZqXUdKVUslJqt1JqQ2fH6Gh2/PsOVEqtUEptt12z28/cqpR6XSl1XCm1q4X9jstjWusu+YWZqjcdGAx4A9uBEY3aXAR8jlkx6Wzge1fH3QnXPAXoa3s9192v2d7rrtduLWbmzytcHXcn/F0HYdbujba97+fquDvhmn8PPGV7HQbkA96ujr2D130eMB7Y1cJ+h+WxrtxDr1ucWmtdCdQuTl1f3eLUWuvvgCCl1IDODtSB2rxmrfVmrXWB7e13mNWh3J09f9cAvwQ+AI53ZnBOYs81Xwd8qLU+DKC1dvfrtueaNeCvlFJAH0xCr+7cMB1La70Rcx0tcVge68oJvaWFp8+0jTs50+u5DfOT3d21ed1KqUhgPvAy3YM9f9cJQF+l1Hql1Fal1E2dFp1z2HPNLwDDMUtY7gR+pbW2dk54LuOwPGbXAhcu4rDFqd2I3dejlLoAk9DPdWpEncOe634WeEBrXWM6b27Pnmv2BCYAM4BewLdKqe+01vucHZyT2HPNs4Fk4CdAHPCVUuprrXWRk2NzJYflsa6c0Hvi4tR2XY9SajTwGjBXa32ik2JzJnuueyKwxJbMQ4GLlFLVWuuPOyVCx7P333ee1voUcEoptREYA7hrQrfnmm8B/qxNcTlNKXUAGAb80DkhuoTD8lhXLrn0xMWp27xmpVQ08CFwoxv31Bpr87q11oO01rFa61hgGXCXGydzsO/f9yfANKWUp1LKDzgLSOnkOB3Jnms+jPmNBKVUODAUyOjUKDufw/JYl+2h6x64OLWd1/wIEAK8ZOutVms3n6HOzuvuVuy5Zq11ilLqC2AHYAVe01o3O/TNHdj59/xH4E2l1E5MKeIBrbVbT6mrlHoXmA6EKqUygf8DvMDxeUwe/RdCiG6iK5dchBBCnAFJ6EII0U1IQhdCiG5CEroQQnQTktCFEKKbkIQuegylVJBS6i7b6wil1DJXxySEI8mwRdFjKKVigU+11qNcHYsQztBlHywSwgn+DMQppZKB/cBwrfUopdTNwOWYh11GAX/FTO96I1ABXKS1zldKxQEvYqZ1LQXu0Frv7eyLEKIlUnIRPcmDQLrWeizw20b7RmGmq50MPAGUaq3HAd8CtbMcvgr8Ums9Afhf4KXOCFoIe0kPXQhjnda6GChWShUCK2zbdwKjlVJ9MIuLvF9vtkefzg9TiJZJQhfCqKj32lrvvRXz/8QDOGnr3QvRJUnJRfQkxYB/ew60zcd9QCl1JdStA9kt1nMV3YckdNFj2OaO/8a2WO/T7TjF9cBtSqntwG6aXyZPCJeRYYtCCNFNSA9dCCG6CUnoQgjRTUhCF0KIbkISuhBCdBOS0IUQopuQhC6EEN2EJHQhhOgm/h+/lIHj0aGDzQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABK9UlEQVR4nO3dd1zWVfvA8c8BRGQoKm5UcObes0zLNDVzlJra0oaZDbW0evo9mWX1ZDbUx8psWY/lzBzlqlzlxoEKLkQFHMhwgICs8/vjgEzlFm64ueF6v168br77fB0Xh+sspbVGCCGE/XOwdQGEEEJYhwR0IYQoISSgCyFECSEBXQghSggJ6EIIUUI42erBXl5e2sfHx1aPF0IIu7R3795IrXWV3I7ZLKD7+Pjg5+dnq8cLIYRdUkqdudkxSbkIIUQJIQFdCCFKCAnoQghRQtgsh56bpKQkwsLCSEhIsHVRioyLiwve3t6UKVPG1kURQti5YhXQw8LC8PDwwMfHB6WUrYtT6LTWREVFERYWhq+vr62LI4Swc3mmXJRS3ymlLiqlDt/kuFJKzVZKBSmlDiql2ua3MAkJCVSuXLlUBHMApRSVK1cuVb+RCCEKjyU59PlAn1sc7ws0TPsaA3xZkAKVlmCerrS9rxCi8OSZctFab1VK+dzilIHAj9rMw7tTKeWplKqhtT5vrUIKIUSxEBMDBw9CaqrZrlABPD2hbFlQCq5cgSNH4PhxuH4dtDbnap3xlZoK3bpB795WL541cui1gNBM22Fp+3IEdKXUGEwtnjp16ljh0danlOKVV17hk08+AeDjjz8mNjaWqVOnMnXqVD766CNOnz5N1apVAXB3dyc2NtaWRRZCFIXLl6FDBwgKKth9lILXXy+2AT23nEGuq2ZorecB8wDat29fLFfWKFu2LMuXL+df//oXXl5eOY57eXnxySefMH36dBuUTghhE6mp8OSTcPo0/PAD1Kpl9l29agJ9YqLZdnODJk2gcWNwdTXB28HBfKZ/FSJrBPQwoHambW/gnBXuaxNOTk6MGTOGzz77jPfffz/H8aeeeor58+fz+uuvU6lSJRuUUAhR5GbMgFWrYNYseOIJW5fmpqwR0FcBLyqlFgGdgCvWyJ9PWDeBAxcOFPQ2WbSu3pqZfWbmed4LL7xAy5Ytee2113Icc3d356mnnmLWrFm88847Vi2fEKIY+uILePNNeOQReOklW5fmlizptrgQ2AE0VkqFKaWeVkqNVUqNTTtlDRAMBAFfA+MKrbRFpHz58jzxxBPMnj071+Mvv/wyP/zwA1evXi3ikgmRi/TGtuwiI+G99+CNNyA6uujLZe+Sk+Hll+GFF6BfP/j220JPmRSUJb1cRuRxXAMvWK1EaSypSRemCRMm0LZtW0aPHp3jmKenJyNHjuSLL76wQclEqTZ9OixcCL16QefOsGULLFtmAnaVKuaralWTv123DuLjTQ7322/hk0/g8cdzD0qJieDsXPTvU1wlJJga+apV8Oqr5s/d0dHWpcqTzOVyE5UqVWLYsGF8++23uR5/5ZVX+Oqrr0hOTi7ikolS6+BB+L//M13jZs+GIUPg66+hSxdTk7zvPqhRwwT3gwdh5EgIDIT9+6FhQ9OoN3Fiztr84sVQqZKpyZc2W7bAX39l3RcTA337mmA+Zw58/LFdBHMoZkP/i5tXX32VOXPm5HrMy8uLwYMH89lnnxVxqUSplJICzz5rAq+fn+n3fOAAtG4N7u55X//PP/DKK6ZRLyXF/EC4dAk++sjUPr28zGfr1jB8eCG/TDGhNYwaZfqSHziQsX/wYPj7b1iwAB591Falyx+ttU2+2rVrp7MLDAzMsa80KK3vLW7D7NkmU/7zz/m/R2qq1q++au7j6pox1GXsWK1jY7W+806z39/feuUuzvbuNe9fuXLGvuvXtVZK69des1258gD46ZvEVamhC2ENly5BeLhpSKtWzeSyrSEpCT79FN5+G/r0KVjtWSnT/a5ePTOa0dfX1MjvvdccX7YM2rY1qZn9+61S/GJt+XLzGRUFcXGm3eHcOfNjrlEj25YtnySgC1FQ166ZHHVUlNmuWhXOnAEXl4Ld9+BB0+fZ39+kAb76quC9LJSCcTfpiFa9Orz2msmznzplAn5J9ssvpsE4NRXOnjV/hyEh5ljt2re+tpiSRlEhCmrpUhPM0/PRFy+a2m5+aQ3ffAOdOpla//Ll5statf5b6dfPfK5dW/jPsqXAQDh61PygBAgNzfpZTKcmyYsEdCEK6ptvzK/okybB5Mnm+7lzb+8e58+b/s4DB0LHjqYB9K67TGNdetApCg0bQv36sGZN0T2zqGzbZv5+IiJM7RxM7yCAsDDzmR7QpYYuRCnx00+mXzeYWt62bfDMMxlzdTz3nNl36NDN75GcnDFj37ZtJnf93XdmrhA3N/jwQ9OPvFq1LJetPbGWSRsmcTnh8o19CckJJKYkWufdlDK19I0bTV/s4iI11fQ8ya/Tp2HAANMFsUkTk77q2tVMtgUZgTwkBCpWNH8HdkgCuhC3IzUVJkwwAfzrr01gd3LKOr/Hk0+aboVffZX7PXbuNJM7ubpC06bQo4fperhnj8mXb95sZuPL1vdZa83kPybzyY5PaP5Fc34+9DMvrXmJKjOqUOvTWkzZNIULsRdunJ+cmsy0LdMYv3Y8OreRpDfTr58ZkLRli+XXFLYlS+Duu/MX1BMSTJ/9lBTTt/yOO0zOfNgwKFcOKlfOWkO303QLSKNonqZOnYq7uzuTJk3K8/j8+fPp3bs3NWvWLOJSiiLj52eG1NesCc8/b4LygAFZa9KVK8PQofDjj6YWGBVl+nnfc4+ptQ8ebBogn3gCTp40qZWPPjLzat/CwfCDBEQEMK79ODae3sijyx/F2dGZ4c2HcyXhCu9tfY/p26bzaItHebzl40zdMpWtZ7YC0LFWRx5taWGf6u7dTYPumjVw//35/IOyst9+M59//WXmEr8dEyfC3r2wciU8+CA88ABs325G2oJJr2QO6HaabgEJ6FY1f/58mjdvLgG9JFu71qQltm0ztb69e01tHVODPh97HmdHZ7zGjct9YIqDAzRvDuvXm6B+GxYcXEAZhzK8e8+7fFzmY9YFraNL7S5Udzf3ORF1glm7ZvH9ge/5/sD3uJZxZf7A+Xzp9yUT1k+gT4M+VHatnPeDypUzXRnXrDEDkQqD1uaHWYMGGft++QXeesv8EBk2zIyABVOzXr/efL958+0/a8EC88NzwACz7eBgfoim8/bOmnK5887bf0YxISmXXLz//vs0btyY++67j2PHjgFw8uRJ+vTpQ7t27ejWrRtHjx7Ncs2yZcvw8/Pj0UcfpXXr1sTHx/Puu+/SoUMHmjdvzpgxY27v115RPK1daxotfXxMjvvnn9G9e/PAzw/g9oEbtT6tRYsvW5DcqQMcPmx6U4SHm3TK9OlmtObmzbcdzFNSU/j58M/0a9iPyq6VKVemHIObDL4RzAEaVm7InH5zCJ0YyncDvmP/c/t5svWTzHtwHpcTLjP5j8mWP7BfP7OQQ7Z/51azdatpgM2cQvnmGxNQv/jC/GYzb57Zn/5bka+vSVfdTm4/IQFiY8385Dfj7W1q6LGxZjyB1NALwYQJWYfjWkPr1jBz5i1P2bt3L4sWLWL//v0kJyfTtm1b2rVrx5gxY5g7dy4NGzZk165djBs3jo0bN964bsiQIcyZM4ePP/6Y9u3bA/Diiy8yZcoUAB5//HF+++03HnzwQeu+kyg6kZGwe7cZ5AMmjTJiBDtDd7DmxBqGNRuGVzkvvvD7gr/P/M09ze7JuLZqVUj7d5Efm05v4lzMOR5r+Vie51YqV4nRbTImlWtZrSWTukziw20f8nSbp7mzjgU10MGDzaRUM2ZkNABbU3qD8dKlJoUSFwebNpk01tSpZi6V99+H0aPND1EHB7P/ySdNUO/Rw7LnREaaz1t1+axd26TFjh8323acQ5caejZ///03gwcPxtXVlfLlyzNgwAASEhLYvn07Q4cOpXXr1jz33HOcP5/3lO+bNm2iU6dOtGjRgo0bNxIQEFAEbyCsavNmMyFWUhJs2GBSBX37ZjllccBiyjqW5esHv+ajXh/h4uTCiqMrrFqMBQcXUL5sefo36p+v69/q/hZV3aoydctUyy6oWRPGjjWr8xR0ybXcnDplPlesMH+mmzaZNTj79TNzq7z5pqmtL1mS8VvRwIEmsN9O2iUiwnzmsvrYDd7e5nPHDvMpNfRCkEdNujCpbKPxUlNT8fT05MBt/MaQkJDAuHHj8PPzo3bt2kydOpWE4tQNTOQtPNzkyaOizFB5Z2cTGDLVtFNSU1gSsIS+DftSvmx5AHrX782KYyuY2Wdmjn9L+bHy6EqWBCxhZIuRuDjlb/SpaxlXJnedzOQ/JrM9dDtda3fNcjz0Sig1PWri6JCpZ80bb5i0xzvvwP/+V5BXyCk4OO3BobBvn8nXu7qanixgAnvTpubZQUGmdl6hguneeTsB3dIaOpiG0szbdkhq6Nncfffd/Prrr8THxxMTE8Pq1atxdXXF19eXpUuXAqbxy9/fP8e1Hh4exMTEANwI3l5eXsTGxrKsICMHRdHT2vz6HxNjBgz9+quZZvb++zkcGcihcJMy+CfkH87HnueRZo/cuHRQ40GEXAkp8IpbyanJvPbHawxaPIhmVZvx7j3vFuh+z7d/niquVXhnS9aVtpYGLMVnlg//3vjvrBdUrw4vvmj63QcGFujZOZw6ZXqZODiYP9s1a8z0v2XLmuMODmYaghMnsv5W1KOHqUnHx1v2nNupoW/fbhq8a9XK1ysVBxLQs2nbti2PPPIIrVu35uGHH6ZbWhepn376iW+//ZZWrVrRrFkzVq5cmePaUaNGMXbsWFq3bk3ZsmV59tlnadGiBYMGDaJD+gAGYR8WLjSBZto0k0dOm0ZZP/wwgxYNotM3ndgeup3FAYsp51QuSyqkf6P+OCiHAqVdzsecp+ePPZmxfQbPt3+ef0b/Q02PgvWecnN2Y1LXSWw4ueFGd8Y1J9YwcvlIHJUjc/bM4VL8pawXvfaaGWTz8ccFenYWWpsaeocOpkY+d64Z+JM+7UC6ESNMsK1SBdq1M/t69DCLcezcadmz0gP6rWro6QH99Gkzn3yZMrfxMsXMzaZhLOwvmT43Q2l972Jr5Uqt3d217txZ6+TkjP3R0fpw+GHNVLTzNGdd8cOKutL0SnrY0mE5bnH393frll+2tPiR1xKv6Xl+8/TsnbP1rJ2zdLUZ1bTr+656gf8Ca7zRDTHXY3SVj6popqJrfVJLl51WVrf7qp3eenqrZip62pZpOS8aOlTr2rXN9LvWEBlppq399FOtZ83KmMb3zJmc5/7zj9br1mVsX76stYOD1lOmWPasf//bnJ+ScuvzKlc2Zejc2fL3sBFuMX2u1NCFADMUPzTU5GwHDjTd3JYuzTpas2JFVh4zv5ltenITLk4uRMdHZ0m3pBvUeBAHww/yV/BfuXZX3Rm2kz1n95Ccmsyes3to81Ubxvw2hpfXvcz4dePxdPFk1zO7LB8MZCF3Z3e2P72dGb1m0MOnBw81eYj1j62nW91u9G3Ql1m7ZhGXFJf1ou7dzZ/N6dPWKUR6g2i9ejBokPm+efPce5fceWfWwU03y6OvWmW6HWYXEWEGejnkEerSa+l2nD8HpIZeHJTW9y42xo41ixqk1xSfeELruLhcT+30dSfdYV4HrbXWARcD9Jt/vqkTkhJynBd6JVRXml5JMxXdZE4TPXPHTH014aqOT4rXY1eP1UxFMxXt9r6bdnzHUXt/6q3XB63XEdcidNiVMJ2UklSor5yb9Fr67J2zsx44dMj8uXz/vXUetHixuV/6Qhpjxmj9zTeWXz9pktbOzhl/R/v2mfvNnJnz3Icf1rpp07zv2b+/uccrr1heDhvBnha40FpbpWeAvdAy2Mi2rlwxk2Lddx88/LCZKbFHj1znHT8fc55dZ3cx7Z5pADSt0pT3e76f6229y3tzavwplgQs4dv93zJh/QSmbJ5CTY+aHI08yuSuk2lfsz1/n/kbZ0dn3ur+Fp4unoX4onnrVrcbd9a+k//u/i8vdXop40DTpqZRcfNms2RbQaXX0NPnW7/ZnDc3c889Jqe/c6f5Pn1agNwW5YiIsGza4fQauh33QQcLG0WVUn2UUseUUkFKqRwrySqlKiqlflVKHVRK7VZKNc9PYVxcXIiKiio1QU5rTVRUFC4FXQhB5N+KFaaRbdo0M0viPfdkCea/H/+dCesmEJcUx2/HTeAY2HigRbcuX7Y8z7R9hh1P72DXM7vo17AfqTqVFY+s4KNeHzGs2TD+2++/fHL/JzYP5ukeafYIJ6JPEHwpOGOng4NpvLTWZF3BweYHhIdH/q6/666s/dF//9185tatOCLi1j1c0qWnWuw85ZJnDV0p5Qh8DvQCwoA9SqlVWuvM/ZjeBA5orQcrpe5IO7/n7RbG29ubsLAwItJbpksBFxcXvNNrB6LoLVpkhvF37Jjj0J6zexiydAgJyQlsD92Oi5MLPp4+NK96+/WVjrU6svDhhVYocOHqVb8XAH8G/8mYdmMyDnTvbhbZOHMG6tYt2ENOnTL58/wqX970etm0yXSr3L3b9MQJDDQ/nJ2dM86NjLSshp5eMy/ou9mYJSmXjkCQ1joYQCm1CBgIZA7oTYH/AGitjyqlfJRS1bTW4bdTmDJlyuBb0pe9EsVHZCT88YdZ9CBbiuVczDkGLR5ENbdqvN39bV5Y8wLxyfGM7zS+RKcEG1dujHd5b/4I/iNnQAdTS888VXB+BAcXaBoEwKTFZs0yXUu1Nr9dffqpmXumZUtzTmpqxkyXeXnoITP9QNu2BSuXjVmScqkFhGbaDkvbl5k/8BCAUqojUBfIUe1USo1RSvkppfxKUy1cFAMbN8KXX5p87dq1Jgj88ouZyS/bwsvbQ7fTZ0EfriRcYdWIVYxuM5oto7bQw6cHz7Z91kYvUDSUUvSq14u/gv8iJTUl40CLFmbhh4KmXVJSTC2/IDV0yOiP/t57Zurip54y+zMP+Lt0yQR1S2rorq4wZkzB12y1MUtq6Lm9YfYk94fALKXUAeAQsB9IznGR1vOAeQDt27cvHYlyYXtBQdC7twkm6bp3h6tX4Y47iG/SkD1ntnI08ii/n/idVcdWUc2tGsuGLaNlNVPb61CrA5ue3GSjFyhaver14vsD37Pv/D461EobEJeeR8/P9LWZhYWZLqIF/U08PY8eGmoaahs3NqNM/f3h8cfNOZYMKiphLAnoYUDmlgJv4FzmE7TWV4HRAMr8Pnoq7UsI23vnHZNX3bPH1DJXrzaTP0VHo99+m14LerMtdBsAni6eTLtnGhM6T8Dd2d3GBbeNnvVM89cfwX9kBHQw85OvXAmXL+e5GMdNZe6DXhDpefQ9e8wIUycn05c9c8OoJcP+i1h4bDj/3f1fevj04L5691n9/pYE9D1AQ6WUL3AWGA6MzHyCUsoTiNNaJwLPAFvTgrwQthUQYOYimTQJmjUz+557zky69dNPrO7oybb123jvnvd4rOVj1K5QGwdVusfbVXWrSuvqrfkj+A/e7PZmxoEaNcxnRET+A3r6pFzWaCvr3dtMw9u7t9lu1coMMNLapE4smZiriMQmxjJpwyTmH5hPYkoiTg5OhRLQ8/yXq7VOBl4E1gNHgCVa6wCl1Fil1Ni005oAAUqpo0BfYLzVSypEfkyZYtbrfP31rPsrVyb5xXG85vcBTas05Y273qCuZ91SH8zT9arXi20h27iWeC1jZ3pgLEj716lTZvStNboHvvkmHDxoRo+CWe8gMhLSp7YuRjX0Obvn8NXer3ii1RMceeEIU3tMLZTnWDSwSGu9BliTbd/cTN/vABpat2hCFNDGjbB8OalT3uKNfdPZd34fiSmJeLl6Mb7TeE5En+BY1DF+feTXrNPGCu6vfz8zts9gccBinmqT1uCYHtDTa775cfKkCebWmADL1dWsepSuVSvz6e9v5nMvRjX0FUdX0KFmB+Y9OK9QnyPVEVEybd5sFgS+4w6mtrnKjO0zuHr9Kk4OTmwL3UaPH3rw/O/P09m7s8UDhUqTe33vpVOtTvx747+JTUybI8UaNfQ9e0xNujCkd1dMz6NHRJjBS+lT8lpB6JVQgqJvb8GPczHn2HV2F4PuGGS1ctyMBHRR8mzZYhrKfHxY+vkLTPOfxbj249j97G42PrmR0+NPM6fvHDrU7MCsPrNKdL/y/FJK8dn9n3E+9jwfbfvI7ExPXeQ3oIeHmx5HhbUIs6enGRiU3nXR0lGit2HI0iHcMecOXl3/KjHXYyy6ZtWxVQAS0IXIl9dfh5o12fDNv3h8+yR6+PRgZp+ZNw6XK1OOFzq+wPant9OxVs4RosLoUrsLw5sPZ8b2GYRcCTGjMcuVy3/KJX1FoMIK6GAGLP3zj2kYtXSUqIXCY8PZfXY3jb0a8+nOT2n+ZXPOx+S9FOWKoytoWKkhTbyaWK0sNyMBXZQs16+j9+9na/sq3L/hcZpUacLSoUsp42jHixbY0Ic9PwTgnc1pqxxVqZL/Gvq2bSb9UZijMR98EM6ehb17LZ+Yy0IbTm4AYMHgBfwz+h8uxF7gjb9yTG2VxZWEK2w8tZFBdwwqkt8EJaCLkuXAAVRiIjP1Tp5p8wzbn9qOl6vteznYq7qedRndejQ/H/6Z6Phok8IoSEDv0MGqOe0c+vc3vWh+/dXU0K2Ycll3ch3V3KrRqnor7qxzJ692eZUf/X9kR+iOm16zNmgtSalJRZJuAQnoooSJ/WcjAI37PsbXA76mXJlyNi6R/Xu+/fMkJCcw/8B8U+PNT8olPt7UmjOlW5JSkrKcEh4bzsJDC80PjlxkPz9XlSubEa0rVli1hp6SmsL6oPXc3+D+G11b3+z2JjU9avLS2pdI1ak5rjkedZyZO2dSza0anWp1sko58iIBXZQoYX/8wlkPGNn3NVsXpcRoUa0Fd9W5iy/9vkRXsaCG/uWXsGFD1n179kBS0o2AvvfcXjz+48H6oPWAmUp6+C/DGbl8JNU/rs6AhQOyTOG78uhKPP7jweGLh/Mu8KBBZubF+Hir1dD3nt9LVHwUfer3ubHP3dmdGb1msPf8Xr7yy5jTPS4pjtErR9Pk8yYcuniI9+59r8i6xUpAFyWG1hqX/Yc43sCTFtVa2Lo4Jcrz7Z8nKDqIEKe4vAP622/DzJlZ920zUyvQtSsAf4f8zfWU64xaOYqIaxF8s+8bNp/ezFt3v8X4TuPZcmYLw5cNJzk1mWuJ13hp7UtcT7nOj/4/5l3Y9GXtwGo19HVB61CoG9MLpxvRfAS96/dmwvoJ7D67m1SdymPLH+OHAz8wsfNETo0/xTNtn7FKGSxR7FYsEiK//AM30fpiIheG9rV1UUqch5s8zATXCfwdf5S6166Z2m+5XNJZiYkm4B87lnX/tm3QpIlJiQCHwg/h7uxOdHw0I34Zgd85P3r49OCdHu+glKJjrY4MWzaM6f9MJy4pjtCroTSo1IDFAYv58L4Pbz2it04d0/C6b1+eAf37/d+TnJrMs+1uPYvmuqB1dKjVIUd7jFKKnx/6mfZft+ehxQ/xQMMH+PXor8y8fybjOxf9gHmpoYsSY9svnwHQ/MGnbVySkqesU1lGtR7F33FHzY6b5dEvXDCfp0/D9evm+9RU02UxU/780MVDdKrVien3TeevU39xPeU68/rPu9ETZGizoQxvPpx3trzDxzs+5vGWj/N297cJuRJyy0bIGwYPNp+3SLnEJ8Uzcf1EPt356S1vtSN0B7vO7sqSbsmssmtlVjyyguj4aObtm8cLHV7g5U4v513GQiA1dFEinLl8hit//0mqAveuPWxdnBKpbY22LHZNm/U6IiL3+VjOpU3EmppqBhE1a2aG+1+6BJ07A6aB8fDFwzzX7jle7vQyZy6foV3NdjSsnHX2kDl957D59GbikuL4qNdHuJVxw8XJhYWHF3JnnTz6so8ZYxa3uEUXyV+O/MKV61e4nnI917WMtdbM2jWLyX9Mpm6Fujzd9uYVhVbVW/HLsF/YdHoTH/T8wGaD1aSGLuzeTwd/ouXclrQLTeJ6o3r5X6tS3FKdCnWIcE3buFkN/VymmbXT0y7pizenBdfgS8HEJ8fToloLHJQDn/X5jMdaPpbjVpVdK/P36L/ZMmoL1d2r41HWg/6N+rM0cCnJqTmWW8iqalX47LMsXSRTdSpXEq7c2P52/7cAJCQnEH4t5+JqkzZMYuL6ifRv1J99z+2jToVbLyDdt2FfPur1EU4OtqsnS0AXdm3N3Fep+PBjzNxenvsuelDuzh62LlKJVadCHSLc0jZu1jB6s4BepsyN6YsPXTwEQIuqeTdcN6jUgNbVW9/YHtF8BBevXWTTqdtfbGTShknU+rQWO8N2cjL6JJtPb6ZrbdNIe+pS1uUblh9Zzqc7P+X59s+zfNjyYrOId14koAu75rJ4Ob2DYdTv53C8dNmsZCMKRQ33Glx2T+t+lx7Qk5NNQ2i6c+fMYhM1amQN6M2a3Vi8+VD4IRSKZlWb3XYZ+jXsh4ezB0sDl97Wdacvn2bO7jnEJcXxwM8P8H8b/w8H5cDU7lMBOHU5I6CfjD7J6JWj6VirIzP7zLSruX4koAu7Vun0BY7c4YW6dMk0vKUvPyasztHBEbeq3qQ4ZFo84uWX4b5MCzWcO2eC+R13mICutQnomWZYPHTxEPUr1ce1jCu3y8XJhV71e7H+5Hq0tnwVy3e2vIODcmDLqC2UcSjD4oDF9G3QN9ca+uO/Po6jcmTJkCU4OzrfdhltSQK6sFtxidfwuZBAQgMfsyRZly6mdigKTe2Kdbni7pRRQ9+wAXbvzliv9fx5Mxd548YmoJ8/DxcvQps2N+5x6OIhi9ItN9OrXi9CroRwIvqERecfizzGj/4/Mq7DOLrV7ca6x9bRvGpzJnedjJuzG1XdqnL68mkALsVfYkfYDiZ1nURdz7r5LqOtSEAXdut44N94JoBzs5a2LkqpcaNhNCLC9CI5edJ0TwwNNSecO5cR0C9dyhgxmhbQ45PiCYoOKlBA713fLDmXPllWXqZumUo5p3K8cZeZSKt19dYcev4Q3X26A+Dr6Xsj5eIfbqbebVejXb7LZ0sS0IXdOrfbzNvi1a6bjUtSetQpX4cLLknoyAgznD9der48c0AHWLTIfKatJhQYEUiqTi3QSN56FetRr2I9iwJ6qk5lxdEVPNnqSaq6Vc31HB9Pn4yAfsEE9MwNsfZEArqwW9cO+gFQo+O9Ni5J6VGnQh0uukJy+IWsAf34cUhIgOjorAH9zz+hfn2TEuP2erjcSu96vdl0ehNJKUmkpKbwld9XRMbl7EoZdjWMhOQEWla7+W9xvp6+hFwJISU1hQPhB6jmVo1q7tUKVD5bkYAu7Jbj8RNcK+uAg7cVFhwWFqnrWTcj5bJ7txnO7+FhAnr64sw1apiVg8qWNbn1zPnz8EO4OLnQoFKDApWjd/3exCbGsjNsJ+9ueZexv4/l892f5zjvRJTJs9/qeb4VfUlOTeZszFn8L/jbbe0cJKALO6W1ptKpcCJqVwI76lZm7+pUqEOkKzhdvgo7d0KnTqY2fvx4Rh/0mjXNnOQN0oJopoC+NWQrrau3LvDsg/f43oODcuDtzW8zbes0wMw9nl36+p/ZR6Fm5uvpC5jpbgMiAmhVrVWBymZLFgV0pVQfpdQxpVSQUirHEh1KqQpKqdVKKX+lVIBSarT1iypEhguxF6h/MYmEhr62LkqpUrt8bSLcQKUv8dahAzRqlDOgQ0baJS2gn4w+id85Px6646ECl8PTxZNOtTqx6fQmmlVtxmtdX2P32d050i4nok/g4uSCd3nvm97Lx9MHgLUn1pKYkliya+hKKUfgc6Av0BQYoZRqmu20F4BArXUroAfwiVLKvjpwCrsScHInta9KD5ei5lHWg3hPt4wdHTuagH7mDASnzV+eHtCbpK2hmdYHfUnAEgCGNRtmlbIMumMQ5cuWZ9nQZTzc9GE0OkdD6YnoE9SvWP+WszPWqVAHhWLFsRWmuCU5oAMdgSCtdbDWOhFYBAzMdo4GPJQZUuUORAN5TLYgRP6d22uGfldpd7eNS1L6OFZJazB0doaWLU1A1xq2bjX7KlUyx194ARYvNjl1YHHAYrp4d7Fa/+5JXSdx9pWzNPZqTPua7fFy9WJd0Los55yIOpFnvr6sU1lqla9F8KVgXJxcbpmeKe4sCei1gNBM22Fp+zKbAzQBzgGHgPFa51yTSSk1Rinlp5Tyi8jvuoSiwEKuhPD8b8/z08GfivS5Edcicl2qKz/iDu4DwKNVB6vcT1iubI209EXr1iaAN2pktv/+G2rW5Ot93/BX8F8mkA8ztfGjkUfxD/fnkWaPWK0cDsoBd2f3G9/3rt+bdUHrbvwbS0lN4eSlkzSslHeATs+jt6jawqaTaxWUJQE9txan7GNu7wcOADWB1sAcpVT5HBdpPU9r3V5r3b6KFVfjFpZJSkni//76Pxr9txFz985l3JpxRMVFFcqzLsReIC4p7sb2jtAd1Py0pmUrzljA6dhxkh1VRsObKDIeteqZbzp2NJ8N0wJmTAy6Zg0mrJ/A5D8mZ7lm8eHFKBRDmw0ttHL1bdCXiLgI9p03P+zDroaRmJJoUY07PY9uzw2iYFlADwMy9wvzxtTEMxsNLNdGEHAKuMM6RRTWMnvXbD745wOGNB3C2kfXEnM9hg//+dDqz1kftJ4GsxvQ+ZvORMdHE5sYy+O/Pk5yajKbTt/+LHnZbTm9hUpnIrji7WVm8RNFqmLdO5jfCmKHP0xyajKxLg5QvToAVyu5E5cUx/4L+2+sCaq1ZnHAYu6uezc1PWoWWrnur38/CsXaE6a3S/rUALdTQ7fn/DlYFtD3AA2VUr5pDZ3DgVXZzgkBegIopaoBjYFgRLERHR/Ne3+/R58GfVjw0AL6NOjDE62e4L+7/0vY1TCrPeengz/Rf2F/vMt7cyzqGH0W9OHFNS8SfCmYBpUasCtsV4Hun5SSxEu/v0CbSCcqtOpspVKL21G7kg+jB8OO6km0ntua7vO730i7hHlkpNR+CfwFgG2h2zgSeYQRzUcUarmquFWhfc32rD6+GrCsD3q6ehXNbx2tqpfwGrrWOhl4EVgPHAGWaK0DlFJjlVJj006bBnRVSh0C/gJe11rfZAZ8YQvvbX2Pq9evMqPXjBv7pvaYikbz7pZ383XPq9evZpnxbuGhhTz262PcWftOdj2zi2VDl7H/wn5+8P+Bf7Udz8pfy+Fy+BiX4i/l+z0+3/M5j/0cgG9kMk4DsrfNi6KQvtDDoMWDCIgIYN/5fcT6mJr3cedYypctT9sabfnliAno07dNx8vVi8dbFf5MmI80e4Q95/YQGBFIUHQQLk4u1CqfvckvpyFNh/DlA1/emH3RXlnUD11rvUZr3UhrXV9r/X7avrla67lp35/TWvfWWrfQWjfXWi8ozEKL23My+iRzds/hqdZP0bxqc7M82N69+CS58Vy75/hu/3dEXLu9RuqT0Sep8UkN+i/sz+WEy/wT8g+jVo6ie93urHtsHRUiY3jQ+16WDV3G6NajeSeoNk03HWLwUdh9dne+3iM8NpzQ//yL17aDfv55eOqpfN1HFEx6QHd2dGZe/3kAHKlkaub7HcJpX7M9Q5oMYdfZXaw9sZbfjv/Gyx1fztd0ubfr8VaP4+TgxHf7v+NEtOnhcssFpdO4Obsxtv1Yi84tzuy79MIiM7bPoIxjGaa1mghjx5p+wu3bw4gRjGo9ihSdwpoTa27rntO2TiMlNYU/Tv5Bh687MGjRIHw8fVj+yHJc1v5hGsp692Zgw/5813cuTp+aBZybRsCusybtkpiSyIqjK0hJTbHomX+vnM1HqxO42utu1OzZMkLURmp61GT6fdP5e/TfPNP2Gaq6VWWri1nCbYcOpUPNDjzc9GEARi4fiVsZN17o+EKRlK2qW1UGNB7Aj/4/EhgRaFH+vCSRgF4KbA/dTj+vrlR/+An4/nvo0QNGjoS//qJNZBlqetTktxO/WXy/41HH+d/B//FixxfZ9OQmYhNjAfh95O9UWrLarLherZpZcOLjj2HBAggLg+rVaRNdlp1hOwGYs3sOgxcP5ku/Ly16buKmP3HU4DF/ocx7bkNKKV678zWaV22OUoqevj35rOJRTs58m411UuhQswONKjeiRdUWXE64zJh2Y6hUrlKRle/pNk8TERfByUsnCzxnjL2RgF7CJSQnEBIWwCczj8DBg7B8uZnSdPZscHFBzZlD/4b9WR+0nsSUxLxviKmduzi58KZzT+70asORF45weNxhGuw8DqNGwb33wuHDMGQITJkCb79thn+PHo1PRCL7QnaRlJLErF2zAPj3xn8THptzkd7sHI4dJ8atDCptoIooHnr69uRsfDifNLhIqgN0rGW6M45sMZJyTuWY2HlikZand/3eN3rTSA1dlCiHLx7m/T9S8T5+3ozae+ABc6ByZXjsMfjf/xhctTsxiTFsPbMVMIvpjvxlZK73Oxp5lJ8P/cwsh/5UuqcfPPYYnmUrUL1MRRg/3iw9tno1uLvDl19CxYqmdv7mm9CsGU4pmopno/nwnw+JvhjC5osPkBx/jdf+fO2W73E54TLVQy9z2ae6pFqKmfvqmSXovtv/HdXcqt2YN2VS10mcGn+K2hWKdjZMJwcnRrUaBdx6Uq6SSH5vLeH2n99Pl1BIuPtOXAcPznrwpZfgm2+4d6MZ8vzb8d9wVI58suMTnByc+GbANzkasub6zcU11YlR3+2DcuXg11/hu+/MRE1BQbB+vZk2FcDLC5YuNecMHgz+ZvGAphGmlv+v4Gp0X/w7P7zajyH+PzK69Wh6+PTI9T12n91Ny0hI6Xj7CwuLwlXXsy71K9bn5KWTdKjV4caiyk4OTjabV3xC5wlotN33WrldUkMv4Q6E+dE0Esq17ZjzYMuW0KMHznO/plfde1l5bCVjfhuDi5MLyanJOXqjaK1ZcXQF/z1WH6cTQSZY33uvqZlPmwYDB0Lv3lmf0a0bfPqpmU71jjvQStE6qgxJqUk8EW1qcoO2ReHr6Uv/n/uz/MjyXN/D/8hmql+DSq1L139Qe5FeS+9Qs3hMxVDFrQof9PzA7hZ5LigJ6CVc1MGduCSDanmTWQmffRZCQhiV3JzTl08TFB3E/wb/D4BtIduynHrgwgGSQ87w6MpgE7wfeAB++MGM1kxONoH7VlxdUT4+3BlbiSrlvKjnHwqurjju3MXOLt/SvGpzHl7yME+tfIrX/3id//vr/7gQewGAC3s3A1CuZZtbPEDYSq96vQDo7C2DvWxJUi4lWEpqCk4BR8xGi5ss+XX//eDgQM+jiTi4OvB4y8cZcsaN/ztRg20Nsgb0FUdX8NIecEpKgc9MN0S8vc0yY9HRUK9e3oVq2pQ7zwTzT6f3URcfgunT4d//purCVWyZsYWX177Mdwe+w8nBievJ1/EP92f1iNUkHj5orr9DZpQojgY3GczvI3+/EdiFjWitbfLVrl07LQpXwMUAPa0bOsXRQev4+Juf2Lmz1h076n3n9un469e09vXVMR5lted/KuiU1JQbp7X8sqU+XttN6x498l+oyZO1LltW65kztQatg4O1HjpU60qVtE5IyHLqjG0zNFPRn+34TE/vik4u46R1UlL+ny1ECQD46ZvEVUm5lGD7z++nxUVIrOcDLi43P7FvX9izhzZO3rhs3w2nTuEecx3XiCsERgQCEHwpmIgTB2kYeg369Ml/oZo2hevXTUNq3brg62vSPtHRpr96JuM7jaeJVxNe3fAqd0RCYn0f6X8uxC1IQC/B9l/YT8uL4Ny67a1P7NvXLFCwfr0ZeJSmZbgZlAQm3dL7ZNqBggT09FVsDh6Ee+4x3/fsaVJCzzwDDz0Ey5bBhAmU6XoXP1R9jlSdStMohUsz+544SYjCJgG9BDt6ag++l8ChVetbn9iuHVSpAkuWmJ4raYsSdL3kxrbQbSQkJ7Dg4AKGh1Uw06TerIHVEukBHTICuoODGVX6zjsmHz90KHz1FQQG0uHzXxnX4mnqXQLVNPvKh0KIzCSgl1BJKUkk+e83GzdrEE3n4GBq3atXQ3w8TJwIderQ/XJFtpzewoMLH8T/3H7uOZFszivIwJ7y5U1DKmQEdDADkaZMMetSbt5sUjDTpsGWLXwe3RmHVC0NokLkQQJ6CZSqU3lq1VPUDY0xO/IK6GDSLmBq0J06QcuWND2fxJkrZ9h4aiOrG7xF2asFzJ+na9XKzJ9dO5cRhF5e0L27GbT0zDNmfcrJaavfSEAX4pYkoJcwWmsmrpvIgoMLeLZMJ/DwMI2Peend29SSn3/e1MBbtaJySCTNyzdk6dCl9DvpYGryvazQLW3uXPj997zPc3c3o1kvXzbbjRsX/NlClGAS0LP5M/hPnl31LAnJCbYuSr4sOryI2btnM6HTBNpHlYXmzU0gzkvlymbOlRdfNNstW6JSUjh0zxIeavKQCcCdOmWs6F4Q3t6WrwX60kvg6gp16oCbW8GfLUQJJn3A0mitmb1rNq9seIVUncrw5sPpWa+nrYt121YdX0UNt+p8UnkE6uD8Gw2cFqlQIeP79IZPf38zCtTPL++RoIWhcmWYORMS7PMHrBBFSWroad7e/DYT1k+gTwOTI06fs9ueaK2J+ucPDn14FYeOneDqVTMSND8aNDB91w8ehFmzTOrm6aetW2BLPfusqakLIW5JAnqa+Qfm07dBX1aPWE0TrybsCNth6yLdtuNRxxm2JQqPhFSYPx/Cw02/7vxwcjLpmj/+MNPuPvWU6aEihCi2JKADkXGRhF4N5V7fe3FQDnT27szOsJ1ZFkC2B5tPbeK+YLjeoxs8+aTpMVIQLVvCoUMm5SI1ZCGKPYsCulKqj1LqmFIqSCn1Ri7HJyulDqR9HVZKpSilim7NqQLaf34/3leg98E4WLiQoafdiIqP4uSlk3lfXIwE7v4Nnyvg3meAdW6YnkcfMADq17fOPYUQhSbPRlGllCPwOdALCAP2KKVWaa0D08/RWs8AZqSd/yAwUWsdXThFtr595/excBm0DH0bgL5A4xdMHt1e1iTUWuO8+R8A1H33Weem3bqZqXHT+4ELIYo1S2roHYEgrXWw1joRWAQMvMX5I4CF1ihcUTlwbh9tw5VJU6xaBUCnqLJ21TB6IvoEHQKvEFvV03r9tdu2hStX4M47rXM/IUShsiSg1wJCM22Hpe3LQSnlCvQBfrnJ8TFKKT+llF9ERMTtlrXQRBzehWuihrvvNgNsHB25L66GXTWMbg7eyL2nIOWeHtZdc7NcOevdSwhRqCwJ6LlFh5u1Fj4IbLtZukVrPU9r3V5r3b5KlSqWlrFQXb1+lfLHzpiNli3NepiNGtEu0hn/C/7EJcXd8vrk1ORi0YB6avMKvOKhfL/BeZ8shCiRLAnoYUDmSTe8gXM3OXc4dpZu8b/gT8tw0A4OZq5ugObNqRt2lRSdwt5ze295/Zd7vqTLt13o+WNPTkSdKIIS56S1ptxWM82t1fLnQgi7Y0lA3wM0VEr5KqWcMUF7VfaTlFIVgO7ASusWsXDtv2AWgUip72uGmAO0aIFraDiuieSZdvlz31ImBHiw/6wfLee2ZM2JNTnO0VrzyLJHeGTZI4UypcCJ6BN0PBLDJd8aULOm1e8vhLAPeQZ0rXUy8CKwHjgCLNFaByilxiqlxmY6dTCwQWt9rXCKWjj2X9hPmwhHnFplWny4eXOU1vRKqJlj5fvMriRcoemv2/hsaQyn1SvU9KjJR9s+ynHeX6f+YknAEpYELGHw4sFWD+qbgzfSJQxU9+5Wva8Qwr5Y1A9da71Ga91Ia11fa/1+2r65Wuu5mc6Zr7UeXlgFLSxHTvvhE5WSddGGtOlm74+vyZ5ze2567YaTG+h6JhWACu9O55WKD7D1zFbOXj174xytNW9teosHrlTj5yZvsT5oPYMWDSLmeozV3uH49tVUuA4V7u5ttXsKIexPqR4pmpCcgEPgERw0WQO6ry+UK0eHqHKEXAnh4rWLuV7/+7HV3BWq0A88AK6uPDVrKw4pmsUBi2+cszZoLYFBO/nl66uMGP81P/Scw5/Bf9Ll2y4ERQcV+B201iTt2AaA6tSpwPcTQtivUh3QD4YfpOn5FLOReREIR0do1oz6Z00Plz1nc9bSU1JTOLltNRXjNWroUJgzh3J7/XkvqDY/H/oZMMF2yqYpTDlUkbKx8XDhAo//FsL6x9ZzPvY8Hb7uwPqg9QV6h6DoIBoFXyHRzUXmCxeilCvVAX1n2E5ahkOquxv4+GQ92Lw5FYJCUahc0y67z+6m2bHLZqNbNxg+HFq04NFjzuw9v5fjUceZuH4iR0/v5YVtSfDAAzBqFHz6KT1T6uD3rB91KtSh38/9mLFtRr67PW4+vZmOZyGpTSvzg0gIUWqV+oDePqosDi1a5lwEokULHMIv0tWlYa4B/bfjv9EtVJFavbpJ0SgFDzyA98EzVEiAvj/1ZdauWSy82A2Xy7Hw1lvwn/+YKWknTsTX04ftT21nSNMhvPbna4xaOYpUnXrb77DtxF+0CgfXO3vk809BCFFSlOqAviN0Oy3Cde5rbjZvDkD/RB/2nN2TpQZ9OeEy8/3nc99ZZxy6dcsYmdmvHyo5mfExzQi+FMy/2oyn/+pjZtm2Tp2genWYOtWs/jNiBG5JsOjhRUztPpUf/X/ktT9eu63ya62J2vEXzimSPxdClOIViy7EXiAx9AwesWRtEE2XFuS7XinPv1IjCLkSQl1Pszbny2tfxjnsAtWiUuGuuzKu6dIFKlRgQnQj6j0zmSc2X0JdvGhq5+kmToTERHjzTThyBPXrr0zpPoXIuEg+2fEJ9SrWY1yHcRa9w8Hwg/gejzQbHTvm549BCFGClNoa+s6wnTRL77zSrFnOE6pXBy8v7jibCHAj7fLrkV/538H/McNtkDkvc0B3coLevam4eSdPNh6GmjHDrGDfrVvGOUrBG2/AmjUQEgIdOqD+/JOZfWbyYKMHeWntS/id88uz/AnJCTy54km6hTuTUqM61Mp1eh0hRClSqgN6y8i0188toCsFrVvjdSyUMg5l2HN2D9tCtvHcb8/RtkZbBkdVNavSZ6/d9+sH58/D+PFw7lzW2nlmffrAnj1mZGefPjj+dw4LHlqAi5ML8/bOy7P8r6x/Bf9wf/pe8sKxU+fbfHshRElUqgP6XdcqQ5Uq5is3bdrgEBBAG68WfLX3K+76/i5cnFxY8MB3OP72u6mdO2XLWvUxa5Ly9dcmBXPvvTcvRIMGsGOHWUBiwgTKr9/MkKZDWBywmPik+JtetiRgCV/6fcmU5i/ifvqcyc8LIUq9UhnQk1OT2XNuDy0jnTIm5MpNmzaQmMgwmhGbGMvkrpMJfCGQJmt2Q2ioqYVnV706tGtnvn/rrbynsnV3h4ULzTVPPMFzFXtx9fpVVh7LfUqcwIhAnlr5FF28uzDF39PszJzSEUKUWqUyoB8KP0RcYhzeYVdyT7eka2Pmd3nZuRtnXznLR70+wl2VhQ8+MI2Q99+f+3UTJpjFMtJr63lxcYGlS8HBgS4TP6Wha21+8P8hx2mx/2xizFf9cXd2Z1WtSTi+/wE88YQsQCGEAEppQN8Wuo0aMeAcE3frGnrDhuDqSpmDh6nmXs3s+9//4PRpmDLl5rXvxx6D+fNvb6EJX1/47jvU/v38J7I1G05u4FxMplmK4+Io26MnG949xZ6ge/F6djzUqwdz5lj+DCFEiVbqAvrKoyt5/c/X6ZtUx+y4VUB3dIRWrWD/frOdnAzvv2/SI/36Wb9wAwdCrVr0DrxOqk5lwcEFNw5dDjlOmRRNTE0van+1EMLDYdEi8PCwfjmEEHapVAX0ObvnMHjxYJpVacZn3s+YnbdKuYBJuxw4AKmpsGwZBAfDv/9t3WXe0ikFAwbgsWkbHSq14M/gP28cunDSH4CTb4wBf3/YsiUjVy+EEJSigJ6UksSrG16ld/3ebB61mfInw6By5Zv3cEnXpg3ExJhA/umnJg0zYEDhFXTAALh2jYfOexJ8KfjG7ugzRwGo5NPUdJXs0qXwyiCEsEulJqAHRQeRmJLIYy0fw7WMKwQEmHRLXjXttIZRPv/c9BufMCHnvC/WdM894O7OPYdiOXPlDMmpyQDEhp4EoEa9XEa1CiEEpSigB0YEAtC0SlPQGgID8063gJnTxckJZs+GihVN75XCVLYs9OlD812nSElOJuxqGADXz4UCUKFOw8J9vhDCbpWagH4k8ggAjSs3Ng2Kly7dukE0Xdmy5rzUVHjuOXBzK+SSAgMG4BZxmXbnuZF20eEXiHFxMF0chRAiF6UmoAdGBOLj6YObs5tJt4BlNXQwjY9OTvDii4VXwMz69UM7OjLwaEZAd4q6RIxnuaJ5vhDCLpWa2RaPRB5h+Tcx8ENbcHU1Oy2poQO8844ZwFNUE2BVrgxt2tD57F42XgpGa41bdCwJlaoWzfOFEHapVNTQU1JTCD17hDaHoyAqynRDrFcPqlWz7Aa1a0OPHoVZxBxUvXrUj3Ei+FIw0fHRVI5JIbWKV5GWQQhhXywK6EqpPkqpY0qpIKXUGzc5p4dS6oBSKkAptcW6xSyYM1fOUCX6utn4z3/g8mU4cqRw+pJbS9261LqUTHBUEKcvn6baNXCsXtPWpRJCFGN5plyUUo7A50AvIAzYo5RapbUOzHSOJ/AF0EdrHaKUKla5gcCIQOpcSduoUyfnDInFkY8Pzsma2NCTnIk8SZs4SKlV19alEkIUY5bU0DsCQVrrYK11IrAIGJjtnJHAcq11CIDW+iLFSGBEILXTA3rt2jYti8XqmuBd4cJljh/fgQNQXrosCiFuwZKAXgsIzbQdlrYvs0ZARaXUZqXUXqXUE7ndSCk1Rinlp5Tyi4iIyF+J8+FI5BGaxbubAUE17SRt4eMDQN3LEBiwCQDXWj42K44QovizJKDnlmjW2badgHbAA8D9wFtKqUY5LtJ6nta6vda6fZW8htxbUWBEIE0TPEwwL1OmyJ5bIGk19LpXIPzUIbPP0kZcIUSpZElADwMy5ym8gXO5nLNOa31Nax0JbAVaWaeIBaO15kjEEXyvOpj8ub1wdye1UkV8LoNXTKrZV7VYNU0IIYoZSwL6HqChUspXKeUMDAdWZTtnJdBNKeWklHIFOgFHrFvU/Dkbc5aYxBiqRSfaV0AHHHx8aRBThqrX0nZIQBdC3EKeAV1rnQy8CKzHBOklWusApdRYpdTYtHOOAOuAg8Bu4But9eHCK7ZltNasPrYalQoeFy/bXUCnbl3qX3WkWiykODmCp6etSySEKMYs6r+ntV4DrMm2b2627RnADOsVrWBORJ3gxbUvsuHkBnqUuwOHxKP2F9B9fKi5Jolq1SCxsiflinO/eSGEzdlBh+zbp7Vm4KKBnI89z6w+sxinOwBd7S+g162Ly/UUmkaAribpFiHErZXIof8bT23kSOQRZvWZxcudXsbpbFobrh0GdIA24YqyNbxtXBghRHFXImvon+/5HC9XL4Y1G2Z2hISYT3sL6Gl90Z2TNciwfyFEHkpcDT3kSggrj63kmTbP4OKUNnd4SIiZx9zeGhXrZhrqLz1chBB5KHEB/Su/rwAY235sxs6QEFM7t7dGRU9PKF/efC+DioQQeShRAT0uKY6v931N/0b9qeuZqXabHtDtjVIZtXSpoQsh8mB/Af3QIRg9GhISsuzWWvPMqmeIjItkUpdJWa8JDbXPgA4ZAV1q6EKIPNhfQA8Ph/nzYcGCLLs/3v4xCw8v5L1736Nb3W4ZBxISzDX2GtDTGkalhi6EyIv9BfSePaFNG5gxwyzcjOmm+MZfbzC06VD+dde/sp4fFmY+7TWg169vPmvUsG05hBDFnv0FdKXg9dfh+HFYZaaUeWfLO9SpUIfvB36PytzwGRcHGzea7+01oD/9NKxZIykXIUSe7C+gAzz8MPj6wvTpHIs4ytYzW3mu3XO4ObtlnPPGG1ChAjz3HDg7Q+PGtitvQXh4QN++ti6FEMIO2GdAd3KCSZNg507++vEdnBycGNV6VMbxU6dMSqZvX1OLP3tWUhZCiBLPPgM6wKhRaC8v6s5fzoONHqS6e/WMY7NmmdWJvvgCHnwQvLxsV04hhCgi9hvQXV05Nugu+gQk8lKNQRn7L12Cb76BESPAW+Y/EUKUHvYb0IH/NI1GAd3XH83Y+dVXcO0avPqqzcolhBC2YNcBfXnCPgI6+ODwzbdw/TpcvgyzZ0OvXtCqWKyAJ4QQRcZuA7rWmmuJ19j3UBe4eBE+/BA6dYLISHjrLVsXTwghipzdBvSE5AQ0mvNdmkODBjB1qsmfb9wI3brleb0QQpQ0dhvQ45LiAHAr6wEffAB9+sCePXDXXTYumRBC2IbdLnBxLekaAK5lXGHoUPMlhBClmN3W0K8lmoCeZXSoEEKUYhYFdKVUH6XUMaVUkFLqjVyO91BKXVFKHUj7mmL9omZ1I+VSRgK6EEKABSkXpZQj8DnQCwgD9iilVmmtA7Od+rfWun8hlDFXWVIuQgghLKqhdwSCtNbBWutEYBEwsHCLlTdJuQghRFaWBPRaQGim7bC0fdl1UUr5K6XWKqWa5XYjpdQYpZSfUsovIiIiH8XNkJ5ykRq6EEIYlgT03FZW1tm29wF1tdatgP8CK3K7kdZ6nta6vda6fZUqVW6roNmlp1wkhy6EEIYlAT0MqJ1p2xs4l/kErfVVrXVs2vdrgDJKqUKd4vBGo6ikXIQQArAsoO8BGiqlfJVSzsBwYFXmE5RS1VXaUkFKqY5p942ydmEzS8+hS8pFCCGMPHu5aK2TlVIvAusBR+A7rXWAUmps2vG5wBDgeaVUMhAPDNdaZ0/LWJWkXIQQIiuLRoqmpVHWZNs3N9P3c4A51i3arcUlxVHWsSyODo5F+VghhCi27HqkqKRbhBAig/0G9KRr0iAqhBCZ2G1Aj0uKkxq6EEJkYrcB/VrSNWkQFUKITOw3oCdKykUIITKz24AuKRchhMjKbgO6pFyEECIruw3ocUlxknIRQohM7DagX0u8hquTpFyEECKd/QZ06YcuhBBZ2GVA11pLo6gQQmRjlwH9esp1UnWqNIoKIUQmdhnQZfk5IYTIyS4Duiw/J4QQOdllQJe50IUQIif7DOiSchFCiBzsMqBLykUIIXKyy4AuKRchhMjJLgO61NCFECInuwzokkMXQoic7DOgS8pFCCFysCigK6X6KKWOKaWClFJv3OK8DkqpFKXUEOsVMSdJuQghRE55BnSllCPwOdAXaAqMUEo1vcl504H11i5kdpJyEUKInCypoXcEgrTWwVrrRGARMDCX814CfgEuWrF8ubqWdA1nR2ecHJwK+1FCCGE3LAnotYDQTNthaftuUErVAgYDc291I6XUGKWUn1LKLyIi4nbLeoPMtCiEEDlZEtBVLvt0tu2ZwOta65Rb3UhrPU9r3V5r3b5KlSoWFjGna4my/JwQQmRnSc4iDKidadsbOJftnPbAIqUUgBfQTymVrLVeYY1CZnct6ZrU0IUQIhtLAvoeoKFSyhc4CwwHRmY+QWvtm/69Umo+8FthBXOQ9USFECI3eQZ0rXWyUupFTO8VR+A7rXWAUmps2vFb5s0Lw7UkSbkIIUR2FnUT0VqvAdZk25drINdajyp4sW4tLikOD2ePwn6MEELYFfscKZooC0QLIUR29hnQJeUihBA52GVAl37oQgiRk10GdOmHLoQQOdldQNdaSz90IYTIhd0F9MSURFJ1qjSKCiFENnYX0GUudCGEyJ39BfS0qXMl5SKEEFnZXUBPX9xCUi5CCJGV3QV0SbkIIUTu7C6gy/JzQgiRO7sL6LL8nBBC5M7+AnqSNIoKIURu7C6gV3evzpCmQ6jimv8Vj4QQoiSyu1WWu9buStfaXW1dDCGEKHbsroYuhBAidxLQhRCihJCALoQQJYQEdCGEKCEkoAshRAkhAV0IIUoICehCCFFCSEAXQogSQmmtbfNgpSKAM/m83AuItGJx7EVpfG9559KjNL53ft65rtY616HyNgvoBaGU8tNat7d1OYpaaXxveefSozS+t7XfWVIuQghRQkhAF0KIEsJeA/o8WxfARkrje8s7lx6l8b2t+s52mUMXQgiRk73W0IUQQmQjAV0IIUqIYh3QlVJ9lFLHlFJBSqk3cjmulFKz044fVEq1tUU5rcmCd3407V0PKqW2K6Va2aKc1pbXe2c6r4NSKkUpNaQoy1cYLHlnpVQPpdQBpVSAUmpLUZfR2iz4911BKbVaKeWf9s6jbVFOa1JKfaeUuqiUOnyT49aLY1rrYvkFOAIngXqAM+APNM12Tj9gLaCAzsAuW5e7CN65K1Ax7fu+9v7Olr53pvM2AmuAIbYudxH8XXsCgUCdtO2qti53Ebzzm8D0tO+rANGAs63LXsD3vhtoCxy+yXGrxbHiXEPvCARprYO11onAImBgtnMGAj9qYyfgqZSqUdQFtaI831lrvV1rfSltcyfgXcRlLAyW/F0DvAT8AlwsysIVEkveeSSwXGsdAqC1tvf3tuSdNeChlFKAOyagJxdtMa1La70V8x43Y7U4VpwDei0gNNN2WNq+2z3Hntzu+zyN+clu7/J8b6VULWAwMLcIy1WYLPm7bgRUVEptVkrtVUo9UWSlKxyWvPMcoAlwDjgEjNdapxZN8WzGanGsOC8SrXLZl72PpSXn2BOL30cpdQ8moN9VqCUqGpa890zgda11iqm82T1L3tkJaAf0BMoBO5RSO7XWxwu7cIXEkne+HzgA3AvUB/5QSv2ttb5ayGWzJavFseIc0MOA2pm2vTE/tW/3HHti0fsopVoC3wB9tdZRRVS2wmTJe7cHFqUFcy+gn1IqWWu9okhKaH2W/vuO1FpfA64ppbYCrQB7DeiWvPNo4ENtkstBSqlTwB3A7qIpok1YLY4V55TLHqChUspXKeUMDAdWZTtnFfBEWitxZ+CK1vp8URfUivJ8Z6VUHWA58Lgd19Syy/O9tda+WmsfrbUPsAwYZ8fBHCz7970S6KaUclJKuQKdgCNFXE5rsuSdQzC/kaCUqgY0BoKLtJRFz2pxrNjW0LXWyUqpF4H1mNbx77TWAUqpsWnH52J6O/QDgoA4zE93u2XhO08BKgNfpNVWk7Wdz1Bn4XuXKJa8s9b6iFJqHXAQSAW+0Vrn2vXNHlj49zwNmK+UOoRJRbyutbbrKXWVUguBHoCXUioMeBsoA9aPYzL0XwghSojinHIRQghxGySgCyFECSEBXQghSggJ6EIIUUJIQBdCiBJCArooNZRSnkqpcWnf11RKLbN1mYSwJum2KEoNpZQP8JvWurmtyyJEYSi2A4uEKAQfAvWVUgeAE0ATrXVzpdQoYBBmsEtz4BPM9K6PA9eBflrraKVUfeBzzLSuccCzWuujRf0SQtyMpFxEafIGcFJr3RqYnO1Yc8x0tR2B94E4rXUbYAeQPsvhPOAlrXU7YBLwRVEUWghLSQ1dCGOT1joGiFFKXQFWp+0/BLRUSrljFhdZmmm2x7JFX0whbk4CuhDG9Uzfp2baTsX8P3EALqfV7oUoliTlIkqTGMAjPxemzcd9Sik1FG6sA1ki1nMVJYcEdFFqpM0dvy1tsd4Z+bjFo8DTSil/IIDcl8kTwmak26IQQpQQUkMXQogSQgK6EEKUEBLQhRCihJCALoQQJYQEdCGEKCEkoAshRAkhAV0IIUqI/wcctakhQsFgfAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4113922267707686e-06, 4.962424323335317e-07, 1.2198518252353178e-06, 8.130555893705317e-07, 8.86714933048566e-07, 7.183821481926228e-07, 7.668178073803733e-07, 7.671199362631218e-07, 1.444618203849193e-06, 9.879660843557931e-07, 1.366446770315942e-06, 7.07334789282269e-07, 6.518079698224703e-07, 9.678683383663682e-07, 1.0550119214209368e-06, 6.270501438849399e-07, 5.038772229059507e-07, 6.525217543052944e-07, 4.948116390308717e-07, 1.6022118846792474e-06]\n",
      "9.070551810407057e-07 3.317514446313991e-07\n",
      "[-0.02240451396189718, -0.02061250310341718, -0.0144149623645447, -0.002241030372923062, -0.024151417133236805, -0.0193373478562097, -0.019816439772819303, -0.02334833916888715, -0.012536045797651416, -0.0823846223922736, -0.02949534115523078, -0.02337977332855333, -0.020314765150207855, -0.03576925703959329, -0.025364653241191137, -0.022140508511657175, -0.034496750718286645, -0.021434877871736743, -0.05076698275838652, -0.026026415255519925, -0.026157372233977638, -0.035367363991322974, -0.020165965137334027, -0.028530187747743435, -0.05588400634356479, -0.02311889433344816, -0.025825479527009768, -0.05008792680826274, -0.013262896449446539, -0.016731327795169604, -0.05862000504409647, -0.03651555868817613, -0.04242112725737056, -0.008750934019397263, -0.029158765729728725, -0.035155877042057485, -0.032748040220588914, -0.03772272260489913, -0.0435459382416265, -0.0343115759252963, -0.02669355621876771, -0.06078906029755051, -0.03637224184397936, -0.018001859826069243, -0.04654153935364615, -0.017822340589351723, -0.045006443966527976, -0.022035434867589524, -0.013385124833756794, -0.009895623585620631, -0.017724122036646386, -0.015065001106651241, -0.0036473296452818153, -0.05722810721721096, -0.011612771096227843, -0.04764026738341214, -0.03614797123695612, -0.05855054756477476, -0.010663561255822046, -0.02615548035014654, -0.018895552708267704, -0.01399912615475369, -0.023395334455915975, -0.03617525962752861, -0.015115743698923403, -0.025453919682863826, -0.02570849097388668, -0.01035277096905018, -0.02403165855177109, -0.02637991545239471, -0.025136578503657726, -0.018240096479581122, -0.02934115764600231, -0.039044054750922624, -0.028926407236394305, -0.02245927172223857, -0.0177257454059348, -0.03350729871775203, -0.023204949114778235, -0.005049981371838784, -0.0369742266491717, -0.02326424273841963, -0.04174811614310308, -0.028048814037261435, -0.024593509312850745, -0.021527484328738508, -0.011996265780879981, -0.0069927229140872415, -0.039274972024442016, -0.019698908950712992, -0.028231240439905474, -0.028796668834777064, -0.04292249581152893, -0.025720129124248113, -0.03359017456397065, -0.02638146448988149, -0.03960489509131112, -0.0052185301296544025, -0.015004616360998826, -0.049028902900756406, -0.011667643119854543, -0.03038392234747867, -0.03543340281610099, -0.033060300610941874, -0.017244075594740886, -0.01796674038865176, -0.006575047164490915, -0.020218975300829882, -0.012574136049496885, -0.04003116660684614, -0.029312091001783037, -0.023715343407727997, -0.03584399772259317, -0.02513079147209342, -0.024642032786323344, -0.016485593356766068, -0.01711092344346364, -0.008456784379999548, -0.05092495476750053, -0.03458249186030884, -0.01244992655077205, -0.01445938469665856, -0.03404864209834758, -0.037754446798476976, -0.020535534851354967, -0.045793516844108005, -0.03246738639579944, -0.028518906258881233, -0.024122278956661934, -0.013378965651921298, -0.02176645996981879, -0.03674141094287897, -0.05146268460248611, -0.018709477737599713, -0.025319652868046112, -0.05978263664358127, -0.046900199796771874, -0.012406844998622369, -0.03614439681331233, -0.024476950608392382, -0.023166621832768988, -0.03677066288443369, -0.02311643512717032, -0.011010751952075834, -0.03169189645529043, -0.028324313335340037, -0.01681387265533729, -0.034153683424525454, -0.019370984358130647, -0.00560170295387296, -0.02858293713569174, -0.03727538939750023, -0.03294139502670732, -0.048664767876776115, -0.040788295638510136, -0.021545124665606825, -0.028866108829543595, -0.01812654170640943, -0.023076964567766242, -0.03246268720626446, -0.02168221644281831, -0.024616920256219203, -0.02015184591643972, -0.019508322579068035, -0.03912087499642245, -0.022257481195774996, -0.009726645393396134, -0.029471500505639597, -0.05170661963556182, -0.02399246793458744, -0.019691511054518926, -0.004388325900886653, -0.02136577644265618, -0.02333370552093858, -0.011176477953116451, -0.05387973113189596, -0.014667206533016484, -0.03903724182175141, -0.020932042223565876, -0.049739761460130066, -0.01973415191231786, -0.035238091396817564, -0.025127334729279683, -0.008336144593398376, -0.015089934932753835, -0.01597712250677523, -0.03311328894330561, -0.0027564235821034154, -0.04542667353357789, -0.004738971116710371, -0.02846273864525107, -0.015077520713568786, -0.048219112285690154, -0.018180863740661378, -0.021602675520373153, -0.02141509432174736, -0.02881383968846321, -0.011851162935942544, -0.028226165362221837, -0.029344192234183726]\n",
      "-0.026921940115638195 0.013232613540019431\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    delta = []\n",
    "    plt.figure()\n",
    "    D = generate_data(apm, opm, num_steps, dt, n=1)\n",
    "    old_out = 0\n",
    "    for tupel in D:\n",
    "        inp = torch.tensor(np.array([old_out, tupel[\"p\"], tupel[\"ttm\"]])).double()\n",
    "        out.append(model(inp).detach().numpy()[0])\n",
    "        delta.append(tupel[\"delta\"])\n",
    "        old_out = out[-1]\n",
    "    plt.plot([i / len(out) for i in range(len(out))], out, label=\"NN\", color=\"green\")\n",
    "    plt.plot([i / len(delta) for i in range(len(delta))], delta, label=\"delta\", color=\"red\")\n",
    "    plt.xlabel(\"time\")\n",
    "    # plt.xlabel(\"\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"test_deep_classic_new_no_delta_final_new_2_cost_1000\" + str(i) + \".png\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "test_res = []\n",
    "delta_res = []\n",
    "for i in range(20):\n",
    "    losses, delta_losses, test_result, delta_test_results = test(model, apm, opm, num_steps=num_steps, dt=dt, n=1)\n",
    "    test_res.append(test_result)\n",
    "    delta_res.append(delta_test_results)\n",
    "\n",
    "print(test_res)\n",
    "print(np.mean(test_res), np.std(test_res))\n",
    "print(test_res_delta)\n",
    "print(np.mean(test_res_delta), np.std(test_res_delta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fcb9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_2(model, apm, opm, num_steps, dt, n=10):\n",
    "    D = generate_data(apm, opm, num_steps, dt, n=1)\n",
    "    losses = []\n",
    "    tc = []\n",
    "    tc_delta = []\n",
    "    delta_losses = []\n",
    "    old_out = 0\n",
    "    old_delta = 0\n",
    "    for tupel in D:\n",
    "        inp = torch.tensor(np.array([old_out, tupel[\"p\"], tupel[\"ttm\"]])).double()\n",
    "        out = model(inp)\n",
    "        trading_costs = abs(old_out - out) + 0.01 * (old_out - out) ** 2\n",
    "        loss = (-tupel[\"nop\"] + tupel[\"op\"]) + out * (tupel[\"np\"] - tupel[\"p\"]) - \\\n",
    "               (T / num_steps) * trading_costs\n",
    "        losses.append(loss.detach().numpy())\n",
    "\n",
    "        trading_costs_delta = abs(old_delta - tupel[\"delta\"]) + 0.01 * (old_delta - tupel[\"delta\"]) ** 2\n",
    "        delta_loss = (-tupel[\"nop\"] + tupel[\"op\"]) + tupel[\"delta\"] * (tupel[\"np\"] - tupel[\"p\"]) - \\\n",
    "                     (T / num_steps) * trading_costs_delta\n",
    "        delta_losses.append(delta_loss)\n",
    "        tc.append(trading_costs.detach().numpy()[0])\n",
    "        tc_delta.append(trading_costs_delta)\n",
    "        old_delta = tupel[\"delta\"]\n",
    "        old_out = out.detach().numpy()[0]\n",
    "    return np.sum(tc), np.sum(tc_delta), np.sum(losses), np.sum(delta_losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3aa81ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:27<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.023031303665486752 0.012544270105663564\n",
      "-0.027527447792799063 0.012327243826499726\n",
      "3.4063337100595343 0.8578088746180229\n",
      "4.139244710554109 1.3405432370306851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "test_res = []\n",
    "test_res_delta = []\n",
    "cost = []\n",
    "costs_deta = []\n",
    "for i in tqdm(range(200)):\n",
    "    tc, tc_delta, test_result, delta_test_results = test_2(model, apm, opm, num_steps=num_steps, dt=dt, n=1)\n",
    "    test_res.append(test_result)\n",
    "    test_res_delta.append(delta_test_results)\n",
    "    cost.append(tc)\n",
    "    costs_deta.append(tc_delta)\n",
    "\n",
    "print(np.mean(test_res), np.std(test_res))\n",
    "#print(test_res_delta)\n",
    "print(np.mean(test_res_delta), np.std(test_res_delta))\n",
    "#print(cost)\n",
    "print(np.mean(cost), np.std(cost))\n",
    "#print(costs_deta)\n",
    "print(np.mean(costs_deta), np.std(costs_deta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a40d2748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.023031303665486752 0.012544270105663564\n",
      "-0.027527447792799063 0.012327243826499726\n",
      "3.4063337100595343 0.8578088746180229\n",
      "4.139244710554109 1.3405432370306851\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(test_res), np.std(test_res))\n",
    "print(np.mean(test_res_delta), np.std(test_res_delta))\n",
    "print(np.mean(cost), np.std(cost))\n",
    "print(np.mean(costs_deta), np.std(costs_deta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "404efdf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc6a8b24d00>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Distribution of returns of two strategies\")\n",
    "sns.kdeplot(test_res, fill=False,label=\"NN\")\n",
    "sns.kdeplot(test_res_delta,fill=False,label =\"Delta_hedge\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ecc17b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc6a93e6460>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Distribution of trading costs of two strategies\")\n",
    "sns.kdeplot(cost, fill=False,label=\"NN\")\n",
    "sns.kdeplot(costs_deta, fill=False,label=\"delta\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23d3d68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-a451aa57e95a>:1: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  pd.Series(test_res)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4be5873f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28cfd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
